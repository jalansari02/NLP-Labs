{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBsKzKsf3Z_Q"
      },
      "source": [
        "## Introduction\n",
        "##### How to get started with topic modeling using LDA in Python\n",
        "** **\n",
        "Topic Models, in a nutshell, are a type of statistical language models used for uncovering hidden structure in a collection of texts. In a practical and more intuitively, you can think of it as a task of:\n",
        "\n",
        "- **Dimensionality Reduction**, where rather than representing a text T in its feature space as {Word_i: count(Word_i, T) for Word_i in Vocabulary}, you can represent it in a topic space as {Topic_i: Weight(Topic_i, T) for Topic_i in Topics}\n",
        "- **Unsupervised Learning**, where it can be compared to clustering, as in the case of clustering, the number of topics, like the number of clusters, is an output parameter. By doing topic modeling, we build clusters of words rather than clusters of texts. A text is thus a mixture of all the topics, each having a specific weight\n",
        "- **Tagging**, abstract “topics” that occur in a collection of documents that best represents the information in them.\n",
        "\n",
        "There are several existing algorithms you can use to perform the topic modeling. The most common of it are, Latent Semantic Analysis (LSA/LSI), Probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "In this tutorial, we’ll take a closer look at LDA, and implement our first topic model using the sklearn implementation in python 2.7\n",
        "\n",
        "### Theoretical Overview\n",
        "LDA is a generative probabilistic model that assumes each topic is a mixture over an underlying set of words, and each document is a mixture of over a set of topic probabilities.\n",
        "\n",
        "![LDA_Model](https://github.com/chdoig/pytexas2015-topic-modeling/blob/master/images/lda-4.png?raw=true)\n",
        "\n",
        "We can describe the generative process of LDA as, given the M number of documents, N number of words, and prior K number of topics, the model trains to output:\n",
        "\n",
        "- `psi`, the distribution of words for each topic K\n",
        "- `phi`, the distribution of topics for each document i\n",
        "\n",
        "#### Parameters of LDA\n",
        "\n",
        "- `Alpha parameter` is Dirichlet prior concentration parameter that represents document-topic density — with a higher alpha, documents are assumed to be made up of more topics and result in more specific topic distribution per document.\n",
        "- `Beta parameter` is the same prior concentration parameter that represents topic-word density — with high beta, topics are assumed to made of up most of the words and result in a more specific word distribution per topic.\n",
        "\n",
        "**To read more: https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzBnAiJQ3Z_T"
      },
      "source": [
        "** **\n",
        "### LDA Implementation\n",
        "\n",
        "1. [Loading data](#load_data)\n",
        "2. [Data cleaning](#clean_data)\n",
        "3. [Exploratory analysis](#eda)\n",
        "4. [Prepare data for LDA analysis](#data_preparation)\n",
        "5. [LDA model training](#train_model)\n",
        "6. [Analyzing LDA model results](#results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeCh_6Jr3Z_U"
      },
      "source": [
        "** **\n",
        "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/project_158/img/nips_logo.png\" alt=\"The logo of NIPS (Neural Information Processing Systems)\">\n",
        "\n",
        "Let’s start by looking at the content of the file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWFEdhw-3Z_U"
      },
      "source": [
        "** **\n",
        "#### Step 1: Loading Data <a class=\"anchor\\\" id=\"load_data\"></a>\n",
        "** **\n",
        "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "Let’s start by looking at the content of the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "mcgSZsoy3Z_V",
        "outputId": "c3af97f1-40b0-470b-9a49-dc160df54776"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     id  year                                              title event_type  \\\n",
              "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
              "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
              "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
              "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
              "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
              "\n",
              "                                            pdf_name          abstract  \\\n",
              "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
              "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
              "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
              "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
              "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
              "\n",
              "                                          paper_text  \n",
              "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4  Neural Network Ensembles, Cross\\nValidation, a...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ec136501-5fcb-4ab9-b39e-5c8ddf1144c3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec136501-5fcb-4ab9-b39e-5c8ddf1144c3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ec136501-5fcb-4ab9-b39e-5c8ddf1144c3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ec136501-5fcb-4ab9-b39e-5c8ddf1144c3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ef2dc967-7976-4252-b621-86542910481d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ef2dc967-7976-4252-b621-86542910481d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ef2dc967-7976-4252-b621-86542910481d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "papers",
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 6560,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1901,\n        \"min\": 1,\n        \"max\": 6603,\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          3087,\n          78,\n          5412\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1987,\n        \"max\": 2016,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          1992,\n          1990,\n          2012\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          \"Natural Actor-Critic for Road Traffic Optimisation\",\n          \"Learning Representations by Recirculation\",\n          \"Quantized Kernel Learning for Feature Matching\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Oral\",\n          \"Spotlight\",\n          \"Poster\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          \"3087-natural-actor-critic-for-road-traffic-optimisation.pdf\",\n          \"78-learning-representations-by-recirculation.pdf\",\n          \"5412-quantized-kernel-learning-for-feature-matching.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3244,\n        \"samples\": [\n          \"Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in statistical learning of latent variable models and in data mining. In this paper, we propose fast and randomized tensor CP decomposition algorithms based on sketching. We build on the idea of count sketches, but introduce many novel ideas which are unique to tensors. We develop novel methods for randomized com- putation of tensor contractions via FFTs, without explicitly forming the tensors. Such tensor contractions are encountered in decomposition methods such as ten- sor power iterations and alternating least squares. We also design novel colliding hashes for symmetric tensors to further save time in computing the sketches. We then combine these sketching ideas with existing whitening and tensor power iter- ative techniques to obtain the fastest algorithm on both sparse and dense tensors. The quality of approximation under our method does not depend on properties such as sparsity, uniformity of elements, etc. We apply the method for topic mod- eling and obtain competitive results.\",\n          \"Many spectral unmixing methods rely on the non-negative decomposition of spectral data onto a dictionary of spectral templates. In particular, state-of-the-art music transcription systems decompose the spectrogram of the input signal onto a dictionary of representative note spectra. The typical measures of fit used to quantify the adequacy of the decomposition compare the data and template entries frequency-wise. As such, small displacements of energy from a frequency bin to another as well as variations of timber can disproportionally harm the fit. We address these issues by means of optimal transportation and propose a new measure of fit that treats the frequency distributions of energy holistically as opposed to frequency-wise. Building on the harmonic nature of sound, the new measure is invariant to shifts of energy to harmonically-related frequencies, as well as to small and local displacements of energy. Equipped with this new measure of fit, the dictionary of note templates can be considerably simplified to a set of Dirac vectors located at the target fundamental frequencies (musical pitch values). This in turns gives ground to a very fast and simple decomposition algorithm that achieves state-of-the-art performance on real musical data.\",\n          \"The problem of  multiclass boosting is considered. A new framework,based on multi-dimensional codewords and predictors is introduced. The optimal set of codewords is derived, and a margin enforcing loss proposed. The resulting risk is minimized by gradient descent on a multidimensional functional space. Two algorithms are proposed: 1) CD-MCBoost, based on coordinate descent, updates one predictor component at a time, 2) GD-MCBoost, based on gradient descent, updates all components jointly. The algorithms differ in the weak learners that they support but are both shown to be 1) Bayes consistent, 2) margin enforcing, and 3) convergent to the global minimum of the risk. They also reduce to AdaBoost when there are only two classes. Experiments show that both methods outperform previous multiclass boosting approaches on a number of datasets.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6553,\n        \"samples\": [\n          \"550\\n\\nAckley and Littman\\n\\nGeneralization and scaling in reinforcement\\nlearning\\nDavid H. Ackley\\nMichael L. Littman\\nCognitive Science Research Group\\nBellcore\\nMorristown, NJ 07960\\n\\nABSTRACT\\nIn associative reinforcement learning, an environment generates input\\nvectors, a learning system generates possible output vectors, and a reinforcement function computes feedback signals from the input-output\\npairs. The task is to discover and remember input-output pairs that\\ngenerate rewards. Especially difficult cases occur when rewards are\\nrare, since the expected time for any algorithm can grow exponentially\\nwith the size of the problem. Nonetheless, if a reinforcement function\\npossesses regularities, and a learning algorithm exploits them, learning\\ntime can be reduced below that of non-generalizing algorithms. This\\npaper describes a neural network algorithm called complementary reinforcement back-propagation (CRBP), and reports simulation results\\non problems designed to offer differing opportunities for generalization.\\n\\n1\\n\\nREINFORCEMENT LEARNING REQUIRES SEARCH\\n\\nReinforcement learning (Sutton, 1984; Barto & Anandan, 1985; Ackley, 1988; Allen,\\n1989) requires more from a learner than does the more familiar supervised learning\\nparadigm. Supervised learning supplies the correct answers to the learner, whereas\\nreinforcement learning requires the learner to discover the correct outputs before\\nthey can be stored. The reinforcement paradigm divides neatly into search and\\nlearning aspects: When rewarded the system makes internal adjustments to learn\\nthe discovered input-output pair; when punished the system makes internal adjustments to search elsewhere.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n1.1\\n\\nMAKING REINFORCEMENT INTO ERROR\\n\\nFollowing work by Anderson (1986) and Williams (1988), we extend the backpropagation algorithm to associative reinforcement learning. Start with a \\\"garden variety\\\" backpropagation network: A vector i of n binary input units propagates\\nthrough zero or more layers of hidden units, ultimately reaching a vector 8 of m\\nsigmoid units, each taking continuous values in the range (0,1). Interpret each 8j\\nas the probability that an associated random bit OJ takes on value 1. Let us call\\nthe continuous, deterministic vector 8 the search vector to distinguish it from the\\nstochastic binary output vector o.\\nGiven an input vector, we forward propagate to produce a search vector 8, and\\nthen perform m independent Bernoulli trials to produce an output vector o. The\\ni - 0 pair is evaluated by the reinforcement function and reward or punishment\\nensues. Suppose reward occurs. We therefore want to make 0 more likely given i.\\nBackpropagation will do just that if we take 0 as the desired target to produce an\\nerror vector (0 - 8) and adjust weights normally.\\nNow suppose punishment occurs, indicating 0 does not correspond with i. By choice\\nof error vector, backpropagation allows us to push the search vector in any direction;\\nwhich way should we go? In absence of problem-specific information, we cannot pick\\nan appropriate direction with certainty. Any decision will involve assumptions. A\\nvery minimal \\\"don't be like 0\\\" assumption-employed in Anderson (1986), Williams\\n(1988), and Ackley (1989)-pushes s directly away from 0 by taking (8 - 0) as the\\nerror vector. A slightly stronger \\\"be like not-o\\\" assumption-employed in Barto &\\nAnandan (1985) and Ackley (1987)-pushes s directly toward the complement of 0\\nby taking ((1 - 0) - 8) as the error vector. Although the two approaches always\\nagree on the signs of the error terms, they differ in magnitudes. In this work,\\nwe explore the second possibility, embodied in an algorithm called complementary\\nreinforcement back-propagation ( CRBP).\\nFigure 1 summarizes the CRBP algorithm. The algorithm in the figure reflects three\\nmodifications to the basic approach just sketched. First, in step 2, instead of using\\nthe 8j'S directly as probabilities, we found it advantageous to \\\"stretch\\\" the values\\nusing a parameter v. When v < 1, it is not necessary for the 8i'S to reach zero or\\none to produce a deterministic output. Second, in step 6, we found it important\\nto use a smaller learning rate for punishment compared to reward. Third, consider\\nstep 7: Another forward propagation is performed, another stochastic binary output vector 0* is generated (using the procedure from step 2), and 0* is compared\\nto o. If they are identical and punishment occurred, or if they are different and\\nreward occurred, then another error vector is generated and another weight update\\nis performed. This loop continues until a different output is generated (in the case\\nof failure) or until the original output is regenerated (in the case of success). This\\nmodification improved performance significantly, and added only a small percentage\\nto the total number of weight updates performed.\\n\\n551\\n\\n\\f552\\n\\nAckley and Littman\\n\\nO. Build a back propagation network with input dimensionality n and output\\ndimensionality m. Let t = 0 and te = O.\\n1. Pick random i E 2n and forward propagate to produce a/s.\\n2. Generate a binary output vector o. Given a uniform random variable ~ E [0,1]\\nand parameter 0 < v < 1,\\nOJ\\n\\n=\\n\\n{1,\\n\\n0,\\n\\nif(sj - !)/v+! ~ ~j\\notherwise.\\n\\n3. Compute reinforcement r = f(i,o). Increment t. If r < 0, let te = t.\\n4. Generate output errors ej. If r > 0, let tj = OJ, otherwise let tj = 1- OJ. Let\\nej = (tj - sj)sj(l- Sj).\\n5. Backpropagate errors.\\n6. Update weights. 1:::..Wjk = 1]ekSj, using 1] = 1]+ if r ~ 0, and 1] = 1]- otherwise,\\nwith parameters 1]+,1]- > o.\\n7. Forward propagate again to produce new Sj's. Generate temporary output\\nvector 0*. If (r > 0 and 0* #- 0) or (r < 0 and 0* = 0), go to 4.\\n8. If te ~ t, exit returning te, else go to 1.\\n\\nFigure 1: Complementary Reinforcement Back Propagation-CRBP\\n\\n2\\n\\nON-LINE GENERALIZATION\\n\\nWhen there are many possible outputs and correct pairings are rare, the computational cost associated with the search for the correct answers can be profound.\\nThe search for correct pairings will be accelerated if the search strategy can effectively generalize the reinforcement received on one input to others. The speed of\\nan algorithm on a given problem relative to non-generalizing algorithms provides a\\nmeasure of generalization that we call on-line generalization.\\nO. Let z be an array of length 2n. Set the z[i] to random numbers from 0 to\\n2m - 1. Let t = te = O.\\n1. Pick a random input i E 2n.\\n2. Compute reinforcement r = f(i, z[i]). Increment t.\\n3. If r < 0 let z[i] = (z[i] + 1) mod 2m , and let te = t.\\n4. If te <t:: t exit returning t e, else go to 1.\\n\\nFigure 2: The Table Lookup Reference Algorithm Tref(f, n, m)\\nConsider the table-lookup algorithm Tref(f, n, m) summarized in Figure 2. In this\\nalgorithm, a separate storage location is used for each possible input. This prevents\\nthe memorization of one i - 0 pair from interfering with any other. Similarly,\\nthe selection of a candidate output vector depends only on the slot of the table\\ncorresponding to the given input. The learning speed of T ref depends only on the\\ninput and output dimensionalities and the number of correct outputs associated\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\nwith each input. When a problem possesses n input bits and n output bits, and\\nthere is only one correct output vector for each input vector, Tre{ runs in about 4n\\ntime (counting each input-output judgment as one.) In such cases one expects to\\ntake at least 2n - 1 just to find one correct i - 0 pair, so exponential time cannot be\\navoided without a priori information. How does a generalizing algorithm such as\\nCRBP compare to Trer?\\n\\n3\\n\\nSIMULATIONS ON SCALABLE PROBLEMS\\n\\nWe have tested CRBP on several simple problems designed to offer varying degrees\\nand types of generalization. In all of the simulations in this section, the following\\ndetails apply: Input and output bit counts are equal (n). Parameters are dependent\\non n but independent of the reinforcement function f. '7+ is hand-picked for each\\nn,l 11- = 11+/10 and II = 0.5. All data points are medians of five runs. The stopping\\ncriterion te ~ t is interpreted as te +max(2000, 2n+l) < t. The fit lines in the figures\\nare least squares solutions to a x bn , to two significant digits.\\nAs a notational convenience, let c = ~\\n\\n3.1\\n\\nn\\n\\nE ij\\n\\n;=1\\n\\n-\\n\\nthe fraction of ones in the input.\\n\\nn-MAJORlTY\\n\\nConsider this \\\"majority rules\\\" problem: [if c > ~ then 0 = In else 0 = on]. The i-o\\nmapping is many-to-l. This problem provides an opportunity for what Anderson\\n(1986) called \\\"output generalization\\\": since there are only two correct output states,\\nevery pair of output bits are completely correlated in the cases when reward occurs.\\n\\nG)\\n\\n'iii\\nu\\nrn\\n\\nC)\\n\\n0\\n\\n::::.\\nG)\\n\\nE\\n\\n;\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n\\nTable\\n\\nD\\n\\nCRBP n-n-n\\n\\n+ CRBP n-n\\n\\n10 3\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n456\\n\\n78\\n\\n91011121314\\n\\nn\\nFigure 3: The n-majority problem\\n\\nFigure 3 displays the simulation results. Note that although Trer is faster than\\nCRBP at small values of n, CRBP's slower growth rate (1.6n vs 4.2n ) allows it to\\ncross over and begin outperforming Trer at about 6 bits. Note also--in violation of\\n1 For n = 1 to 12. we used '1+\\n0.219. 0.170. 0.121}.\\n\\n= {2.000. 1.550. 1.130.0.979.0.783.0.709.0.623.0.525.0.280.\\n\\n553\\n\\n\\f554\\n\\nAckley and Littman\\n\\nsome conventional wisdom-that although n-majority is a linearly separable problem, the performance of CRBP with hidden units is better than without. Hidden\\nunits can be helpful--even on linearly separable problems-when there are opportunities for output generalization.\\n\\n3.2\\n\\nn-COPY AND THE 2k -ATTRACTORS FAMILY\\n\\nAs a second example, consider the n-copy problem: [0 = i]. The i-o mapping is now\\n1-1, and the values of output bits in rewarding states are completely uncorrelated,\\nbut the value of each output bit is completely correlated with the value of the\\ncorresponding input bit. Figure 4 displays the simulation results. Once again, at\\n\\nG)\\n\\n'ii\\n\\ntA\\nQ\\n0\\n\\n::::.\\nG)\\n\\n-\\n\\n.5\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n150*2.0I\\\\n\\n\\nD\\n\\n10 3\\n10 2\\n\\n12*2.2I\\\\n\\n\\n+\\n\\nTable\\nCRBP n-n-n\\nCRBP n-n\\n\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 4: The n-copy problem\\nlow values of n, Trer is faster, but CRBP rapidly overtakes Trer as n increases. In\\nn-copy, unlike n-majority, CRBP performs better without hidden units.\\nThe n-majority and n-copy problems are extreme cases of a spectrum. n-majority\\ncan be viewed as a \\\"2-attractors\\\" problem in that there are only two correct\\noutputs-all zeros and all ones-and the correct output is the one that i is closer\\nto in hamming distance. By dividing the input and output bits into two groups\\nand performing the majority function independently on each group, one generates\\na \\\"4-aUractors\\\" problem. In general, by dividing the input and output bits into\\n1 ~ Ie ~ n groups, one generates a \\\"2i:-attractors\\\" problem. When Ie = 1, nmajority results, and when Ie n, n-copy results.\\n\\n=\\n\\nFigure 5 displays simulation results on the n = 8-bit problems generated when Ie is\\nvaried from 1 to n. The advantage of hidden units for low values of Ie is evident,\\nas is the advantage of \\\"shortcut connections\\\" (direct input-to-output weights) for\\nlarger values of Ie. Note also that combination of both hidden units and shortcut\\nconnections performs better than either alone.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n105~--------------------------------~\\n\\nCASP 8-10-8\\n-+- CASP 8-8\\n.... CASP 8-10-Sls\\n-0-\\n\\n... Table\\n\\n3\\n\\n2\\n\\n1\\n\\n5\\n\\n4\\n\\n7\\n\\n6\\n\\n8\\n\\nk\\n\\nFigure 5: The 21:- attractors family at n = 8\\n\\n3.3\\n\\nn-EXCLUDED MIDDLE\\n\\nAll of the functions considered so far have been linearly separable. Consider this\\n\\\"folded majority\\\" function: [if\\n< c < then 0 on else 0 In]. Now, like\\nn-majority, there are only two rewarding output states, but the determination of\\nwhich output state is correct is not linearly separable in the input space. When\\nn = 2, the n-excluded middle problem yields the EQV (i.e., the complement of\\nXOR) function, but whereas functions such as n-parity [if nc is even then 0\\non\\nelse 0 = In] get more non-linear with increasing n, n-excluded middle does not.\\n\\ni\\n\\ni\\n\\n=\\n\\n=\\n\\n=\\n\\n107~------------------------------~~\\n\\n-\\n\\n10 6\\n10 5\\n\\nD)\\n\\n10 4\\n10 3\\n\\nI)\\n\\n'ii\\nu\\nf)\\n\\n.2\\n\\nI)\\n\\nE\\n\\n:::\\n\\nx\\nc\\n\\n17oo*1.6\\\"n\\n\\nTable\\n\\nCRSP n-n-n/s\\n\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 6: The n-excluded middle problem\\nFigure 6 displays the simulation results. CRBP is slowed somewhat compared to\\nthe linearly separable problems, yielding a higher \\\"cross over point\\\" of about 8 bits.\\n\\n555\\n\\n\\f556\\n\\nAckley and Littman\\n\\n4\\n\\nSTRUCTURING DEGENERATE OUTPUT SPACES\\n\\nAll of the scaling problems in the previous section are designed so that there is\\na single correct output for each possible input. This allows for difficult problems\\neven at small sizes, but it rules out an important aspect of generalizing algorithms\\nfor associative reinforcement learning: If there are multiple satisfactory outputs\\nfor given inputs, a generalizing algorithm may impose structure on the mapping it\\nproduces.\\nWe have two demonstrations of this effect, \\\"Bit Count\\\" and \\\"Inverse Arithmetic.\\\"\\nThe Bit Count problem simply states that the number of I-bits in the output should\\nequal the number of I-bits in the input. When n = 9, Tref rapidly finds solutions\\ninvolving hundreds of different output patterns. CRBP is slower--especially with\\nrelatively few hidden units-but it regularly finds solutions involving just 10 output\\npatterns that form a sequence from 09 to 19 with one bit changing per step.\\n0+Ox4=0\\n1+0x4=1\\n2+0x4=2\\n3+0x4=3\\n\\n0+2x4=8\\n1+2x4=9\\n2 + 2 x 4 = 10\\n3+2x4=11\\n\\n4+0x4=4 4+ 2 x 4 =\\n5+0x4=5 5 + 2 x 4 =\\n6+0x4=6 6 + 2 x 4 =\\n7+0x4=7 7 + 2 x 4 =\\n\\n12\\n13\\n14\\n15\\n\\n2+2-4=0 2+2+4=8\\n3+2-4=1 3+2+4=9\\n2+2+4=2 2 + 2 x 4 = 10\\n3+2+4=3 3+2x4=1l\\n6+2-4=4\\n7+2-4=5\\n6+2+4=6\\n7+2-.;-4=7\\n\\n6+\\n7+\\n6+\\n7+\\n\\n2+ 4 =\\n2+ 4 =\\n2x4=\\n2x4=\\n\\n0+4 x 4 = 16 0+6 x 4 =\\n1+4x4=17 1 + 6 x 4 =\\n2 + 4 x 4 = 18 2 + 6 x 4 =\\n3 +4 x 4 = 19 3 + 6 x 4 =\\n\\n24\\n25\\n26\\n27\\n\\n4+4\\n5+ 4\\n6+ 4\\n7+ 4\\n\\n=\\n=\\n=\\n=\\n\\n28\\n29\\n30\\n31\\n24\\n25\\n26\\n27\\n\\nx\\nx\\nx\\nx\\n\\n4=\\n4=\\n4=\\n4=\\n\\n6+ 6 + 4 =\\n7+6+4=\\n2+ 4 x 4 =\\n3+ 4 x 4=\\n\\n12 4 x 4 +\\n13 5 + 4 x\\n14 6 + 4 x\\n15 7 +4 x\\n\\n4=\\n4=\\n4\\n4=\\n\\n=\\n\\n20 4 + 6 x\\n21 5 + 6 x\\n22 6 + 6 x\\n23 7 + 6 x\\n\\n4\\n4\\n4\\n4\\n\\n16\\n17\\n18\\n19\\n\\n0+6 x\\n1+ 6 x\\n2+ 6x\\n3+ 6x\\n\\n4=\\n4=\\n4=\\n4=\\n\\n20\\n21\\n22\\n23\\n\\n4+\\n5+\\n6+\\n7+\\n\\n4 = 28\\n4 = 29\\n4 30\\n4 = 31\\n\\n6\\n6\\n6\\n6\\n\\nx\\nx\\nx\\nx\\n\\n=\\n\\nFigure 7: Sample CRBP solutions to Inverse Arithmetic\\n\\nThe Inverse Arithmetic problem can be summarized as follows: Given i E 25 , find\\n:1:, y, z E 23 and 0, <> E {+(OO)' -(01)' X (10)' +(11)} such that :I: oy<>z = i. In all there are\\n13 bits of output, interpreted as three 3-bit binary numbers and two 2-bit operators,\\nand the task is to pick an output that evaluates to the given 5-bit binary input\\nunder the usual rules: operator precedence, left-right evaluation, integer division,\\nand division by zero fails.\\nAs shown in Figure 7, CRBP sometimes solves this problem essentially by discovering positional notation, and sometimes produces less-globally structured solutions,\\nparticularly as outputs for lower-valued i's, which have a wider range of solutions.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n5\\n\\nCONCLUSIONS\\n\\nSome basic concepts of supervised learning appear in different guises when the\\nparadigm of reinforcement learning is applied to large output spaces. Rather than\\na \\\"learning phase\\\" followed by a \\\"generalization test,\\\" in reinforcement learning\\nthe search problem is a generalization test, performed simultaneously with learning.\\nInformation is put to work as soon as it is acquired.\\nThe problem of of \\\"overfitting\\\" or \\\"learning the noise\\\" seems to be less of an issue,\\nsince learning stops automatically when consistent success is reached. In experiments not reported here we gradually increased the number of hidden units on\\nthe 8-bit copy problem from 8 to 25 without observing the performance decline\\nassociated with \\\"too many free parameters.\\\"\\nThe 2 k -attractors (and 2 k -folds-generalizing Excluded Middle) families provide\\na starter set of sample problems with easily understood and distinctly different\\nextreme cases.\\nIn degenerate output spaces, generalization decisions can be seen directly in the\\ndiscovered mapping. Network analysis is not required to \\\"see how the net does it.\\\"\\nThe possibility of ultimately generating useful new knowledge via reinforcement\\nlearning algorithms cannot be ruled out.\\nReferences\\nAckley, D.H. (1987) A connectionist machine for genetic hillclimbing. Boston, MA: Kluwer\\nAcademic Press.\\nAckley, D.H. (1989) Associative learning via inhibitory search. In D.S. Touretzky (ed.),\\nAdvances in Neural Information Processing Systems 1, 20-28. San Mateo, CA: Morgan\\nKaufmann.\\nAllen, R.B. (1989) Developing agent models with a neural reinforcement technique. IEEE\\nSystems, Man, and Cybernetics Conference. Cambridge, MA.\\nAnderson, C.W. (1986) Learning and problem solving with multilayer connectionist systems. University of Mass. Ph.D. dissertation. COINS TR 86-50. Amherst, MA.\\nBarto, A.G. (1985) Learning by statistical cooperation of self-interested neuron-like computing elements. Human Neurobiology, 4:229-256.\\nBarto, A.G., & Anandan, P. (1985) Pattern recognizing stochastic learning automata.\\nIEEE Transactions on Systems, Man, and Cybernetics, 15, 360-374.\\nRumelhart, D.E., Hinton, G.E., & Williams, R.J. (1986) Learning representations by backpropagating errors. Nature, 323, 533-536.\\nSutton, R.S. (1984) Temporal credit assignment in reinforcement learning. University of\\nMass. Ph.D. dissertation. COINS TR 84-2. Amherst, MA.\\nWilliams, R.J. (1988) Toward a theory of reinforcement-learning connectionist systems.\\nCollege of Computer Science of Northeastern University Technical Report NU-CCS-88-3.\\nBoston, MA.\\n\\n557\\n\\n\\f\",\n          \"Dynamics of Supervised Learning with\\nRestricted Training Sets and Noisy Teachers\\n\\nA.C.C. Coolen\\nDept of Mathematics\\nKing's College London\\nThe Strand, London WC2R 2LS, UK\\ntcoolen@mth.kc1.ac.uk\\n\\nC.W.H.Mace\\nDept of Mathematics\\nKing's College London\\nThe Strand, London WC2R 2LS, UK\\ncmace@mth.kc1.ac.uk\\n\\nAbstract\\nWe generalize a recent formalism to describe the dynamics of supervised\\nlearning in layered neural networks, in the regime where data recycling\\nis inevitable, to the case of noisy teachers. Our theory generates reliable\\npredictions for the evolution in time of training- and generalization errors, and extends the class of mathematically solvable learning processes\\nin large neural networks to those situations where overfitting can occur.\\n\\n1 Introduction\\nTools from statistical mechanics have been used successfully over the last decade to study\\nthe dynamics of learning in layered neural networks (for reviews see e.g. [1] or [2]). The\\nsimplest theories result upon assuming the data set to be much larger than the number\\nof weight updates made, which rules out recycling and ensures that any distribution of\\nrelevance will be Gaussian. Unfortunately, both in terms of applications and in terms of\\nmathematical interest, this regime is not the most relevant one. Most complications and\\npeculiarities in the dynamics of learning arise precisely due to data recycling, which creates\\nfor the system the possibility to improve performance by memorizing answers rather than\\nby learning an underlying rule. The dynamics of learning with restricted training sets was\\nfirst studied analytically in [3] (linear learning rules) and [4] (systems with binary weights).\\nThe latter studies were ahead of their time, and did not get the attention they deserved just\\nbecause at that stage even the simpler learning dynamics without data recycling had not\\nyet been studied. More recently attention has moved back to the dynamics of learning\\nin the recycling regime. Some studies aimed at developing a general theory [5, 6, 7],\\nsome at finding exact solutions for special cases [8]. All general theories published so far\\nhave in common that they as yet considered realizable scenario's: the rule to be learned\\nwas implementable by the student, and overfitting could not yet occur. The next hurdle is\\nthat where restricted training sets are combined with unrealizable rules. Again some have\\nturned to non-typical but solvable cases, involving Hebbian rules and noisy [9] or 'reverse\\nwedge' teachers [10]. More recently the cavity method has been used to build a general\\ntheory [11] (as yet for batch learning only). In this paper we generalize the general theory\\nlaunched in [6,5,7], which applies to arbitrary learning rules, to the case of noisy teachers.\\nWe will mirror closely the presentation in [6] (dealing with the simpler case of noise-free\\nteachers), and we refer to [5, 7] for background reading on the ideas behind the formalism.\\n\\n\\fA. C. C. Coolen and C. W. H. Mace\\n\\n238\\n\\n2 Definitions\\nAs in [6, 5] we restrict ourselves for simplicity to perceptrons. A student perceptron operates a linear separation, parametrised by a weight vector J E iRN :\\nS:{-I,I}N -t{-I,I}\\n\\nS(e) = sgn[J?e]\\n\\nIt aims to emulate a teacher o~erating a similar rule, which, however, is characterized by a\\nvariable weight vector BE iR ,drawn at random from a distribution P(B) such as\\nP(B) = >'6[B+B*]\\n\\noutput noise:\\n\\n+ (1->')6[B-B*]\\n\\n(1)\\n\\nP(B) = [~~/NrN e- tN (B-B')2/E2\\n(2)\\nThe parameters>. and ~ control the amount of teacher noise, with the noise-free teacher\\nB = B* recovered in the limits>. -t 0 and ~ -t O. The student modifies J iteratively, using\\nexamples of input vectors which are drawn at random from a fixed (randomly composed)\\nE {-I, I}N with a> 0, and the corresponding\\ntraining set containing p = aN vectors\\nvalues of the teacher outputs. We choose the teacher noise to be consistent, i.e. the answer\\nwill remain the same when that particular question\\ngiven by the teacher to a question\\nre-appears during the learning process. Thus T(e?) = sgn[BJL . e], with p teacher weight\\nvectors BJL, drawn randomly and independently from P(B), and we generalize the training\\nl , B l ), . .. , (e, BP)}. Consistency of teacher noise is natural\\nset accordingly to jj =\\nin terms of applications, and a prerequisite for overfitting phenomena. Averages over the\\ntraining set will be denoted as ( ... ) b; averages over all possible input vectors E {-I, I}N\\nas ( ... )e. We analyze two classes of learning rules, of the form J (? + 1) = J (?) + f).J (?):\\n\\nGaussian weight noise:\\n\\ne\\n\\ne\\n\\ne\\n\\nHe\\n\\ne\\n\\n= 11 {e(?) 9 [J(?)?e(?), B(?)?e(?)] - ,J(?) }\\nf).J(?) = 11 {(e 9 [J(?)?e, B?eDl> - ,J(m) }\\n\\non-line:\\n\\nf).J(?)\\n\\nbatch :\\n\\n(3)\\n\\nIn on-line learning one draws at each step ? a question/answer pair (e (?), B (?)) at random from the training set. In batch learning one iterates a deterministic map which is an\\naverage over all data in the training set. Our performance measures are the training- and\\ngeneralization errors, defined as follows (with the step function O[x > 0] = 1, O[x < 0] = 0):\\nEt(J)\\n\\n= (O[-(J ?e)(B ?em b\\n\\nEg(J)\\n\\n= (O[-(J ?e)(B* ?e)])e\\n\\n(4)\\n\\nWe introduce macroscopic observables, taylored to the present problem, generalizing [5, 6]:\\nQ[J]=J 2,\\nR[J]=J?B*,\\nP[x,y,z;J]=(6[x-J?e]6[y-B*?e]6[z-B?eDl> (5)\\nAs in [5, 6] we eliminate technical subtleties by assuming the number of arguments (x, y, z)\\nfor which P[x, y, z; J] is evaluated to go to infinity after the limit N -t 00 has been taken.\\n\\n3 Derivation of Macroscopic Laws\\nUpon generalizing the calculations in [6, 5], one finds for on-line learning:\\n\\n!\\n!\\n\\nQ = 2'f} !dXdydZ P[x, y, z] xg[x, z] - 2'f},Q + 'f}2!dXdYdZ P[x, y, z] g2[x, z]\\n\\n(6)\\n\\nR = 'f} !dXdydZ P[x, y, z] y9[x, z]- 'f},R\\n\\n(7)\\n\\n:t\\n\\nP[x, y, z] =\\n\\n~\\n\\n!\\n\\ndx' P[x', y, z] {6[x-x' -'f}G[x', z]] -6[x-x']}\\n\\n-'f}! / dx'dy'dz' / dx'dy'dz'9[x', z]A[x, y, z; x',y', z']\\n\\n1\\n+'i'f}2\\n\\n!\\n\\n+ 'f}, :x\\n\\nEP2P[x, y, z]\\ndx'dy'dz' P[x', y', z']92[x', z'] 8x\\n\\n{xP[x , y, z]}\\n\\n(8)\\n\\n\\fSupervised Learning with Restricted Training Sets\\n\\n239\\n\\nThe complexity of the problem is concentrated in a Green's function:\\nA[x, y, Zj x', y', z'] = lim\\nN-+oo\\n\\n(( ([1-6ee , ]6[x-J?e]6[y-B*?e]6[z-B?e] (e?e')6[x' -J?e']6[y' - B*?e']6[y' - B?e'])i?i> )QW;t\\n\\nJ\\n\\nIt involves a conditional average of the form (K[J])QW;t = dJ Pt(JIQ,R,P)K[J], with\\nPt(J) 6[Q-Q[J]]6[R- R[J]] nXYZ 6[P[x, y, z] -P[x, y, Zj J]]\\nPt(JIQ,R,P)\\nJdJ Pt(J) 6[Q - Q[J]]6[R- R[J]] nXYZ 6[P[x, y, z] - P[x, y, z; J]]\\n\\n=\\n\\nin which Pt (J) is the weight probability density at time t. The solution of (6,7,8) can be\\nused to generate the N -+ 00 performance measures (4) at any time:\\nEt\\n\\n=/\\n\\ndxdydz P[x, y, z]O[-xz]\\n\\nEg\\n\\n= 11\\\"-1 arccos[RIVQ]\\n\\n(9)\\n\\nExpansion of these equations in powers of\\\"\\\" and retaining only the terms linear in \\\"\\\" gives\\nthe corresponding equations describing batch learning. So far this analysis is exact.\\n\\n4\\n\\nClosure of Macroscopic Laws\\n\\nAs in [6, 5] we close our macroscopic laws (6,7,8) by making the two key assumptions\\nunderlying dynamical replica theory:\\n(i) For N -+ 00 our macroscopic observables obey closed dynamic equations.\\n(ii) These equations are self-averaging with respect to the specific realization of D.\\n\\n(i) implies that probability variations within {Q, R, P} subshells are either absent or irrelevant to the macroscopic laws. We may thus make the simplest choice for Pt (J IQ, R, P):\\nPt(JIQ,R,P) -+ 6[Q-Q[J]] 6[R-R[J]]\\n\\nII 6[P[x,y,z]-P[x,y,ZjJ]]\\n\\n(10)\\n\\nxyz\\n\\nThe procedure (10) leads to exact laws if our observables {Q, R, P} indeed obey closed\\nequations for N -+ 00. It is a maximum entropy approximation if not. (ii) allows us\\nto average the macroscopic laws over all training sets; it is observed in simulations, and\\nproven using the formalism of [4]. Our assumptions (10) result in the closure of (6,7,8),\\nsince now the Green's function can be written in terms of {Q, R, Pl. The final ingredient\\nof dynamical replica theory is doing the average of fractions with the replica identity\\n\\n/ JdJ W[JID]GIJID])\\n\\n\\\\\\n\\nJdJ W[JID]\\n\\n= lim\\nsets\\n\\n/dJ I\\n\\n???\\n\\ndJn (G[J 1 ID]\\n\\nn-+O\\n\\nIT\\n\\nW[JO<ID])sets\\n\\na=1\\n\\nOur problem has been reduced to calculating (non-trivial) integrals and averages. One\\nfinds that P[x, y, z] P[x, zly]P[y] with Ply] (211\\\")-!exp[-!y 21With the short-hands\\nDy = P[y]dy and (f(x, y, z)) = Dydxdz P[x, zly]f(x, y, z) we can write the resulting\\nmacroscopic laws, for the case of output noise (1), in the following compact way:\\n\\n=\\n\\nd\\n\\ndt Q = 2\\\",(V - ,Q)\\n\\n[)\\n\\n[)tP[x,zly] =\\n\\n=\\n\\nJ\\n\\n+ rJ2 Z\\n\\nd\\n\\ndtR = \\\",(W - ,R)\\n\\n(11)\\n\\n1 [)x[)22P[x,zIY]\\na1/dx'P[x',zly] {6[x-x'-\\\",G[x',z]]-6[x-x'] }+2\\\",2Z\\n\\n-\\\",:x {P[x,zly]\\n\\n[U(x-RY)+Wy-,x+[V-RW-(Q-R2)U]~[x,y,z])}\\n\\n(12)\\n\\nwith\\n\\nU = (~[x, y, z]9[x, z]),\\n\\nv = (x9[x, z]),\\n\\nW = (y9[x, z]),\\n\\nZ = (9 2[x, z])\\n\\nThe solution of (12) is at any time of the following form:\\n\\nP[x,zly]\\n\\n= (1-,x)6[y-z]P+[xly] + ,x6[y+z]P-[xly]\\n\\n(13)\\n\\n\\fA. C. C. Coolen and C. W. H. Mace\\n\\n240\\n\\nFinding the function <I> [x, y, z] (in replica symmetric ansatz) requires solving a saddle-point\\nproblem for a scalar observable q and two functions M?[xly]. Upon introducing\\n\\nB = . . :. V. .,. .q.,-Q___R,-2\\nQ(I-q)\\n(with Jdx M?[xly]\\n\\nJdx M?[xly]eBxs J[x, y]\\nJdx M?[xly]eBxs\\n\\n(f[x, y])? =\\n*\\n\\n= 1 for all y) the saddle-point equations acquire the fonn\\np?[Xly] =\\n\\nfor all X, y :\\n\\n((x-Ry)2) + (qQ-R 2)[I-!:.]\\na\\n\\n!\\n\\nDs (O[X -xl);\\n\\n2 !DYDS S[(I-A)(X); + A(X);]\\n= qQ+Q-2R\\n..jqQ_R2\\n\\n(14)\\n(15)\\n\\nThe equations (14) which detennine M?[xly] have the same structure as the corresponding\\n(single) equation in [5, 6], so the proofs in [5, 6] again apply, and the solutions M?[xly],\\ngiven a q in the physical range q E [R2/Q, 1], are unique. The function <I> [x, y, z] is then\\ngiven by\\n<I> [X,\\n\\ny, z]\\n\\n=!\\n\\nDs s\\n{(I-A)O[Z-y](o[X -x)); + AO[Z+Y](o[X -xl);}\\n..jqQ_R2 P[X, zly]\\n(16)\\n\\nWorking out predictions from these equations is generally CPU-intensive, mainly due to\\nthe functional saddle-point equation (14) to be solved at each time step. However, as in [7]\\none can construct useful approximations of the theory, with increasing complexity:\\n\\n(i) Large a approximation (giving the simplest theory, without saddle-point equations)\\n(ii) Conditionally Gaussian approximation for M[xly] (with y-dependent moments)\\n(iii) Annealed approximation of the functional saddle-point equation\\n\\n5 Benchmark Tests: The Limits a --+ 00 and ,\\\\ --+ 0\\nWe first show that in the limit a --+ 00 our theory reduces to the simple (Q, R) formalism\\nof infinite training sets, as worked out for noisy teachers in [12]. Upon making the ansatz\\n\\np?[xly] = P[xly] = [27r(Q-R 2)]-t e- t [x- Rv]2/(Q-R 2)\\n\\n(17)\\n\\none finds\\n\\n<I>[x,y,Z] = (x-Ry)/(Q-R 2)\\n\\nM?[xly] = P[xly],\\n\\nInsertion of our ansatz into (12), followed by rearranging of terms and usage of the above\\nexpression for <I> [x, y, z], shows that (12) is satisfied. The remaining equations (11) involve\\nonly averages over the Gaussian distribution (17), and indeed reduce to those of [12]:\\n\\n~! Q =\\n\\n(I-A) { 2(x9[x, y))\\n1 d\\n--d R\\n1} t\\n\\n+ 1}{92[x, y)) } + A {2(x9[x,-y)) + 1}(92[x,-y)) } - 2,Q\\n\\n= (I-A)(y9[x,y)) + A(y9[x,-yl) -,R\\n\\nNext we turn to the limit A --+ 0 (restricted training sets & noise-free teachers) and show that\\nhere our theory reproduces the fonnalism of [6,5]. Now we make the following ansatz:\\n\\nP+[xly] = P[xly],\\n\\nP[x, zly]\\n\\n= o[z-y]P[xIY]\\n\\n(18)\\n\\nInsertion shows that for A = 0 solutions of this fonn indeed solve our equations, giving\\n<p[x, y, z]--+ <I> [x, y] and M+[xly]\\nM[xly), and leaving us exactly with the fonnalism\\nof [6, 5] describing the case of noise-free teachers and restricted training sets (apart from\\nsome new tenns due to the presence of weight decay, which was absent in [6, 5]).\\n\\n=\\n\\n\\f241\\n\\nSupervised Learning with Restricted Training Sets\\n0. , r------~--__,\\n\\n0..4\\n\\n~-------_____I\\n\\n0..4\\n\\n11>=0.'\\n\\n0..3\\n\\na=4\\n\\n0. ,\\n\\n0..0.\\n\\n--\\n\\n, 0.\\n\\n0.2\\n\\n_ __ ___ _____ _\\n\\na= 1\\n\\n0;=1\\n\\n------- ---- -- --- -\\n\\n0.\\n\\n0;=2\\n\\n=-=\\n-\\n\\n0;=2\\n\\n- - ----- -\\n\\na=4\\na=4\\n\\n= =-=\\n--=-=--=-=--=-=-=-- -=-=-_oed\\n\\na=4\\n\\n,\\n\\n0;=2\\n\\n':::::========:::j\\n\\n0..3\\n\\n-- - ----\\n\\n0;=1\\n\\n:::---- - -----1\\n\\n0;=2\\n\\n0..2\\n\\n11>=0.'\\n\\n~-------~\\n\\n0;=1\\n\\n0.,\\n\\n11>=0,\\n\\n\\\"\\n\\n,\\n\\nno. I\\n\\n0.\\n\\n, 0.\\n\\n\\\"\\n\\nFigure 1: On-line Hebbian learning: conditionally Gaussian approximation versus exact\\nsolution in [9] (.,., = 1, ,X = 0.2). Left: \\\"I = 0.1, right: \\\"I = 0.5. Solid lines: approximated\\ntheory, dashed lines: exact result. Upper curves: Eg as functions of time (here the two\\ntheories agree), lower curves: E t as functions of time.\\n\\n6\\n\\nBenchmark Tests: Hebbian Learning\\n\\nThe special case of Hebbian learning, i.e. Q[x, z] = sgn(z), can be solved exactly at any\\ntime, for arbitrary {a, ,x, \\\"I} [9], providing yet another excellent benchmark for our theory.\\nFor batch execution of Hebbian learning the macroscopic laws are obtained upon expanding\\n(11,12) and retaining only those terms which are linear in.,.,. All integrations can now be\\ndone and all equations solved explicitly, resulting in U =0, Z = 1, W = (I-2,X)J2/7r, and\\n\\nQ\\n\\n= Qo e-2rryt +\\n\\n2Ro(I-2'x) e-17\\\"Yt[I_e-rrrt]\\n\\\"I\\n\\nf{ + [~(I-2,X)2+.!.]\\n\\nV:;\\n\\n7r\\n\\na\\n\\n[I-e- 17 \\\"Y tF\\n\\\"12\\n\\nR = Ro e- 17\\\"Y t +(I-2'x)J2/7r[I-e- 17\\\"Y t ]/\\\"I\\nq = [aR2+(I_e- 17\\\"Yt)2 i'l]/aQ\\np?[xIY] = [27r(Q-R2)] -t e-tlz-RH sgn(y)[1-e-\\\"..,t]/a\\\"Y]2/(Q-R2)\\n(19)\\nFrom these results, in tum, follow the performance measures Eg = 7r- 1 arccos[ R/ JQ) and\\n\\nE = ! - !(1-,X)!D\\n2\\n\\nt\\n\\n2\\n\\nerf[IYIR+[I-e- 77\\\"Y t ]/a\\\"l] + !,X!D erf[IYIR-[I-e- 17\\\"Y t ]/a\\\"l]\\nY\\nJ2(Q-R2)\\n2\\ny\\nJ2(Q-R2)\\n\\nComparison with the exact solution, calculated along the lines of [9] or, equivalently, obtained upon putting t ?\\nin [9], shows that the above expressions are all exact.\\n\\n.,.,-2\\n\\nFor on-line execution we cannot (yet) solve the functional saddle-point equation in general.\\nHowever, some analytical predictions can still be extracted from (11,12,13):\\n\\nQ = Qo e-217\\\"Yt + 2Ro(I-2,X) e-77\\\"Yt[I_e-17\\\"Yt]\\n\\\"I\\n\\nR = Ro e- 17\\\"Y t + (I-2,X)J2/7r[I-e- 17\\\"Y t ]/\\\"I\\n\\nJ\\n\\nf{ + [~(I-2,X)2+.!.]\\n\\nV:;\\n\\n7r\\n\\na\\n\\n[I_e- 17\\\"Y t ]2\\n\\\"12\\n\\n+ !L[I_e- 217\\\"Y t ]\\n2\\\"1\\n\\ndx xP?[xIY] = Ry ? sgn(y)[I-e- 17\\\"Y t ]/a\\\"l\\n\\nwith U =0, W = (I-2,X)J2/7r, V = W R+[I-e- 17\\\"Y t ]/a\\\"l, and Z = 1. Comparison with the\\nresults in [9] shows that the above expressions, and thus also that of E g , are all fully exact,\\nat any time. Observables involving P[x, y, z] (including the training error) are not as easily\\nsolved from our equations. Instead we used the conditionally Gaussian approximation\\n(found to be adequate for the noiseless Hebbian case [5, 6, 7]). The result is shown in\\nfigure 1. The agreement is reasonable, but significantly less than that in [6]; apparently\\nteacher noise adds to the deformation of the field distribution away from a Gaussian shape.\\n\\n\\f242\\n\\nA. C. C. Coolen and C. W H. Mac\\n\\n~\\n\\n0.6\\n\\n000000\\n\\n0.4\\n\\n0.4\\n\\nE\\n\\n~\\n\\n0.2\\n\\nI\\ni\\n0.0\\n\\n0\\n\\n4\\n\\n2\\n\\n6\\n\\n10\\n\\n0.0\\n\\n-3\\n\\n-2\\n\\n-I\\n\\n0\\nX\\n\\n0.6\\n\\nf\\n\\n0.4\\n\\n0.4 [\\n\\nE\\n0.2\\n\\n0.2\\n\\n0.0\\n\\nL-o!i6iIII.\\\"\\\"\\\"\\\"\\\"',-\\\"--~_~~_ _--'\\n\\n-3\\n\\n-2\\n\\n-I\\n\\n0\\n\\n2\\n\\n3\\n\\nX\\n\\n,=\\n\\nFigure 2: Large a approximation versus numerical simulations (with N = 10,000), for\\n0 and A = 0.2. Top row: Perceptron rule, with.,., = ~. Bottom row: Adatron rule,\\nwith.,., = ~. Left: training errors E t and generalisation errors Eg as functions of time, for\\naE {~, 1, 2}. Lines: approximated theory, markers: simulations (circles: E t , squares: Eg) .\\nRight: joint distributions for student field and teacher noise p?[x] = dy P[x, y, z = ?y]\\n(upper: P+[x], lower: P-[x]). Histograms: simulations, lines: approximated theory.\\n\\nJ\\n\\n7\\n\\nNon-Linear Learning Rules: Theory versus Simulations\\n\\nIn the case of non-linear learning rules no exact solution is known against which to test our\\nformalism, leaving numerical simulations as the yardstick. We have evaluated numerically\\nthe large a approximation of our theory for Perceptron learning, 9[x, z] = sgn(z)O[-xz],\\nand for Adatron learning, 9[x, z] = sgn(z)lzIO[-xz]. This approximation leads to the\\nfollowing fully explicit equation for the field distributions:\\n\\n1/\\n\\nd\\n-p?[xly]\\n= dt\\na\\n.\\n\\nWith\\n\\nU=\\n\\n' +1\\n\\ndx' p?[x'ly]{o[x-x'-.,.,.1'[x', ?y]] -o[x-x]}\\n\\n_ ~ {P[ I ] [W _\\n.,., 8\\nx y\\ny\\n\\nJ\\n\\nX\\n\\n~ p?[xly]\\n\\n_.,.,2 Z!:I 2\\n2\\nuX\\n\\n,X + U[X?(y)-RY]+(V-RW)[X-X?(y)]]}\\nQ _ R2\\n\\nDydx {(I-A)P+[xly][x-P(y)]9[x,Y]+AP-[xly][x-x-(y)]9[x,-y])\\nV =\\nW=\\nZ=\\n\\n!\\n1\\n1\\n\\nDydx x {(I-A)P+[xly]9[x, Y]+AP-[xly]9[x,-y])\\nDydx y {(1-A)P+[xly]9[x, Y]+AP-[xly]9[x,-y])\\n\\nDydx {(I-A)P+[xly]92[x, Y]+AP-[xly]9 2[x,-yJ)\\n\\n\\fSupervised Learning with Restricted Training Sets\\n\\n243\\n\\nJ\\n\\nand with the short-hands X?(y) = dx xP?[xly). The result of our comparison is shown\\nin figure 2. Note: E t increases monotonically with a, and Eg decreases monotonically\\nwith a, at any t. As in the noise-free formalism [7], the large a approximation appears to\\ncapture the dominant terms both for a -7 00 and for a -7 O. The predicting power of our\\ntheory is mainly limited by numerical constraints. For instance, the Adatron learning rule\\ngenerates singularities at x = 0 in the distributions P?[xly) (especially for small \\\"I) which,\\nalthough predicted by our theory, are almost impossible to capture in numerical solutions.\\n\\n8 Discussion\\nWe have shown how a recent theory to describe the dynamics of supervised learning with\\nrestricted training sets (designed to apply in the data recycling regime, and for arbitrary online and batch learning rules) [5, 6, 7] in large layered neural networks can be generalized\\nsuccessfully in order to deal also with noisy teachers. In our generalized approach the joint\\ndistribution P[x, y, z) for the fields of student, 'clean' teacher, and noisy teacher is taken to\\nbe a dynamical order parameter, in addition to the conventional observables Q and R. From\\nthe order parameter set {Q, R, P} we derive the generalization error Eg and the training\\nerror E t . Following the prescriptions of dynamical replica theory one finds a diffusion\\nequation for P[x, y, z], which we have evaluated by making the replica-symmetric ansatz.\\nWe have carried out several orthogonal benchmark tests of our theory: (i) for a -7 00 (no\\ndata recycling) our theory is exact, (ii) for A -7 0 (no teacher noise) our theory reduces\\nto that of [5, 6, 7], and (iii) for batch Hebbian learning our theory is exact. For on-line\\nHebbian learning our theory is exact with regard to the predictions for Q, R, Eg and the\\ny-dependent conditional averages Jdx xP?[xly), at any time, and a crude approximation\\nof our equations already gives reasonable agreement with the exact results [9] for E t . For\\nnon-linear learning rules (Perceptron and Adatron) we have compared numerical solution\\nof a simple large a aproximation of our equations to numerical simulations, and found\\nsatisfactory agreement. This paper is a preliminary presentation of results obtained in the\\nsecond stage of a research programme aimed at extending our theoretical tools in the arena\\nof learning dynamics, building on [5, 6, 7]. Ongoing work is aimed at systematic application of our theory and its approximations to various types of non-linear learning rules, and\\nat generalization of the theory to multi-layer networks.\\n\\nReferences\\n[1]\\n[2]\\n[3]\\n[4]\\n[5]\\n[6]\\n[7]\\n[8]\\n[9]\\n[10]\\n[11]\\n[12]\\n\\nMace C.W.H. and Coolen AC.C (1998), Statistics and Computing 8, 55\\nSaad D. (ed.) (1998), On-Line Learning in Neural Networks (Cambridge: CUP)\\nHertz J.A., Krogh A and Thorgersson G.I. (1989), J. Phys. A 22, 2133\\nHomerH. (1992a), Z. Phys. B 86, 291 and Homer H. (1992b), Z. Phys. B 87,371\\nCoolen A.C.C. and Saad D. (1998), in On-Line Learning in Neural Networks, Saad\\nD. (ed.), (Cambridge: CUP)\\nCoolen AC.C. and Saad D. (1999), in Advances in Neural Information Processing\\nSystems 11, Kearns D., Solla S.A., Cohn D.A (eds.), (MIT press)\\nCoolen A.C.C. and Saad D. (1999), preprints KCL-MTH-99-32 & KCL-MTH-99-33\\nRae H.C., Sollich P. and Coolen AC.C. (1999), in Advances in Neural Information\\nProcessing Systems 11, Kearns D., Solla S.A., Cohn D.A. (eds.), (MIT press)\\nRae H.C., Sollich P. and Coolen AC.C. (1999),J. Phys. A 32, 3321\\nInoue J.I. (1999) private communication\\nWong K.YM., Li S. and Tong YW. (1999),preprint cond-mat19909004\\nBiehl M., Riegler P. and Stechert M. (1995), Phys. Rev. E 52, 4624\\n\\n\\f\",\n          \"Predicting Action Content On-Line and in\\nReal Time before Action Onset ? an\\nIntracranial Human Study\\n\\nShengxuan Ye\\nCalifornia Institute of Technology\\nPasadena, CA\\nsye@caltech.edu\\n\\nUri Maoz\\nCalifornia Institute of Technology\\nPasadena, CA\\nurim@caltech.edu\\nIan Ross\\nHuntington Hospital\\nPasadena, CA\\nianrossmd@aol.com\\n\\nAdam Mamelak\\nCedars-Sinai Medical Center\\nLos Angeles, CA\\nadam.mamelak@cshs.org\\n\\nChristof Koch\\nCalifornia Institute of Technology\\nPasadena, CA\\nAllen Institute for Brain Science\\nSeattle, WA\\nkoch@klab.caltech.edu\\n\\nAbstract\\nThe ability to predict action content from neural signals in real time before the action occurs has been long sought in the neuroscientific study of decision-making,\\nagency and volition. On-line real-time (ORT) prediction is important for understanding the relation between neural correlates of decision-making and conscious,\\nvoluntary action as well as for brain-machine interfaces. Here, epilepsy patients,\\nimplanted with intracranial depth microelectrodes or subdural grid electrodes for\\nclinical purposes, participated in a ?matching-pennies? game against an opponent.\\nIn each trial, subjects were given a 5 s countdown, after which they had to raise\\ntheir left or right hand immediately as the ?go? signal appeared on a computer\\nscreen. They won a fixed amount of money if they raised a different hand than\\ntheir opponent and lost that amount otherwise. The question we here studied was\\nthe extent to which neural precursors of the subjects? decisions can be detected in\\nintracranial local field potentials (LFP) prior to the onset of the action.\\nWe found that combined low-frequency (0.1?5 Hz) LFP signals from 10 electrodes\\nwere predictive of the intended left-/right-hand movements before the onset of the\\ngo signal. Our ORT system predicted which hand the patient would raise 0.5 s\\nbefore the go signal with 68?3% accuracy in two patients. Based on these results,\\nwe constructed an ORT system that tracked up to 30 electrodes simultaneously,\\nand tested it on retrospective data from 7 patients. On average, we could predict\\nthe correct hand choice in 83% of the trials, which rose to 92% if we let the system\\ndrop 3/10 of the trials on which it was less confident. Our system demonstrates?\\nfor the first time?the feasibility of accurately predicting a binary action on single\\ntrials in real time for patients with intracranial recordings, well before the action\\noccurs.\\n\\n1\\n\\n\\f1\\n\\nIntroduction\\n\\nThe work of Benjamin Libet [1, 2] and others [3, 4] has challenged our intuitive notions of the relation between decision making and conscious voluntary action. Using electrocorticography (EEG),\\nthese experiments measured brain potentials from subjects that were instructed to flex their wrist at a\\ntime of their choice and note the position of a rotating dot on a clock when they felt the urge to move.\\nThe results suggested that a slow cortical wave measured over motor areas?termed ?readiness potential? [5], and known to precede voluntary movement [6]?begins a few hundred milliseconds before the average reported time of the subjective ?urge? to move. This suggested that action onset and\\ncontents could be decoded from preparatory motor signals in the brain before the subject becomes\\naware of an intention to move and of the contents of the action. However, the readiness potential\\nwas computed by averaging over 40 or more trials aligned to movement onset after the fact. More\\nrecently, it was shown that action contents can be decoded using functional magnetic-resonance\\nimaging (fMRI) several seconds before movement onset [7]. But, while done on a single-trial basis,\\ndecoding the neural signals took place off-line, after the experiment was concluded, as the sluggish\\nnature of fMRI hemodynamic signals precluded real-time analysis. Moreover, the above studies\\nfocused on arbitrary and meaningless action?purposelessly raising the left or right hand?while\\nwe wanted to investigate prediction of reasoned action in more realistic, everyday situations with\\nconsequences for the subject.\\nIntracranial recordings are good candidates for single-trial, ORT analysis of action onset and contents [8, 9], because of the tight temporal pairing of LFP to the underlying neuronal signals. Moreover, such recordings are known to be cleaner and more robust, with signal-to-noise ratios up to\\n100 times larger than surface recordings like EEG [10, 11]. We therefore took advantage of a rare\\nopportunity to work with epilepsy patients implanted with intracranial electrodes for clinical purposes. Our ORT system (Fig. 1) predicts, with far above chance accuracy, which one of two future\\nactions is about to occur on this one trial and feeds the prediction back to the experimenter, all\\nbefore the onset of the go signal that triggers the patient?s movement (see Experimental Methods).\\nWe achieve relatively high prediction performance using only part of the data?learning from brain\\nactivity in past trials only (Fig. 2) to predict future ones (Fig. 3)?while still running the analysis\\nquickly enough to act upon the prediction before the subject moved.\\n\\n2\\n2.1\\n\\nExperimental Methods\\nSubjects\\n\\nSubjects in this experiment were 8 consenting intractable epilepsy patients that were implanted with\\nintracranial electrodes as part of their presurgical clinical evaluation (ages 18?60, 3 males). They\\nwere inpatients in the neuro-telemetry ward at the Cedars Sinai Medical Center or the Huntington\\nMemorial Hospital, and are designated with CS or HMH after their patient numbers, respectively. Six\\nof them?P12CS, P15CS, P22CS and P29?31HMH were implanted with intracortical depth electrodes targeting their bilateral anterior-cingulate cortex, amygdala, hippocampus and orbitofrontal\\ncortex. These electrodes had eight 40 ?m microwires at their tips, 7 for recording and 1 serving as\\na local ground. Two patients, P15CS and P22CS, had additional microwires in the supplementary\\nmotor area. We utilized the LFP recorded from the microwires in this study. Two other patients,\\nP16CS and P19CS, were implanted with an 8?8 subdural grid (64 electrodes) over parts of their\\ntemporal and prefrontal dorsolateral cortices. The data of one patient?P31HMH?was excluded\\nbecause microwire signals were too noisy for meaningful analysis. The institutional review boards\\nof Cedars Sinai Medical Center, the Huntington Memorial Hospital and the California Institute of\\nTechnology approved the experiments.\\nDuring the experiment, the subject sat in a hospital bed in a semi-inclined ?lounge chair? position.\\nThe stimulus/analysis computer (bottom left of Fig. 4) displaying the game screen (bottom right\\ninset of Fig. 4) was positioned to be easily viewable for the subject. When playing against the\\nexperimenter, the latter sat beside the bed. The response box was placed within easy reach of the\\nsubject (Fig. 4).\\n2\\n\\n\\f2.2\\n\\nExperiment Design\\n\\nAs part of our focus on purposeful, reasoned action, we had the subjects play a matching-pennies\\ngame?a 2-choice version of ?rock paper scissors??either against the experimenter or against a\\ncomputer. The subjects pressed down a button with their left hand and another with their right on a\\nresponse box. Then, in each trial, there was a 5 s countdown followed by a go signal, after which\\nthey had to immediately lift one of their hands. It was agreed beforehand that the patient would win\\nthe trial if she lifted a different hand than her opponent, and lose if she raised the same hand as her\\nopponent. Both players started off with a fixed amount of money, $5, and in each trial $0.10 was\\ndeducted from the loser and awarded to the winner. If a player lifted her hand before the go signal,\\ndid not lift her hand within 500 ms of the go signal, or lifted no hand or both hands at the go signal?\\nan error trial?she lost $0.10 without her opponent gaining any money. The subjects were shown the\\ncountdown, the go signal, the overall score, and various instructions on a stimulus computer placed\\nbefore them (Fig. 4). Each game consisted of 50 trials. If, at the end of the game, the subject had\\nmore money than her opponent, she received that money in cash from the experimenter.\\nBefore the experimental session began, the experimenter explained the rules of the game to the subject, and she could practice playing the game until she was familiar with it. Consequently, patients\\nusually made only few errors during the games (<6% of the trials). Following the tutorial, the subject played 1?3 games against the computer and then once against the experimenter, depending on\\ntheir availability and clinical circumstances. The first 2 games of P12CS were removed because\\nthe subject tended to constantly raise the right hand regardless of winning or losing. Two patients,\\nP15CS and P19CS, were tested in actual ORT conditions. In such sessions?3 games each?the\\nsubjects always played against the experimenter. These ORT games were different from the other\\ngames in two respects. First, a computer screen was placed behind the patient, in a location where\\nshe could not see it. Second, the experimenter was wearing earphones (Fig. 1,4). Half a second before go-signal onset, an arrow pointing towards the hand that the system predicted the experimenter\\nhad to raise to win the trial was displayed on that screen. Simultaneously, a monophonic tone was\\nplayed in the experimenter?s earphone ipsilateral to that hand. The experimenter then lifted that hand\\nat the go signal (see Supplemental Movie).\\n\\nCheetah Machine\\nCollect\\nand save\\ndata\\n\\nPatient\\nwith intracranial electrodes\\n\\nDown\\nsampling\\n\\nBuffer\\n\\n1Gbps\\nRouter\\n\\nTTL Signal\\n\\nThe winner is\\nPlayer 1\\nPLAYER 1 PLAYER 2\\nSCORE 1\\n\\nAnalysis/stimulus machine\\n\\nSCORE 2\\n\\nResponse Box Game Screen\\n\\n/\\nExperimenter\\n\\nResult\\nInterpreta\\ntion\\n\\nAnalysis\\n\\nFiltering\\n\\nDisplay/Sound\\n\\nFigure 1: A schematic diagram of the on-line real-time (ORT) system. Neural signals flow from\\nthe patient through the Cheetah machine to the analysis/stimulus computer, which controls the input\\nand output of the game and computes the prediction of the hand the patient would raise at the go\\nsignal. It displays it on a screen behind the patient and informs the experimenter which hand to raise\\nby playing a tone in his ipsilateral ear using earphones.\\n\\n3\\n\\n\\f3\\n3.1\\n\\nThe real-time system\\nHardware and software overview\\n\\n?V\\n\\n?V\\n\\n?V\\n\\nNeural data from the intracranial electrodes were transferred to a recording system (Neuralynx,\\nDigital Lynx), where it was collected and saved to the local Cheetah machine, down sampled\\nfrom 32 kHz to 2 kHz and buffered. The data were then transferred, through a dedicated 1 Gbps\\nlocal-area network, to the analysis/stimulus machine. This computer first band-pass-filtered the\\ndata to the 0.1?5 Hz range (delta and lower theta bands) using a second-order zero-lag elliptic\\nfilter with an attenuation of 40 dB (cf. Figs. 2a and 2b). We found that this frequency range?\\ngenerally comparable to that of the readiness potential?resulted in optimal prediction performance.\\nIt then ran the analysis algorithm (see below) on the filtered data. This computer also controlled\\nthe game screen, displaying the names of the players, their current scores and various instructions.\\nThe analysis/stimulus computer further\\ncontrolled the response box, which con- (a)\\n800\\nsisted of 4 LED-lit buttons. The buttons of the subject and her opponent\\n600\\nflashed red or blue whenever she or her\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nopponent won, respectively. Addition(b)100\\nally, the analysis/stimulus computer sent\\n0\\na unique transistor-transistor logic (TTL)\\n?100\\n?200\\npulse whenever the game screen changed\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nor a button was pressed on the response\\nbox, which synchronized the timing of (c) 100\\n0\\nthese events with the LFP recordings.\\n?100\\nIn real-time game sessions, the analy?200\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nsis/stimulus computer also displayed the\\nappropriate arrow on the computer screen (d) 1\\nbehind the subject and played the tone\\n0\\nto the appropriate ear of the experimenter\\n?1\\n0.5 s before go-signal onset (Figs. 1,4).\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nThe analysis software was based on a\\nmachine-learning algorithm that trained\\non past-trials data to predict the current\\ntrial and is detailed below. The training phase included the first 70% of the\\ntrials, with the prediction carried out on\\nthe remaining 30% using the trained parameters, together with an online weighting system (see below). The system examined only neural activity, and had no\\naccess to the subject?s left/right-choice\\nhistory. After filtering all the training\\ntrials (Fig. 2b), the system found the\\nmean and standard error over all leftward\\nand rightward training trials, separately\\n(Fig. 2c, left designated in red). It then\\nfound the electrodes and time windows\\nwhere the left/right separation was high\\n(Fig. 2d,e; see below), and trained the classifiers on these time windows (Fig. 2f?g).\\nThe best electrode/time-window/classifier\\n(ETC) combinations were then used to\\npredict the current trial in the prediction\\nphase (Fig. 3). The number of ETCs that\\ncan be actively monitored is currently limited to 10 due to the computational power\\nof the real-time system.\\n\\nEl 49?T1\\n\\n(e)\\n\\nEl 49?T2\\n\\nEl 49?T3\\n\\n1\\n0\\n?1\\n?5\\n\\n?4\\n\\n?3\\n?2\\n?1\\nCountdown to go signal at t=0 (seconds)\\n\\n0\\n\\n(f)\\nClassifier\\nCf1\\n\\nClassifier\\nCf2\\n\\n...\\n\\nClassifier\\nCf6\\n\\nEl 49?T1?Cf1\\nEl 49?T1?Cf2\\nEl 49?T1?Cf6\\n...\\nEl 49?T2?Cf1\\nEl 49?T2?Cf2\\nEl 49?T2?Cf6\\nEl 49?T3?Cf1\\nEl 49?T3?Cf2\\nEl 49?T3?Cf6\\n\\n(g)\\nCombination\\nEl49-T1-Cf2\\n\\nCombination\\nEl49-T2-Cf2\\n\\n...\\n\\nCombination\\nEl49-T2-Cf6\\n\\nFigure 2: The ORT-system?s training phase. Left (in\\nred) and right (in blue) raw signals (a) are low-pass filtered (b). Mean?standard errors of signals preceeding left- and right-hand movments (c) are used to compute a left/right separability index (d), from which time\\nwindows with good separation are found (e). Seven\\nclassifiers are then applied to all the time windows (f)\\nand the best electrode/time-window/classifier combinations are selected (g) and used in the prediction phase\\n(Fig. 3).\\n\\n4\\n\\n\\f?V\\n\\n100\\n0\\n?100\\n?200\\n?5\\n\\n?4\\n\\n?3\\n\\n?2\\n\\n?1\\n\\n0\\n\\nTrained classifiers\\n\\nCombination\\nE l 49?T1?Cf2\\n\\nCombination\\nE l 49?T2?Cf2\\n\\nWeight = 1\\n\\nWeight = 1\\n\\nCombination\\nE l 49?T2?Cf6\\n\\n&\\n\\nWeight = 1\\n\\nPredicted result\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nR\\n\\nL\\n\\n&\\n\\nR\\n\\nL\\nReal result\\n\\nAdjust the weights\\n\\nL\\n\\n==\\n\\nFigure 3: The ORT-system?s prediction phase. A new signal?from 5 to 0.5 seconds before the\\ngo signal?is received in real time, and each electrode/time-window/classifier combination (ETC)\\nclassifies it as resulting in left- or right-hand movement. These predictions are then compared to the\\nactual hand movement, with the weights associated with ETCs that correctly (incorrectly) predicted\\nincreasing (decreasing).\\n\\n3.2\\n\\nComputing optimal left/right-separating time windows\\n\\nThe algorithm focused on finding the time windows with the best left/right separation for the different recording electrodes over the training set (Fig. 2c?e). That is, we wanted to predict whether\\nthe signal aN (t) on trial N will result in a leftward or rightward movement?i.e., whether the label of the N th trial will be Lt or Rt, respectively. For each electrode, we looked at the N ? 1\\nprevious trials a1 (t), a2 (t), . . . , aN ?1 (t), and their associated labels as l1 , l2 , . . . , lN ?1 . Now, let\\nN ?1\\n?1\\nL(t) = {ai (t) | li = Lt}N\\ni=1 and R(t) = {ai (t) | li = Rt}i=1 be the set of previous leftward and\\nrightward trials in the training set, respectively. Furthermore, let Lm (t) (Rm (t)) and Ls (t) (Rs (t))\\nbe the mean and standard error of L(t) (R(t)), respectively. We can now define the normalized\\nrelative left/right separation for each electrode at time t (see Fig. 2d):\\n?\\n[Lm (t) ? Ls (t)] ? [Rm (t) + Rs (t)]\\n?\\n?\\nif [Lm (t) ? Ls (t)] ? [Rm (t) + Rs (t)] > 0\\n?\\n?\\nLm (t) ? Rm (t)\\n?\\n?\\n?\\n?\\n?\\n[Rm (t) ? Rs (t)] ? [Lm (t) + Ls (t)]\\n?(t) =\\n?\\nif [Rm (t) ? Rs (t)] ? [Lm (t) + Ls (t)] > 0\\n?\\n?\\n?\\nRm (t) ? Lm (t)\\n?\\n?\\n?\\n?\\n?\\n?\\n0\\notherwise\\nThus, ?(t) > 0 (?(t) < 0) means that the leftward trials tend to be considerably higher (lower)\\nthan rightward trials for that electrode at time t, while ?(t) = 0 suggests no left/right separation at\\ntime t. We define a consecutive time period of |?(t)| > 0 for t < prediction time (the time before\\nthe go signal when we want the system to output a prediction; -0.5 s for the ORT trials) as a time\\nwindow (Fig. 2e). After all time windows are found for all electrodes, time windows lessRthan M ms\\nt\\napart are combined into one. Then, for each time window from t1 to t2 we define a = t12 |?(t)|dt.\\nWe then eliminate all time windows satisfying a < A. We found the values M = 200 ms and\\nA = 4, 500 ?V ? ms to be optimal for real-time analysis. This resulted in 20?30 time windows over\\nall 64 electrodes that we monitored.\\n5\\n\\n\\f1\\n$4.80\\n\\n$5.20\\n\\nP15CS\\n\\nUri\\n\\nFigure 4: The experimental setup in the clinic. At 400 ms before the go signal, the patient and\\nexperimenter are watching the game screen (inset on bottom right) on the analysis/stimulus computer\\n(bottom left) and still pressing down the buttons of the response box. The realtime system already\\ncomputed a prediction, and thus displays an arrow on the screen behind the patient and plays a tone\\nin the experimenter?s ear ipsilateral to the hand it predicts he should raise to beat the patient (see\\nSupplemental Movie).\\n3.3\\n\\nClassifiers selection and ETC determination\\n\\nWe used ensemble learning with 7 types of relatively simple binary classifiers (due to real-time\\nprocessing considerations) on every electrode?s time windows (Fig. 2f). Classifiers A to G would\\nclassify aN (t) as Lt if:\\nP\\nP\\nP\\n(A) Defining aN,M , Lm,M and Rm,M as aN (t), Lm (t) and Rm (t) over time window M ,\\n\\u0001\\n\\u0001\\n\\u0001\\n(i) sign Rm,M 6= sign aN,M = sign Lm,M , or\\n\\f\\n\\f\\n\\f \\f\\n\\u0001\\n\\u0001\\n\\u0001\\n(ii) sign Rm,M = sign aN,M = sign Lm,M and \\fLm,M \\f > \\fRm,M \\f, or\\n\\f\\n\\f\\n\\f \\f\\n\\u0001\\n\\u0001\\n\\u0001\\n(iii) sign Rm (t) 6= sign SN,M 6= sign Lm (t) and \\fLm,M \\f < \\fRm,M \\f;\\n\\f\\n\\u0001\\n\\u0001\\f \\f\\n\\u0001\\n\\u0001\\f\\n(B) \\fmean aN (t) ? mean Lm (t) \\f < \\fmean aN (t) ? mean Rm (t) \\f;\\n\\f\\n\\f\\n\\u0001\\n\\u0001\\f\\n\\u0001\\n\\u0001\\f\\n(C) \\fmedian aN (t) ? median Lm (t) \\f < \\fmedian aN (t) ? median Rm (t) \\f over the time\\nwindow;\\n\\f\\n\\f\\n\\f\\n\\f\\n\\f\\n(D) aN (t) ? Lm (t)\\fL2 < \\faN (t) ? Rm (t)\\fL2 over the time window;\\n(E) aN (t) is convex/concave like Lm (t) while Rm (t) is concave/convex, respectively;\\n(F) Linear support-vector machine (SVM) designates it as so; and\\n(G) k-nearest neighbors (KNN) with Euclidean distance designates it as so.\\nEach classifier is optimized for certain types of features. To estimate how well its classification\\nwould generalize from the training to the test set, we trained and tested it using a 70/30 crossvalidation procedure within the training set. We tested each classifier on every time window of every\\nelectrode, discarding those with accuracy <0.68, which left 12.0 ? 1.6% of the original 232 ? 18\\nETCs, on average (?standard error). The training phase therefore ultimately output a set of S binary\\nETC combinations (Fig. 2g) that were used in the prediction phase (Fig. 3).\\n3.4\\n\\nThe prediction-phase weighting system\\n\\nIn the prediction phase, each of the overall S binary ETCs calculates a prediction, ci ? {?1, 1} (for\\nright and left, respectively), independently at the desired prediction time. All classifiers are initially\\n6\\n\\n\\fPS\\ngiven the same weight, w1 = w2 = ? ? ? = wS = 1. We then calculate ? = i=1 wi ? ci and predict\\nleft (right) if ? > d (? < ?d), or declare it an undetermined trial if ?d < ? < d. Here d is the\\ndrop-off threshold for the prediction. Thus the larger d is, the more confident the system needs to be\\nto make a prediction, and the larger the proportion of trials on which the system abstains?the dropoff rate. Weight wi associated with ETCi is increased (decreased) by 0.1 whenever ETCi predicts\\nthe hand movement correctly (incorrectly). A constantly erring ETC would therefore be associated\\nwith an increasingly small and then increasingly negative weight.\\n3.5\\n\\nImplementation\\n\\nThe algorithm was implemented in MATLAB 2011a (MathWorks, Natick, MA) as well as in C++\\non Visual Studio 2008 (Microsoft, Redmond, WA) for enhanced performance. The neural signals\\nwere collected by the Digital Lynx S system using Cheetah 5.4.0 (Neuralynx, Redmond, WA). The\\nsimulated-ORT system was also implemented in MATLAB 2011a. The simulated-ORT analyses\\ncarried out in this paper used real patient data saved on the Digital Lynx system.\\n1\\n\\n0.9\\n\\nDrop rate:\\nNone\\n0.18\\n0\\u0011\\u0016\\u0013\\n\\nPrediction accuracy\\n\\n0.8\\n\\n0.7\\nSignificant accuracy\\n(p=0.05)\\n0.6\\n\\n0.5\\n\\n?5\\n\\n?4.5\\n\\n?4\\n\\n?3.5\\n\\n?3\\n\\n?2.5\\nTime (s)\\n\\n?2\\n\\n?1.5\\n\\n?1\\n\\n?0.5\\n\\n0\\nGo-signal\\nonset\\n\\nFigure 5: Across-subjects average of the prediction accuracy of simulated-ORT versus time before\\nthe go signal. The mean accuracies over time when the system predicts on every trial, is allowed\\nto drop 19% or 30% of the trials, are depicted in blue, green and red, respectively (?standard error\\nshaded). Values above the dashed horizontal line are significant at p = 0.05.\\n\\n4\\n\\nResults\\n\\nWe tested our prediction system in actual real time on 2 patients?P15CS and P19CS (a depth\\nand grid patient, respectively), with a prediction time of 0.5 s before the go signal (see Supplementary Movie). Because of computational limitations, the ORT system could only track 10\\nelectrodes with just 1 ETC per electrode in real time. For P15CS, we achieved an accuracy of\\n72?2% (?standard error; accuracy = number of accurately predicted trials / [total number of trials - number of dropped trials]; p = 10?8 , binomial test) without modifying the weights online during the prediction (see Section 3.4). For P19CS we did not run patient-specific training of the ORT system, and used parameter values that were good on average over previous patients instead. The prediction accuracy was significantly above chance 63?2% (?standard error; p = 7 ? 10?4 , binomial test). To understand how much we could improve our accuracy\\nwith optimized hardware/software, we ran the simulated-ORT at various prediction times along\\n7\\n\\n\\fAccuracy\\n\\nthe 5 s countdown leading to the go signal. We further tested 3 drop-off rates?0, 0.19 and\\n0.30 (Fig. 5; drop-off rate = number of dropped trials / total number of trials; these resulted\\nfrom 3 drop-off thresholds?0, 0.1 and 0.2?respectively, see Section 3.4:). Running offline,\\nwe were able to track 20?30 ETCs, which resulted in considerably higher accuracies (Figs. 5,6).\\nAveraged over all subjects, the accuracy rose from about 65% more than\\n1\\n4 s before the go signal to 83?92%\\nclose to go-signal onset, depending\\n0.9\\non the allowed drop-off rate. In particular, we found that for a predic0.8\\ntion time of 0.5 s before go-signal\\nonset, we could achieve accuracies\\n0.7\\nof 81?5% and 90?3% (?standard\\nerror) for P15CS and P19CS, re0.6\\nspectively, with no drop off (Fig. 6).\\nPatients:\\nP12CS\\nWe also analyzed the weights that\\nP15CS\\nour weighting system assigned to the\\n0.5\\nP16CS\\nP19CS\\ndifferent ETCs. We found that the\\nP22CS\\nempirical distribution of weights to\\nP29HMH\\n0.4\\nP30HMH\\nETCs associated with classifiers A to\\nG was, on average: 0.15, 0.12, 0.16,\\n?5 ?4.5 ?4 ?3.5 ?3 ?2.5 ?2 ?1.5 ?1 ?0.5 0\\n0.22, 0.01, 0.26 and 0.07, respecTime before go signal (at t=0) (seconds)\\ntively. This suggests that the linear\\nSVM and L2-norm comparisons (of\\naN to Lm and Rm ) together make up Figure 6: Simulated-ORT accuracy over time for individual\\nnearly half of the overall weights at- patients with no drop off.\\ntributed to the classifiers, while the\\ncurrent concave/convex measure is of\\nlittle use as a classifier.\\n\\n5\\n\\nDiscussion\\n\\nWe constructed an ORT system that, based on intracranial recordings, predicted which hand a person would raise well before movement onset at accuracies much greater than chance in a competitive environment. We further tested this system off-line, which suggested that with optimized\\nhardware/software, such action contents would be predictable in real time at relatively high accuracies already several seconds before movement onset. Both our prediction accuracy and drop-off\\nrates close to movement onset are superior to those achieved before movement onset with noninvasive methods like EEG and fMRI [7, 12?14]. Importantly, our subjects played a matching pennies game?a 2-choice version of rock-paper-scissors [15]?to keep their task realistic, with minor\\nthough real consequences, unlike the Libet-type paradigms whose outcome bears no consequences\\nfor the subjects. It was suggested that accurate online, real-time prediction before movement onset\\nis key to investigating the relation between the neural correlates of decisions, their awareness, and\\nvoluntary action [16, 17]. Such prediction capabilities would facilitate many types of experiments\\nthat are currently infeasible. For example, it would make it possible to study decision reversals on\\na single-trial basis, or to test whether subjects can guess above chance which of their action contents are predictable from their current brain activity, potentially before having consciously made up\\ntheir mind [16, 18]. Accurately decoding these preparatory motor signals may also result in earlier\\nand improved classification for brain-computer interfaces [13, 19, 20]. The work we present here\\nsuggests that such ORT analysis might well be possible.\\nAcknowledgements\\nWe thank Ueli Rutishauser, Regan Blythe Towel, Liad Mudrik and Ralph Adolphs for meaningful\\ndiscussions. This research was supported by the Ralph Schlaeger Charitable Foundation, Florida\\nState University?s ?Big Questions in Free Will? initiative and the G. Harold & Leila Y. Mathers\\nCharitable Foundation.\\n8\\n\\n\\fReferences\\n[1] B. Libet, C. Gleason, E. Wright, and D. Pearl. Time of conscious intention to act in relation to\\nonset of cerebral activity (readiness-potential): The unconscious initiation of a freely voluntary\\nact. Brain, 106:623, 1983.\\n[2] B. Libet. Unconscious cerebral initiative and the role of conscious will in voluntary action.\\nBehavioral and brain sciences, 8:529?539, 1985.\\n[3] P. Haggard and M. Eimer. On the relation between brain potentials and the awareness of\\nvoluntary movements. Experimental Brain Research, 126:128?133, 1999.\\n[4] A. Sirigu, E. Daprati, S. Ciancia, P. Giraux, N. Nighoghossian, A. Posada, and P. Haggard.\\nAltered awareness of voluntary action after damage to the parietal cortex. Nature Neuroscience,\\n7:80?84, 2003.\\n[5] H. Kornhuber and L. Deecke. Hirnpotenti?alanderungen bei Willk?urbewegungen und passiven\\nBewegungen des Menschen: Bereitschaftspotential und reafferente Potentiale. Pfl?ugers Archiv\\nEuropean Journal of Physiology, 284:1?17, 1965.\\n[6] H. Shibasaki and M. Hallett. What is the Bereitschaftspotential? Clinical Neurophysiology,\\n117:2341?2356, 2006.\\n[7] C. Soon, M. Brass, H. Heinze, and J. Haynes. Unconscious determinants of free decisions in\\nthe human brain. Nature Neuroscience, 11:543?545, 2008.\\n[8] I. Fried, R. Mukamel, and G. Kreiman. Internally generated preactivation of single neurons in\\nhuman medial frontal cortex predicts volition. Neuron, 69:548?562, 2011.\\n[9] M. Cerf, N. Thiruvengadam, F. Mormann, A. Kraskov, R. Quian Quiorga, C. Koch, and\\nI. Fried. On-line, voluntary control of human temporal lobe neurons. Nature, 467:1104?1108,\\n2010.\\n[10] T. Ball, M. Kern, I. Mutschler, A. Aertsen, and A. Schulze-Bonhage. Signal quality of simultaneously recorded invasive and non-invasive EEG. Neuroimage, 46:708?716, 2009.\\n[11] G. Schalk, J. Kubanek, K. Miller, N. Anderson, E. Leuthardt, J. Ojemann, D. Limbrick,\\nD. Moran, L. Gerhardt, and J. Wolpaw. Decoding two-dimensional movement trajectories\\nusing electrocorticographic signals in humans. Journal of Neural engineering, 4:264, 2007.\\n[12] O. Bai, V. Rathi, P. Lin, D. Huang, H. Battapady, D. Y. Fei, L. Schneider, E. Houdayer, X. Chen,\\nand M. Hallett. Prediction of human voluntary movement before it occurs. Clinical Neurophysiology, 122:364?372, 2011.\\n[13] O. Bai, P. Lin, S. Vorbach, J. Li, S. Furlani, and M. Hallett. Exploration of computational\\nmethods for classification of movement intention during human voluntary movement from\\nsingle trial EEG. Clinical Neurophysiology, 118:2637?2655, 2007.\\n[14] U. Maoz, A. Arieli, S. Ullman, and C. Koch. Using single-trial EEG data to predict laterality\\nof voluntary motor decisions. Society for Neuroscience, 38:289.6, 2008.\\n[15] C. Camerer. Behavioral game theory: Experiments in strategic interaction. Princeton University Press, 2003.\\n[16] J. D. Haynes. Decoding and predicting intentions. Annals of the New York Academy of Sciences, 1224:9?21, 2011.\\n[17] P. Haggard. Decision time for free will. Neuron, 69:404?406, 2011.\\n[18] J. D. Haynes. Beyond libet. In W. Sinnott-Armstrong and L. Nadel, editors, Conscious will\\nand responsibility, pages 85?96. Oxford University Press, 2011.\\n[19] A. Muralidharan, J. Chae, and D. M. Taylor. Extracting attempted hand movements from EEGs\\nin people with complete hand paralysis following stroke. Frontiers in neuroscience, 5, 2011.\\n[20] E. Lew, R. Chavarriaga, S. Silvoni, and J. R. Milln. Detection of self-paced reaching movement\\nintention from EEG signals. Frontiers in Neuroengineering, 5:13, 2012.\\n\\n9\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(\"/content/NIPS Papers.zip\", \"r\") as zip_ref:\n",
        "    # Extract the file to a temporary directory\n",
        "    zip_ref.extractall(\"temp\")\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "papers = pd.read_csv(\"temp/NIPS Papers/papers.csv\")\n",
        "\n",
        "# Print head\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF24UeOs3Z_W"
      },
      "source": [
        "** **\n",
        "#### Step 2: Data Cleaning <a class=\"anchor\\\" id=\"clean_data\"></a>\n",
        "** **\n",
        "\n",
        "Since the goal of this analysis is to perform topic modeling, let's focus only on the text data from each paper, and drop other metadata columns. Also, for the demonstration, we'll only look at 100 papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "shHTlwxu3Z_W",
        "outputId": "71e2eba9-57ed-44e8-d25b-84e76764ca0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      year                                              title  \\\n",
              "1661  1989  A Self-organizing Associative Memory System fo...   \n",
              "1819  2004   Conditional Random Fields for Object Recognition   \n",
              "1972  2005                Robust Fisher Discriminant Analysis   \n",
              "1888  2004                   Theories of Access Consciousness   \n",
              "6150  2016  MoCap-guided Data Augmentation for 3D Pose Est...   \n",
              "\n",
              "                                               abstract  \\\n",
              "1661                                   Abstract Missing   \n",
              "1819                                   Abstract Missing   \n",
              "1972                                   Abstract Missing   \n",
              "1888                                   Abstract Missing   \n",
              "6150  This paper addresses the problem of 3D human p...   \n",
              "\n",
              "                                             paper_text  \n",
              "1661  332\\n\\nHormel\\n\\nA Sell-organizing Associative...  \n",
              "1819  Conditional Random Fields for Object\\nRecognit...  \n",
              "1972  Robust Fisher Discriminant Analysis\\n\\nSeung-J...  \n",
              "1888  Theories Of Access Consciousness\\nMichael D. C...  \n",
              "6150  MoCap-guided Data Augmentation\\nfor 3D Pose Es...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d5c8b5ed-b398-4c1f-b467-60816ae2ebe9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1661</th>\n",
              "      <td>1989</td>\n",
              "      <td>A Self-organizing Associative Memory System fo...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>332\\n\\nHormel\\n\\nA Sell-organizing Associative...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1819</th>\n",
              "      <td>2004</td>\n",
              "      <td>Conditional Random Fields for Object Recognition</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Conditional Random Fields for Object\\nRecognit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1972</th>\n",
              "      <td>2005</td>\n",
              "      <td>Robust Fisher Discriminant Analysis</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Robust Fisher Discriminant Analysis\\n\\nSeung-J...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1888</th>\n",
              "      <td>2004</td>\n",
              "      <td>Theories of Access Consciousness</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Theories Of Access Consciousness\\nMichael D. C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6150</th>\n",
              "      <td>2016</td>\n",
              "      <td>MoCap-guided Data Augmentation for 3D Pose Est...</td>\n",
              "      <td>This paper addresses the problem of 3D human p...</td>\n",
              "      <td>MoCap-guided Data Augmentation\\nfor 3D Pose Es...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5c8b5ed-b398-4c1f-b467-60816ae2ebe9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d5c8b5ed-b398-4c1f-b467-60816ae2ebe9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d5c8b5ed-b398-4c1f-b467-60816ae2ebe9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a0369f94-23d2-49b4-a191-0d5c50ba3563\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a0369f94-23d2-49b4-a191-0d5c50ba3563')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a0369f94-23d2-49b4-a191-0d5c50ba3563 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "papers",
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1987,\n        \"max\": 2016,\n        \"num_unique_values\": 28,\n        \"samples\": [\n          2013,\n          1993,\n          2003\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Fast Parameter Estimation Using Green's Functions\",\n          \"Generalization and Scaling in Reinforcement Learning\",\n          \"Discovering the Structure of a Reactive Environment by Exploration\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 59,\n        \"samples\": [\n          \"Abstract Missing\",\n          \"This paper introduces a novel mathematical and computational framework, namely {\\\\it Log-Hilbert-Schmidt metric} between positive definite operators on a Hilbert space. This is a generalization of the Log-Euclidean metric on the Riemannian manifold of positive definite matrices to the infinite-dimensional setting. The general framework is applied in particular to compute distances between covariance operators on a Reproducing Kernel Hilbert Space (RKHS), for which we obtain explicit formulas via the corresponding Gram matrices. Empirically, we apply our formulation to the task of multi-category image classification, where each image is represented by an infinite-dimensional RKHS covariance operator. On several challenging datasets, our method significantly outperforms approaches based on covariance matrices computed directly on the original input features, including those using the Log-Euclidean metric, Stein and Jeffreys divergences, achieving new state of the art results.\",\n          \"Resting state activity is brain activation that arises in the absence of any task, and is usually measured in awake subjects during prolonged fMRI scanning sessions where the only instruction given is to close the eyes and do nothing.  It has been recognized in recent years that resting state activity is implicated in a wide variety of brain function.  While certain networks of brain areas have different levels of activation at rest and during a task, there is nevertheless significant similarity between activations in the two cases.  This suggests that recordings of resting state activity can be used as a source of unlabeled data to augment discriminative regression techniques in a semi-supervised setting.  We evaluate this setting empirically yielding three main results: (i) regression tends to be improved by the use of Laplacian regularization even when no additional unlabeled data are available, (ii) resting state data may have a similar marginal distribution to that recorded during the execution of a visual processing task reinforcing the hypothesis that these conditions have similar types of activation, and (iii) this source of information can be broadly exploited to improve the robustness of empirical inference in fMRI studies, an inherently data poor domain.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Fast Parameter Estimation\\nUsing Green's Functions\\n\\nK. Y. Michael Wong\\nDepartment of Physics\\nHong Kong University\\nof Science and Technology\\nClear Water Bay, Hong Kong\\nphkywong@ust.hk\\n\\nFuIi Li\\nDepartment of Applied Physics\\nXian Jiaotong University\\nXian , China 710049\\nflli @xjtu. edu. en\\n\\nAbstract\\nWe propose a method for the fast estimation of hyperparameters\\nin large networks, based on the linear response relation in the cavity method, and an empirical measurement of the Green's function. Simulation results show that it is efficient and precise, when\\ncompared with cross-validation and other techniques which require\\nmatrix inversion.\\n\\n1\\n\\nIntroduction\\n\\nIt is well known that correct choices of hyperparameters in classification and regression tasks can optimize the complexity of the data model , and hence achieve the\\nbest generalization [1]. In recent years various methods have been proposed to estimate the optimal hyperparameters in different contexts, such as neural networks [2],\\nsupport vector machines [3, 4, 5] and Gaussian processes [5]. Most of these methods are inspired by the technique of cross-validation or its variant, leave-one-out\\nvalidation. While the leave-one-out procedure gives an almost unbiased estimate\\nof the generalization error, it is nevertheless very tedious. Many of the mentioned\\nattempts aimed at approximating this tedious procedure without really having to\\nsweat through it. They often rely on theoretical bounds, inverses to large matrices,\\nor iterative optimizations.\\n\\nIn this paper, we propose a new approach to hyperparameter estimation in large\\nsystems. It is known that large networks are mean-field systems, so that when one\\nexample is removed by the leave-one-out procedure, the background adjustment\\ncan be analyzed by a self-consistent perturbation approach. Similar techniques\\nhave been applied to the neural network [6], Bayesian learning [7] and the support\\nvector machine [5]. They usually involve a macroscopic number of unknown variables, whose solution is obtained through the inversion of a matrix of macroscopic\\nsize, or iteration. Here we take a further step to replace it by a direct measurement\\nof the Green's function via a small number of learning processes. The proposed\\nprocedure is fast since it does not require repetitive cross-validations, matrix inversions, nor iterative optimizations for each set of hyperparaemters. We will also\\npresent simulation results which show that it is an excellent approximation.\\n\\n\\fThe proposed technique is based on the cavity method, which was adapted from\\ndisordered systems in many-body physics. The basis of the cavity method is a\\nself-consistent argument addressing the situation of removing an example from the\\nsystem. The change on removing an example is described by the Green's function,\\nwhich is an extremely general technique used in a wide range of quantum and\\nclassical problems in many-body physics [8]. This provides an excellent framework\\nfor the leave-one-out procedure. In this paper, we consider two applications of\\nthe cavity method to hyperparameter estimation, namely the optimal weight decay\\nand the optimal learning time in feedforward networks. In the latter application,\\nthe cavity method provides, as far as we are aware of, the only estimate of the\\nhyperparameter beyond empirical stopping criteria and brute force cross-validation.\\n\\n2\\n\\nSteady-State Hyperparameter Estimation\\n\\nConsider the network with adjustable parameters w. An energy function E is\\ndefined with respect to a set of p examples with inputs and outputs respectively\\ngiven by {IL and y'\\\", JL = 1, ... ,p, where (IL is an N-dimensional input vector with\\ncomponents\\nj = 1,? ?? ,N, and N ? 1 is macroscopic. We will first focus on\\nthe dynamics of a single-layer feedforward network and generalize the results to\\nmultilayer networks later. In single-layer networks, E has the form\\n\\ne;,\\n\\nE =\\n\\nL f(X'\\\",y'\\\") + R(w).\\n\\n(1)\\n\\n'\\\"\\n\\nHere f( x'\\\" , y'\\\") represents the error function with respect to example JL. It is expressed in terms of the activation x'\\\" == w? (IL. R( w) represents a regularization\\nterm which is introduced to limit the complexity of the network and hence enhance\\nthe generalization ability. Learning is achieved by the gradient descent dynamics\\n\\ndWj(t) _ _ ~_oE_\\ndt\\n\\n(2)\\n\\nThe time-dependent Green's function Gjk(t, s) is defined as the response of the\\nweight Wj at time t due to a unit stimulus added at time s to the gradient term with\\nrespect to weight Wk, in the limit of a vanishing magnitude of the stimulus. Hence\\nif we compare the evolution of Wj(t) with another system Wj(t) with a continuous\\nperturbative stimulus Jhj(t), we would have\\n\\ndWj(t) = _~ oE\\ndt\\nNow.\\n\\nJh()\\nJ t ,\\n\\n(3)\\n\\ndsGjk(t,s)Jhk(s).\\n\\n(4)\\n\\nJ\\n\\n+\\n\\nand the linear response relation\\n\\nWj(t) = Wj(t)\\n\\n+L\\n\\nJ\\n\\nk\\n\\nNow we consider the evolution ofthe network w;'\\\"(t) in which example JL is omitted\\nfrom the training set. For a system learning macroscopic number of examples, the\\nchanges induced by the omission of an example are perturbative, and we can assume\\nthat the system has a linear response. Compared with the original network Wj(t),\\nthe gradient of the error of example JL now plays the role of the stimulus in (3).\\nHence we have\\n\\n(5)\\n\\n\\fMultiplying both sides by ~f and summing over j, we obtain\\n\\n1-'( ) -\\n\\nh t - x\\n\\nI-'()\\nt\\n\\n+\\n\\nJ\\n\\nds\\n\\n[1\\n' \\\" I-'G ( ) I-']OE(XI-'(S)'YI-')\\nN \\\"7:~j jk t ,s ~k\\noxl-'(s)'\\n\\n(6)\\n\\nHere hl-'(t) == V;\\\\I-'(t) . ~ is called the cavity activation of example ft. When the\\ndynamics has reached the steady state, we arrive at\\n\\nI-'\\n\\nhI-'\\n= x\\n\\nwhere, = limt--+oo\\n\\n+,\\n\\nOE(XI-' , yl-')\\noxl-'\\n'\\n\\nJdS[Ljk ~fGjk (t , s)~r]jN\\n\\n(7)\\n\\nis the susceptibility.\\n\\nAt time t , the generalization error is defined as the error function averaged over the\\ndistribution of input (, and their corresponding output y, i.e. ,\\n\\n(8)\\nwhere x == V; . (is the network activation. The leave-one-out generalization error is\\nan estimate of 109 given in terms ofthe cavity activations hI-' by fg = LI-' 10 (hI-' ,yl-')jp.\\nHence if we can estimate the Green's function, the cavity activation in (7) provides\\na convenient way to estimate the leave-one-out generalization error without really\\nhaving to undergo the validation process.\\nWhile self-consistent equations for the Green's function have been derived using\\ndiagrammatic methods [9], their solutions cannot be computed except for the specific case of time-translational invariant Green's functions , such as those in Adaline\\nlearning or linear regression. However, the linear response relation (4) provides a\\nconvenient way to measure the Green's function in the general case. The basic idea\\nis to perform two learning processes in parallel, one following the original process\\n(2) and the other having a constant stimulus as in (3) with 6hj (t) = TJ6jk, where\\n8j k is the Kronecka delta. When the dynamics has reached the steady state, the\\nmeasurement Wj - Wj yields the quantity TJ Lk dsGjk(t, s).\\n\\nJ\\n\\nA simple averaging procedure, replacing all the pairwise measurements between the\\nstimulation node k and observation node j, can be applied in the limit of large\\nN. We first consider the case in which the inputs are independent and normalized,\\ni.e., (~j) = 0, (~j~k) = 8j k. In this case, it has been shown that the off-diagonal\\nGreen's functions can be neglected, and the diagonal Green's functions become selfaveraging, i.e. , Gjk(t , s) = G(t, s)8jk , independent of the node labels [9], rendering\\n, = limt--+oo J dsG(t, s).\\nIn the case that the inputs are correlated and not normalized, we can apply standard\\nprocedures of whitening transformation to make them independent and normalized\\n[1]. In large networks, one can use the diagrammatic analysis in [9] to show that\\nthe (unknown) distribution of inputs does not change the self-averaging property of\\nthe Green's functions after the whitening transformation. Thereafter, the measurement of Green's functions proceeds as described in the simpler case of independent\\nand normalized inputs. Since hyperparameter estimation usually involves a series\\nof computing fg at various hyperparameters, the one-time preprocessing does not\\nincrease the computational load significantly.\\nThus the susceptibility, can be measured by comparing the evolution of two processes: one following the original process (2), and the other having a constant\\nstimulus as in (3) with 8h j (t) = TJ for all j. When the dynamics has reached the\\nsteady state, the measurement (Wj - Wj) yields the quantity TJ,.\\n\\n\\fWe illustrate the extension to two-layer networks by considering the committee machine, in which the errorfunction takes the form E(2:: a !(x a), y) , where a = 1,? ??, nh\\nis the label of a hidden node, Xa == wa . [is the activation at the hidden node a,\\nand! represents the activation function. The generalization error is thus a function\\nof the cavity activations of the hidden nodes, namely, E9 = 2::JL E(2::a !(h~), yJL) /p,\\nwhere h~ = w~JL . (IL . When the inputs are independent and normalized, they are\\nrelated to the generic activations by\\n\\nhJL- JL+'\\\"\\naE(2::c !(X~) , yJL)\\na - Xa ~ \\\"lab\\na JL\\n'\\nXb\\nb\\n\\n(9)\\n\\nwhere \\\"lab = limt~ oo J dsGab(t, s) is the susceptibility tensor. The Green's function\\nGab(t, s) represents the response of a weight feeding hidden node a due to a stimulus\\napplied at the gradient with respect to a weight feeding node b. It is obtained by\\nmonitoring nh + 1 learning processes, one being original and each of the other nh\\nprocesses having constant stimuli at the gradients with respect to one of the hidden\\nnodes, viz.,\\n\\ndw~~) (t) _\\ndt\\n\\n1\\n\\naE\\n\\n- - N ------=:(b)\\naW aj\\n\\n+ 'f)rSab ,\\n\\n(10)\\n\\nb = 1, ... ,nh?\\n\\nWhen the dynamics has reached the steady state, the measurement (w~7\\nyields the quantity 'f)'Yab.\\n\\n-\\n\\nWaj)\\n\\nWe will also compare the results with those obtained by extending the analysis of\\nlinear unlearning leave-one-out (LULOO) validation [6]. Consider the case that the\\nregularization R(w) takes the form of a weight decay term, R(w) = N 2::ab AabWa .\\nWb/2. The cavity activations will be given by\\n\\nhJL = JL + '\\\"\\na Xa ~\\nb\\n\\n(\\n1-\\n\\n,\\\"\\n\\n11\\n\\niJ 2:: j k ~'j(A + Q)~}bk~r\\n\\n) aE(2:: c !(xn, yJL))\\na JL\\n'\\n2::cjdk ~'j !'(xn(A + Q)~, dd'(x~)~r\\nXb\\n1\\n\\n(11)\\nwhere E~ represents the second derivative of E with respect to the student output\\nfor example /1, and the matrix Aaj,bk = AabrSjk and Q is given by\\n\\nQaj,bk\\n\\n= ~ 2: ~'j f'(x~)f'(x~)~r?\\n\\n(12)\\n\\nJL\\nThe LULOO result of (11) differs from the cavity result of (9) in that the susceptibility \\\"lab now depends on the example label /1, and needs to be computed by\\ninverting the matrix A + Q. Note also that second derivatives of the error term\\nhave been neglected.\\nTo verify the proposed method by simulations, we generate examples from a noisy\\nteacher network which is a committee machine\\n\\nyJL = ~ erf\\nnh\\n\\n(1yf2Ba ? f ) +\\n\\n(Jzw\\n\\n(13)\\n\\nHere Ba is the teacher vector at the hidden node a. (J is the noise level. ~'j and\\nzJL are Gaussian variables with zero means and unit variances. Learning is done by\\nthe gradient descent of the energy function\\n\\n(14)\\n\\n\\fand the weight decay parameter ,X is the hyperparameter to be optimized. The\\ngeneralization error fg is given by\\n\\nwhere the averaging is performed over the distribution of input { and noise z. It can\\nbe computed analytically in terms of the inner products Qab = wa . Wb, Tab = Ba . Bb\\nand Rab = Ba . Wb [10]. However, this target result is only known by the teacher ,\\nsince Tab and Rab are not accessible by the student.\\nFigure 1 shows the simulation results of 4 randomly generated samples. Four results\\nare compared: the target generalization error observed by the teacher, and those\\nestimated by the cavity method, cross-validation and extended LULOO. It can\\nbe seen that the cavity method yields estimates of the optimal weight decay with\\ncomparable precision as the other methods.\\nFor a more systematic comparison, we search for the optimal weight decay in 10 samples using golden section search [11] for the same parameters as in Fig. 1. Compared\\nwith the target results, the standard deviations of the estimated optimal weight decays are 0.3, 0.25 and 0.24 for the cavity method, sevenfold cross-validation and\\nextended LULOO respectively. In another simulation of 80 samples of the singlelayer perceptron, the estimated optimal weight decays have standard deviations of\\n1.2, 1.5 and 1.6 for the cavity method, tenfold cross-validation and extended LULOO respectively (the parameters in the simulations are N = 500, p = 400 and a\\nranging from 0.98 to 2.56).\\nTo put these results in perspective, we mention that the computational resources\\nneeded by the cavity method is much less than the other estimations. For example,\\nin the single-layer perceptrons, the CPU time needed to estimate the optimal weight\\ndecay using the golden section search by the teacher, the cavity method, tenfold\\ncross-validation and extended LULOO are in the ratio of 1 : 1.5 : 3.0 : 4.6.\\nBefore concluding this section, we mention that it is possible to derive an expression\\nof the gradient dEg I d,X of the estimated generalization error with respect to the\\nweight decay. This provides us an even more powerful tool for hyperparameter\\nestimation. In the case of the search for one hyperparameter, the gradient enables\\nus to use the binary search for the zero of the gradient, which converges faster\\nthan the golden section search. In the single-layer experiment we mentioned, its\\nprecision is comparable to fivefold cross-validations, and its CPU time is only 4%\\nmore than the teacher's search. Details will be presented elsewhere. In the case of\\nmore than one hyperparameters, the gradient information will save us the need for\\nan exhaustive search over a multidimensional hyperparameter space.\\n\\n3\\n\\nDynamical Hyperparameter Estimation\\n\\nThe second example concerns the estimation of a dynamical hyperparameter,\\nnamely the optimal early stopping time, in cases where overtraining may plague\\nthe generalization ability at the steady state. In perceptrons, when the examples\\nare noisy and the weight decay is weak, the generalization error decreases in the\\nearly stage of learning, reaches a minimum and then increases towards its asymptotic value [12, 9]. Since the early stopping point sets in before the system reaches\\nthe steady state, most analyses based on the equilibrium state are not applicable.\\nCross-validation stopping has been proposed as an empirical method to control\\novertraining [13]. Here we propose the cavity method as a convenient alternative.\\n\\n\\f0.52\\nG----8 target\\n\\neQ)\\n\\n(b)\\n\\nG----EJ cavity\\n\\n0 - 0 LULOO\\n\\n<=\\n\\n0\\n\\n~\\n\\n.!::!\\n\\n0.46\\n\\nm\\n\\nQ)\\n<=\\n\\nQ)\\n\\n0>\\n\\n0.40\\n(d)\\n\\n(c)\\n\\neQ)\\n<=\\n\\n0\\n\\n~\\n\\n.!::!\\n\\nm\\n\\nQ)\\n<=\\n\\nQ)\\n\\n0>\\n\\n0.40\\n\\no\\n\\n0\\nweight decay A\\n\\n2\\nweight decay A\\n\\nFigure 1: (a-d) The dependence ofthe generalization error of the multilayer perceptron on the weight decay for N = 200, p = 700, nh = 3, (J = 0.8 in 4 samples. The\\nsolid symbols locate the optimal weight decays estimated by the teacher (circle), the\\ncavity method (square), extended LULOO (diamond) and sevenfold cross-validation\\n(triangle) .\\n\\nIn single-layer perceptrons, the cavity activations of the examples evolve according\\nto (6), enabling us to estimate the dynamical evolution of the estimated generalization error when learning proceeds. The remaining issue is the measurement of\\nthe time-dependent Green's function. We propose to introduce an initial homogeneous stimulus, that is, Jhj (t) = 1]J(t) for all j. Again, assuming normalized and\\nindependent inputs with (~j) = 0 and (~j~k) = Jjk , we can see from (4) that the\\nmeasurement (Wj(t) - Wj(t)) yields the quantity 1]G(t, 0).\\nWe will first consider systems that are time-translational invariant, i.e., G(t, s) =\\nG(t - s, 0). Such are the cases for Adaline learning and linear regression [9], where\\nthe cavity activation can be written as\\n\\nh'\\\"(t) = x'\\\"(t) +\\n\\nJ\\n\\ndsG(t - s, 0) OE(X'\\\"(S), y'\\\").\\nox,\\\"(s)\\n\\n(16)\\n\\nThis allows us to estimate the generalization error Eg(t) via Eg(t)\\n2:.,\\\" E(h'\\\"(t), y'\\\")/p, whose minimum in time determines the early stopping point.\\nTo verify the proposed method in linear regression, we randomly generate examples from a noisy teacher with y'\\\" = iJ . f'\\\" + (Jzw Here iJ is the teacher vector with B2 = 1.\\nand z'\\\" are independently generated with zero means and\\nunit variances. Learning is done by the gradient descent of the energy function\\nE(t) = 2:.,\\\" (y'\\\" - w(t) . f'\\\")2/2 . The generalization error Eg(t) is the error av-\\n\\ne;\\n\\neraged over the distribution of input [ and their corresponding output y, i.e.,\\nEg(t) = ((iJ . [ + (JZ - w? [)2/2). As far as the teacher is concerned, Eg(t) can\\nbe computed as Eg(t) = (1 - 2R(t) + Q(t) + (J2)/2. where R(t) = w(t) . iJ and\\nQ(t) = W(t)2.\\nFigure 2 shows the simulation results of 6 randomly generated samples. Three results are compared: the teacher's estimate, the cavity method and cross-validation.\\nSince LULOO is based on the equilibrium state, it cannot be used in the present\\n\\n\\fcontext. Again, we see that the cavity method yields estimates of the early stopping time with comparable precision as cross-validation. The ratio of the CPU time\\nbetween the cavity method and fivefold cross-validation is 1 : 1.4.\\nFor nonlinear regression and multilayer networks, the Green 's functions are not\\ntime-translational invariant. To estimate the Green 's functions in this case, we have\\ndevised another scheme of stimuli. Preliminary results for the determination of the\\nearly stopping point are satisfactory and final results will be presented elsewhere.\\n1 .1\\n\\neQ.i\\nc::\\n\\na\\n\\n~\\n\\n.!::!\\n\\n0.9\\n\\n~\\n\\n<l>\\n\\nc::\\n<l>\\n\\n0>\\n\\n0.7\\n\\neQ.i\\nc::\\n\\na\\n\\n~\\nc;;\\nQ.i\\n\\n.!::!\\n\\n0.9\\n\\nc::\\n<l>\\n\\n0>\\n\\n0.7\\n\\n0\\n\\n2\\ntime t\\n\\n0\\n\\n2\\ntime t\\n\\n0\\n\\n2\\n\\n4\\n\\ntime t\\n\\nFigure 2: (a-f) The evolution of the generalization error of linear regression for\\nN = 500, p = 600 and (J = 1. The solid symbols locate the early stopping points\\nestimated by the teacher (circle), the cavity method (square) and fivefold crossvalidation (diamond).\\n\\n4\\n\\nConclusion\\n\\nWe have proposed a method for the fast estimation of hyperparameters in large\\nnetworks, based on the linear response relation in the cavity method, combined\\nwith an empirical method of measuring the Green's function. Its efficiency depends\\non the independent and identical distribution of the inputs, greatly reducing the\\nnumber of networks to be monitored. It does not require the validation process\\nor the inversion of matrices of macroscopic size, and hence its speed compares\\nfavorably with cross-validation and other perturbative approaches such as extended\\nLULOO. For multilayer networks, we will explore further speedup of the Green 's\\nfunction measurement by multiplexing the stimuli to the different hidden units into\\na single network, to be compared with a reference network. We will also extend the\\ntechnique to other benchmark data to study its applicability.\\nOur initial success indicates that it is possible to generalize the method to more\\ncomplicated systems in the future. The concept of Green's functions is very general,\\nand its measurement by comparing the states of a stimulated system with a reference\\none can be adopted to general cases with suitable adaptation. Recently, much\\nattention is paid to the issue of model selection in support vector machines [3, 4, 5].\\nIt would be interesting to consider how the proposed method can contribute to these\\ncases.\\n\\n\\fAcknowledgements\\n\\nWe thank C. Campbell for interesting discussions and H. Nishimori for encouragement. This work was supported by the grant HKUST6157/99P from the Research\\nGrant Council of Hong Kong.\\n\\nReferences\\n[1] C. M. Bishop, Neural Networks for Pattern Recognition, Clarendon Press, Oxford\\n(1995).\\n[2] G. B. Orr and K-R. Muller, eds., Neural Networks: Tricks of th e Trad e, Springer,\\nBerlin (1998).\\n[3] O. Chapelle and V. N. Vapnik, Advances in Neural Information Processing Systems\\n12, S. A. Solla, T. KLeen and K-R. Muller, eds., MIT Press, Cambridge, 230 (2000).\\n[4] S. S. Keerthi, Technical Report CD-OI-02,\\nhttp://guppy.mpe.nus. edu .sg/ mpessk/nparm.html (2001).\\n[5] M. Opper and O. Winther , Advances in Large Margin Classifiers, A. J. Smola, P.\\nBartlett, B. Sch6lkopf and D. Schuurmans, eds., MIT Press, Cambridge, 43 (1999) .\\n[6] J. Larsen and L. K Hansen , Advances in Computational Math ematics 5 , 269 (1996).\\n[7] M. Opper and O. Winther , Phys. R ev. Lett. 76 , 1964 (1996).\\n[8] A. L. Fetter and J. D. Walecka, Quantum Theory of Many-Particle Systems, McGrawHill, New York (1971).\\n[9] K Y. M. Wong, S. Li and Y. W . Tong, Phys. Rev. E 62 , 4036 (2000).\\n[10] D. Saad and S. A. Solla, Phys. R ev. Lett. 74, 4337 (1995).\\n[11] W. H. Press, B. P. Flannery, S. A. Teukolsky and W . T. Vett erling, Num erical\\nR ecipes in C: Th e Art of Sci entific Computing, Cambridge University Press, Cambridge (1990).\\n[12] A. Krogh and J. A. Hertz , J. Phys. A 25 , 1135 (1992).\\n[13] S. Amari, N. Murata, K-R. Muller , M. Finke and H. H. Yang, IEEE Trans. on N eural\\nN etworks 8, 985 (1997) .\\n\\n\\f\",\n          \"550\\n\\nAckley and Littman\\n\\nGeneralization and scaling in reinforcement\\nlearning\\nDavid H. Ackley\\nMichael L. Littman\\nCognitive Science Research Group\\nBellcore\\nMorristown, NJ 07960\\n\\nABSTRACT\\nIn associative reinforcement learning, an environment generates input\\nvectors, a learning system generates possible output vectors, and a reinforcement function computes feedback signals from the input-output\\npairs. The task is to discover and remember input-output pairs that\\ngenerate rewards. Especially difficult cases occur when rewards are\\nrare, since the expected time for any algorithm can grow exponentially\\nwith the size of the problem. Nonetheless, if a reinforcement function\\npossesses regularities, and a learning algorithm exploits them, learning\\ntime can be reduced below that of non-generalizing algorithms. This\\npaper describes a neural network algorithm called complementary reinforcement back-propagation (CRBP), and reports simulation results\\non problems designed to offer differing opportunities for generalization.\\n\\n1\\n\\nREINFORCEMENT LEARNING REQUIRES SEARCH\\n\\nReinforcement learning (Sutton, 1984; Barto & Anandan, 1985; Ackley, 1988; Allen,\\n1989) requires more from a learner than does the more familiar supervised learning\\nparadigm. Supervised learning supplies the correct answers to the learner, whereas\\nreinforcement learning requires the learner to discover the correct outputs before\\nthey can be stored. The reinforcement paradigm divides neatly into search and\\nlearning aspects: When rewarded the system makes internal adjustments to learn\\nthe discovered input-output pair; when punished the system makes internal adjustments to search elsewhere.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n1.1\\n\\nMAKING REINFORCEMENT INTO ERROR\\n\\nFollowing work by Anderson (1986) and Williams (1988), we extend the backpropagation algorithm to associative reinforcement learning. Start with a \\\"garden variety\\\" backpropagation network: A vector i of n binary input units propagates\\nthrough zero or more layers of hidden units, ultimately reaching a vector 8 of m\\nsigmoid units, each taking continuous values in the range (0,1). Interpret each 8j\\nas the probability that an associated random bit OJ takes on value 1. Let us call\\nthe continuous, deterministic vector 8 the search vector to distinguish it from the\\nstochastic binary output vector o.\\nGiven an input vector, we forward propagate to produce a search vector 8, and\\nthen perform m independent Bernoulli trials to produce an output vector o. The\\ni - 0 pair is evaluated by the reinforcement function and reward or punishment\\nensues. Suppose reward occurs. We therefore want to make 0 more likely given i.\\nBackpropagation will do just that if we take 0 as the desired target to produce an\\nerror vector (0 - 8) and adjust weights normally.\\nNow suppose punishment occurs, indicating 0 does not correspond with i. By choice\\nof error vector, backpropagation allows us to push the search vector in any direction;\\nwhich way should we go? In absence of problem-specific information, we cannot pick\\nan appropriate direction with certainty. Any decision will involve assumptions. A\\nvery minimal \\\"don't be like 0\\\" assumption-employed in Anderson (1986), Williams\\n(1988), and Ackley (1989)-pushes s directly away from 0 by taking (8 - 0) as the\\nerror vector. A slightly stronger \\\"be like not-o\\\" assumption-employed in Barto &\\nAnandan (1985) and Ackley (1987)-pushes s directly toward the complement of 0\\nby taking ((1 - 0) - 8) as the error vector. Although the two approaches always\\nagree on the signs of the error terms, they differ in magnitudes. In this work,\\nwe explore the second possibility, embodied in an algorithm called complementary\\nreinforcement back-propagation ( CRBP).\\nFigure 1 summarizes the CRBP algorithm. The algorithm in the figure reflects three\\nmodifications to the basic approach just sketched. First, in step 2, instead of using\\nthe 8j'S directly as probabilities, we found it advantageous to \\\"stretch\\\" the values\\nusing a parameter v. When v < 1, it is not necessary for the 8i'S to reach zero or\\none to produce a deterministic output. Second, in step 6, we found it important\\nto use a smaller learning rate for punishment compared to reward. Third, consider\\nstep 7: Another forward propagation is performed, another stochastic binary output vector 0* is generated (using the procedure from step 2), and 0* is compared\\nto o. If they are identical and punishment occurred, or if they are different and\\nreward occurred, then another error vector is generated and another weight update\\nis performed. This loop continues until a different output is generated (in the case\\nof failure) or until the original output is regenerated (in the case of success). This\\nmodification improved performance significantly, and added only a small percentage\\nto the total number of weight updates performed.\\n\\n551\\n\\n\\f552\\n\\nAckley and Littman\\n\\nO. Build a back propagation network with input dimensionality n and output\\ndimensionality m. Let t = 0 and te = O.\\n1. Pick random i E 2n and forward propagate to produce a/s.\\n2. Generate a binary output vector o. Given a uniform random variable ~ E [0,1]\\nand parameter 0 < v < 1,\\nOJ\\n\\n=\\n\\n{1,\\n\\n0,\\n\\nif(sj - !)/v+! ~ ~j\\notherwise.\\n\\n3. Compute reinforcement r = f(i,o). Increment t. If r < 0, let te = t.\\n4. Generate output errors ej. If r > 0, let tj = OJ, otherwise let tj = 1- OJ. Let\\nej = (tj - sj)sj(l- Sj).\\n5. Backpropagate errors.\\n6. Update weights. 1:::..Wjk = 1]ekSj, using 1] = 1]+ if r ~ 0, and 1] = 1]- otherwise,\\nwith parameters 1]+,1]- > o.\\n7. Forward propagate again to produce new Sj's. Generate temporary output\\nvector 0*. If (r > 0 and 0* #- 0) or (r < 0 and 0* = 0), go to 4.\\n8. If te ~ t, exit returning te, else go to 1.\\n\\nFigure 1: Complementary Reinforcement Back Propagation-CRBP\\n\\n2\\n\\nON-LINE GENERALIZATION\\n\\nWhen there are many possible outputs and correct pairings are rare, the computational cost associated with the search for the correct answers can be profound.\\nThe search for correct pairings will be accelerated if the search strategy can effectively generalize the reinforcement received on one input to others. The speed of\\nan algorithm on a given problem relative to non-generalizing algorithms provides a\\nmeasure of generalization that we call on-line generalization.\\nO. Let z be an array of length 2n. Set the z[i] to random numbers from 0 to\\n2m - 1. Let t = te = O.\\n1. Pick a random input i E 2n.\\n2. Compute reinforcement r = f(i, z[i]). Increment t.\\n3. If r < 0 let z[i] = (z[i] + 1) mod 2m , and let te = t.\\n4. If te <t:: t exit returning t e, else go to 1.\\n\\nFigure 2: The Table Lookup Reference Algorithm Tref(f, n, m)\\nConsider the table-lookup algorithm Tref(f, n, m) summarized in Figure 2. In this\\nalgorithm, a separate storage location is used for each possible input. This prevents\\nthe memorization of one i - 0 pair from interfering with any other. Similarly,\\nthe selection of a candidate output vector depends only on the slot of the table\\ncorresponding to the given input. The learning speed of T ref depends only on the\\ninput and output dimensionalities and the number of correct outputs associated\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\nwith each input. When a problem possesses n input bits and n output bits, and\\nthere is only one correct output vector for each input vector, Tre{ runs in about 4n\\ntime (counting each input-output judgment as one.) In such cases one expects to\\ntake at least 2n - 1 just to find one correct i - 0 pair, so exponential time cannot be\\navoided without a priori information. How does a generalizing algorithm such as\\nCRBP compare to Trer?\\n\\n3\\n\\nSIMULATIONS ON SCALABLE PROBLEMS\\n\\nWe have tested CRBP on several simple problems designed to offer varying degrees\\nand types of generalization. In all of the simulations in this section, the following\\ndetails apply: Input and output bit counts are equal (n). Parameters are dependent\\non n but independent of the reinforcement function f. '7+ is hand-picked for each\\nn,l 11- = 11+/10 and II = 0.5. All data points are medians of five runs. The stopping\\ncriterion te ~ t is interpreted as te +max(2000, 2n+l) < t. The fit lines in the figures\\nare least squares solutions to a x bn , to two significant digits.\\nAs a notational convenience, let c = ~\\n\\n3.1\\n\\nn\\n\\nE ij\\n\\n;=1\\n\\n-\\n\\nthe fraction of ones in the input.\\n\\nn-MAJORlTY\\n\\nConsider this \\\"majority rules\\\" problem: [if c > ~ then 0 = In else 0 = on]. The i-o\\nmapping is many-to-l. This problem provides an opportunity for what Anderson\\n(1986) called \\\"output generalization\\\": since there are only two correct output states,\\nevery pair of output bits are completely correlated in the cases when reward occurs.\\n\\nG)\\n\\n'iii\\nu\\nrn\\n\\nC)\\n\\n0\\n\\n::::.\\nG)\\n\\nE\\n\\n;\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n\\nTable\\n\\nD\\n\\nCRBP n-n-n\\n\\n+ CRBP n-n\\n\\n10 3\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n456\\n\\n78\\n\\n91011121314\\n\\nn\\nFigure 3: The n-majority problem\\n\\nFigure 3 displays the simulation results. Note that although Trer is faster than\\nCRBP at small values of n, CRBP's slower growth rate (1.6n vs 4.2n ) allows it to\\ncross over and begin outperforming Trer at about 6 bits. Note also--in violation of\\n1 For n = 1 to 12. we used '1+\\n0.219. 0.170. 0.121}.\\n\\n= {2.000. 1.550. 1.130.0.979.0.783.0.709.0.623.0.525.0.280.\\n\\n553\\n\\n\\f554\\n\\nAckley and Littman\\n\\nsome conventional wisdom-that although n-majority is a linearly separable problem, the performance of CRBP with hidden units is better than without. Hidden\\nunits can be helpful--even on linearly separable problems-when there are opportunities for output generalization.\\n\\n3.2\\n\\nn-COPY AND THE 2k -ATTRACTORS FAMILY\\n\\nAs a second example, consider the n-copy problem: [0 = i]. The i-o mapping is now\\n1-1, and the values of output bits in rewarding states are completely uncorrelated,\\nbut the value of each output bit is completely correlated with the value of the\\ncorresponding input bit. Figure 4 displays the simulation results. Once again, at\\n\\nG)\\n\\n'ii\\n\\ntA\\nQ\\n0\\n\\n::::.\\nG)\\n\\n-\\n\\n.5\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n150*2.0I\\\\n\\n\\nD\\n\\n10 3\\n10 2\\n\\n12*2.2I\\\\n\\n\\n+\\n\\nTable\\nCRBP n-n-n\\nCRBP n-n\\n\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 4: The n-copy problem\\nlow values of n, Trer is faster, but CRBP rapidly overtakes Trer as n increases. In\\nn-copy, unlike n-majority, CRBP performs better without hidden units.\\nThe n-majority and n-copy problems are extreme cases of a spectrum. n-majority\\ncan be viewed as a \\\"2-attractors\\\" problem in that there are only two correct\\noutputs-all zeros and all ones-and the correct output is the one that i is closer\\nto in hamming distance. By dividing the input and output bits into two groups\\nand performing the majority function independently on each group, one generates\\na \\\"4-aUractors\\\" problem. In general, by dividing the input and output bits into\\n1 ~ Ie ~ n groups, one generates a \\\"2i:-attractors\\\" problem. When Ie = 1, nmajority results, and when Ie n, n-copy results.\\n\\n=\\n\\nFigure 5 displays simulation results on the n = 8-bit problems generated when Ie is\\nvaried from 1 to n. The advantage of hidden units for low values of Ie is evident,\\nas is the advantage of \\\"shortcut connections\\\" (direct input-to-output weights) for\\nlarger values of Ie. Note also that combination of both hidden units and shortcut\\nconnections performs better than either alone.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n105~--------------------------------~\\n\\nCASP 8-10-8\\n-+- CASP 8-8\\n.... CASP 8-10-Sls\\n-0-\\n\\n... Table\\n\\n3\\n\\n2\\n\\n1\\n\\n5\\n\\n4\\n\\n7\\n\\n6\\n\\n8\\n\\nk\\n\\nFigure 5: The 21:- attractors family at n = 8\\n\\n3.3\\n\\nn-EXCLUDED MIDDLE\\n\\nAll of the functions considered so far have been linearly separable. Consider this\\n\\\"folded majority\\\" function: [if\\n< c < then 0 on else 0 In]. Now, like\\nn-majority, there are only two rewarding output states, but the determination of\\nwhich output state is correct is not linearly separable in the input space. When\\nn = 2, the n-excluded middle problem yields the EQV (i.e., the complement of\\nXOR) function, but whereas functions such as n-parity [if nc is even then 0\\non\\nelse 0 = In] get more non-linear with increasing n, n-excluded middle does not.\\n\\ni\\n\\ni\\n\\n=\\n\\n=\\n\\n=\\n\\n107~------------------------------~~\\n\\n-\\n\\n10 6\\n10 5\\n\\nD)\\n\\n10 4\\n10 3\\n\\nI)\\n\\n'ii\\nu\\nf)\\n\\n.2\\n\\nI)\\n\\nE\\n\\n:::\\n\\nx\\nc\\n\\n17oo*1.6\\\"n\\n\\nTable\\n\\nCRSP n-n-n/s\\n\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 6: The n-excluded middle problem\\nFigure 6 displays the simulation results. CRBP is slowed somewhat compared to\\nthe linearly separable problems, yielding a higher \\\"cross over point\\\" of about 8 bits.\\n\\n555\\n\\n\\f556\\n\\nAckley and Littman\\n\\n4\\n\\nSTRUCTURING DEGENERATE OUTPUT SPACES\\n\\nAll of the scaling problems in the previous section are designed so that there is\\na single correct output for each possible input. This allows for difficult problems\\neven at small sizes, but it rules out an important aspect of generalizing algorithms\\nfor associative reinforcement learning: If there are multiple satisfactory outputs\\nfor given inputs, a generalizing algorithm may impose structure on the mapping it\\nproduces.\\nWe have two demonstrations of this effect, \\\"Bit Count\\\" and \\\"Inverse Arithmetic.\\\"\\nThe Bit Count problem simply states that the number of I-bits in the output should\\nequal the number of I-bits in the input. When n = 9, Tref rapidly finds solutions\\ninvolving hundreds of different output patterns. CRBP is slower--especially with\\nrelatively few hidden units-but it regularly finds solutions involving just 10 output\\npatterns that form a sequence from 09 to 19 with one bit changing per step.\\n0+Ox4=0\\n1+0x4=1\\n2+0x4=2\\n3+0x4=3\\n\\n0+2x4=8\\n1+2x4=9\\n2 + 2 x 4 = 10\\n3+2x4=11\\n\\n4+0x4=4 4+ 2 x 4 =\\n5+0x4=5 5 + 2 x 4 =\\n6+0x4=6 6 + 2 x 4 =\\n7+0x4=7 7 + 2 x 4 =\\n\\n12\\n13\\n14\\n15\\n\\n2+2-4=0 2+2+4=8\\n3+2-4=1 3+2+4=9\\n2+2+4=2 2 + 2 x 4 = 10\\n3+2+4=3 3+2x4=1l\\n6+2-4=4\\n7+2-4=5\\n6+2+4=6\\n7+2-.;-4=7\\n\\n6+\\n7+\\n6+\\n7+\\n\\n2+ 4 =\\n2+ 4 =\\n2x4=\\n2x4=\\n\\n0+4 x 4 = 16 0+6 x 4 =\\n1+4x4=17 1 + 6 x 4 =\\n2 + 4 x 4 = 18 2 + 6 x 4 =\\n3 +4 x 4 = 19 3 + 6 x 4 =\\n\\n24\\n25\\n26\\n27\\n\\n4+4\\n5+ 4\\n6+ 4\\n7+ 4\\n\\n=\\n=\\n=\\n=\\n\\n28\\n29\\n30\\n31\\n24\\n25\\n26\\n27\\n\\nx\\nx\\nx\\nx\\n\\n4=\\n4=\\n4=\\n4=\\n\\n6+ 6 + 4 =\\n7+6+4=\\n2+ 4 x 4 =\\n3+ 4 x 4=\\n\\n12 4 x 4 +\\n13 5 + 4 x\\n14 6 + 4 x\\n15 7 +4 x\\n\\n4=\\n4=\\n4\\n4=\\n\\n=\\n\\n20 4 + 6 x\\n21 5 + 6 x\\n22 6 + 6 x\\n23 7 + 6 x\\n\\n4\\n4\\n4\\n4\\n\\n16\\n17\\n18\\n19\\n\\n0+6 x\\n1+ 6 x\\n2+ 6x\\n3+ 6x\\n\\n4=\\n4=\\n4=\\n4=\\n\\n20\\n21\\n22\\n23\\n\\n4+\\n5+\\n6+\\n7+\\n\\n4 = 28\\n4 = 29\\n4 30\\n4 = 31\\n\\n6\\n6\\n6\\n6\\n\\nx\\nx\\nx\\nx\\n\\n=\\n\\nFigure 7: Sample CRBP solutions to Inverse Arithmetic\\n\\nThe Inverse Arithmetic problem can be summarized as follows: Given i E 25 , find\\n:1:, y, z E 23 and 0, <> E {+(OO)' -(01)' X (10)' +(11)} such that :I: oy<>z = i. In all there are\\n13 bits of output, interpreted as three 3-bit binary numbers and two 2-bit operators,\\nand the task is to pick an output that evaluates to the given 5-bit binary input\\nunder the usual rules: operator precedence, left-right evaluation, integer division,\\nand division by zero fails.\\nAs shown in Figure 7, CRBP sometimes solves this problem essentially by discovering positional notation, and sometimes produces less-globally structured solutions,\\nparticularly as outputs for lower-valued i's, which have a wider range of solutions.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n5\\n\\nCONCLUSIONS\\n\\nSome basic concepts of supervised learning appear in different guises when the\\nparadigm of reinforcement learning is applied to large output spaces. Rather than\\na \\\"learning phase\\\" followed by a \\\"generalization test,\\\" in reinforcement learning\\nthe search problem is a generalization test, performed simultaneously with learning.\\nInformation is put to work as soon as it is acquired.\\nThe problem of of \\\"overfitting\\\" or \\\"learning the noise\\\" seems to be less of an issue,\\nsince learning stops automatically when consistent success is reached. In experiments not reported here we gradually increased the number of hidden units on\\nthe 8-bit copy problem from 8 to 25 without observing the performance decline\\nassociated with \\\"too many free parameters.\\\"\\nThe 2 k -attractors (and 2 k -folds-generalizing Excluded Middle) families provide\\na starter set of sample problems with easily understood and distinctly different\\nextreme cases.\\nIn degenerate output spaces, generalization decisions can be seen directly in the\\ndiscovered mapping. Network analysis is not required to \\\"see how the net does it.\\\"\\nThe possibility of ultimately generating useful new knowledge via reinforcement\\nlearning algorithms cannot be ruled out.\\nReferences\\nAckley, D.H. (1987) A connectionist machine for genetic hillclimbing. Boston, MA: Kluwer\\nAcademic Press.\\nAckley, D.H. (1989) Associative learning via inhibitory search. In D.S. Touretzky (ed.),\\nAdvances in Neural Information Processing Systems 1, 20-28. San Mateo, CA: Morgan\\nKaufmann.\\nAllen, R.B. (1989) Developing agent models with a neural reinforcement technique. IEEE\\nSystems, Man, and Cybernetics Conference. Cambridge, MA.\\nAnderson, C.W. (1986) Learning and problem solving with multilayer connectionist systems. University of Mass. Ph.D. dissertation. COINS TR 86-50. Amherst, MA.\\nBarto, A.G. (1985) Learning by statistical cooperation of self-interested neuron-like computing elements. Human Neurobiology, 4:229-256.\\nBarto, A.G., & Anandan, P. (1985) Pattern recognizing stochastic learning automata.\\nIEEE Transactions on Systems, Man, and Cybernetics, 15, 360-374.\\nRumelhart, D.E., Hinton, G.E., & Williams, R.J. (1986) Learning representations by backpropagating errors. Nature, 323, 533-536.\\nSutton, R.S. (1984) Temporal credit assignment in reinforcement learning. University of\\nMass. Ph.D. dissertation. COINS TR 84-2. Amherst, MA.\\nWilliams, R.J. (1988) Toward a theory of reinforcement-learning connectionist systems.\\nCollege of Computer Science of Northeastern University Technical Report NU-CCS-88-3.\\nBoston, MA.\\n\\n557\\n\\n\\f\",\n          \"Discovering the Structure of a Reactive Environment by Exploration\\n\\nDiscovering the Structure of a Reactive Environment\\nby Exploration\\nMichael C. Mozer\\nDepartment of Computer Science\\nand Institute of Cognitive Science\\nUniversity of Colorado\\nBoulder, CO 80309-0430\\n\\nJonatban Bachrach\\nDepartmentofCompu~\\n\\nand Infonnation Science\\nUniversity of Massachusetts\\nAmherst, MA 01003\\n\\nABSTRACT\\nConsider a robot wandering around an unfamiliar environment. performing actions and sensing the resulting environmental states. The robot's task is to construct an internal model of its environment. a model that will allow it to predict\\nthe consequences of its actions and to determine what sequences of actions to\\ntake to reach particular goal states. Rivest and Schapire (1987&, 1987b;\\nSchapire. 1988) have studied this problem and have designed a symbolic algorithm to strategically explore and infer the structure of \\\"finite state\\\" environments. The heart of this algorithm is a clever representation of the environment\\ncalled an update graph. We have developed a connectionist implementation of\\nthe update graph using a highly-specialized network architecture. With back\\npropagation learning and a trivial exploration strategy - choosing random actions - the connectionist network can outperfonn the Rivest and Schapire algorithm on simple problems. The network has the additional strength that it\\ncan accommodate stochastic environments. Perhaps the greatest virtue of the\\nconnectionist approach is that it suggests generalizations of the update graph\\nrepresentation that do not arise from a traditional, symbolic perspective.\\n\\n1 INTRODUCTION\\nConsider a robot placed in an unfamiliar environment The robot is allowed to wander\\naround the environment, performing actions and sensing the resulting environmental\\nstates. With sufficient exploration, the robot should be able to construct an internal\\nmodel of the environment, a model that will allow it to predict the consequences of its actions and to determine what sequence of actions must be taken to reach a particular goal\\nstate. In this paper, we describe a connectionist network that accomplishes this task,\\nbased on a representation of finite-state automata developed by Rivest and Scbapire\\n\\n439\\n\\n\\f440\\n\\nMozer and Bachrach\\n\\n(1987a, 1987b; Schapire. 1988).\\nThe environments we wish to consider can be modeled by a finite-state automaton (FSA).\\nIn each environment. the robot has a set of discrete actions it can execute to move from\\none environmental state to another. At each environmental state. a set of binary-valued\\nsensations can be detected by the robot To illustrate the concepts and methods in our\\nwork, we use as an extended example a simple environment, the n -room world (from\\nRivest and Schapire). The n -room world consists of n rooms arranged in a circular\\nchain. Each room is connected to the two adjacent rooms. In each room is a light bulb\\nand light switch. The robot can sense whether the light in the room where it currently\\nstands is on or off. The robot has three possible actions: move to the next room down\\nthe chain (0). move to the next room up the chain (U). and toggle the light switch in the\\ncurrent room (T).\\n\\n2 MODELING THE ENVIRONMENT\\nIf the FSA corresponding to the n -room world is known, the sensory consequences of\\n\\nany sequence of actions can be predicted. Further. the FSA can be used to determine a\\nsequence of actions to take to obtain a certain goal state. Although one might try\\ndeveloping an algorithm to learn the FSA directly, there are several arguments against\\ndoing so (Schapire, 1988). Most important is that the FSA often does not capture structure inherent in the environment. Rather than trying to learn the FSA, Rivest and\\nScbapire suggest learning another representation of the environment called an update\\ngraph. The advantage of the update graph is that in environments with regularities, the\\nnumber of nodes in the update graph can be much smaller than in the FSA (e.g., 2n\\nversus 2\\\" for the n -room world). Rivest and Schapire's formal definition of the update\\ngraph is based on the notion of tests that can be performed on the environment. and the\\nequivalence of different tests. In this section, we present an alternative, more intuitive\\nview of the update graph that facilitates a connectionist interpretation.\\nConsider a three-room world. To model this environment, the essential knowledge required is the status of the lights in the current room (CUR), the next room up from the\\nClUTent room (UP), and the next room down from the current room (DOWN). Assume the\\nupdate graph has a node for each of these environmental variables. Further assume that\\neach node has an associated value indicating whether the light in the particular room is\\non or off.\\nIf we know the values of the variables in the current environmental state, what will their\\nnew values be after taking some action, say u1 When the robot moves to the next room\\nup, the new value of CUR becomes the previous value of UP; the new value of DOWN becomes the previous value of CUR; and in the three-room world, the new value of UP be-\\n\\ncomes the previous value of DOWN. As depicted in Figure la, this action thus results in\\nshifting values around in the three nodes. This makes sense because moving up does not\\naffect the status of any light, but it does alter the robot's position with respect to the three\\nrooms. Figure 1b shows the analogous flow of information for the action o. Finally, the\\naction T should cause the status of the current room's light to be complemented while the\\nother two rooms remain unaffected (Figure 1c). In Figure 1d, the three sets of links from\\nFigures la-c have been superimposed and have been labeled with the appropriate action.\\nOne final detail: The Rivest and Schapire update graph formalism does not make use of\\nthe \\\"complementation\\\" link. To avoid it, each node may be split into two values. one\\n\\n\\fDiscovering the Structure of a Reactive Environment by Exploration\\n\\nrepresenting the status of a room and the other its complement (Figure Ie). Toggling\\nthus involves exchanging the values of CUR and CUR. Just as the values of CUR, UP, and\\nDOWN must be shifted for the actions u and D, so must their complements.\\nGiven the update graph in Figure Ie and the value of each node for the current environmental state, the resuk of any sequence of actions can be predicted simply by shifting\\nvalues around in the graph. Thus, as far as predicting the input/output behavior of the environment is concerned, the update graph serves the same purpose as the FSA.\\nA defining and nonobvious (from the current description) property of an update graph is\\nthat each node has exactly one incoming link for each action. We call this the oneinput-per-action constraint. For example, CUR gets input from CUR for the action T,\\nfrom UP for u. and from DOWN for D.\\n(6)\\n\\n(a)\\n\\nN\\n\\n(c)\\n\\n@\\n\\n(d)\\n\\n~\\n\\n-~\\n\\nT\\n\\n(e)\\n\\nFlgure 1: (a) Links between nodes indicating the desired infonnation flow on pedonning the action u. CUR\\nrepresenu that status of the Jighu in the ament room, UP the status of the Jighu in the next room up, and DOWN\\nthe status of the lights in the next room down. (b) Links between nodes indicating the desired infonnation flow\\non perfonning the action D. (c) Links between nodes indicating the desired infonnation flow on perfonning the\\naction T. The \\\"_\\\" on the link from CUR to iuelf indicates that the value must be complemented. (d) Links\\nfrom the three separate actions superimposed and labeled by the action. (e) The complementation link can be\\navoided by adding a set of nodes that represent the complemenu of the original seL Thil is the update grapb for\\na three-room world.\\n\\n441\\n\\n\\f442\\n\\nMozer and Bachrach\\n\\n3 THE RIVEST AND SCHAPIRE ALGORITHM\\nRivest and Schapire have developed a symbolic algorithm (hereafter, the RS algorithm) to\\nstrategically explore an environment and learn its update graph representation. The RS\\nalgorithm fonnulates explicit hypotheses about regularities in the environment and tests\\nthese hypotheses one or a relatively small number at a time. As a result, the algorithm\\nmay not make full use of the environmental feedback obtained. It thus seems worthwhile\\nto consider alternative approaches that could allow more efficient use of the environmental feedback, and hence, more efficient learning of the update graph. We have taken connectionist approach, which has shown quite promising results in preliminary experiments\\nand suggests other significant benefits. We detail these benefits below, but must first\\ndescribe the basic approach.\\n\\n4 THE UPDATE GRAPH AS A CONNECTIONIST NETWORK\\nHow might we tum the update graph into a connectionist network? Start by asswning\\none unit in a network for each node in the update graph. The activity level of the unit\\nrepresents the truth value associated with the update graph node. Some of these units\\nserve as \\\"outputs\\\" of the network. For example, in the three-room world, the output of\\nthe network is the unit that represents the status of the current room. In other enviroDments, there may several sensations in which case there will be several output units.\\nWhat is the analog of the labeled links in the update graph? The labels indicate that\\nvalues are to be sent down a link when a particular action occurs. In connectionist tenns,\\nthe links should be gated by the action. To elaborate, we might include a set of units that\\nrepresent the possible actions; these units act to multiplicatively gate the flow of activity\\nbetween units in the update graph. Thus, when a particular action is to be perfonned, the\\ncorresponding action unit is activated, and the connections that are gated by this action\\nbecome enabled. If the action units fonn a local representation, i.e., only one is active at\\na time, exactly one set of connections is enabled at a time. Consequently, the gated connections can be replaced by a set of weight matrices, one per action. To predict the\\nconsequences of a particular action, the weight matrix for that action is plugged into the\\nnetwork and activity is allowed to propagate through the connections. Thus, the network\\nis dynamically rewired contingent on the current action.\\nThe effect of activity propagation should be that the new activity of a unit is the previous\\nactivity of some other unit A linear activation function is sufficient to achieve this:\\nX(t) =Wa(t)X(t-l),\\n\\n(1)\\n\\nwhere a (t) is the action selected at time t, Wa (t) is the weight matrix associated with this\\naction, and X(t) is the activity vector that results from taking action a (t). Assuming\\nweight matrices which have zeroes in each row except for one connection of strength 1\\n(the one-input-per-action constraint), the activation rule will cause activity values to be\\ncopied around the network.\\n\\n5 TRAINING THE NETWORK TO BE AN UPDATE GRAPH\\nWe have described a connectionist network that can behave as an update graph, and now\\ntum to the procedure used to learn the connection strengths in this network. For expository purposes, assume that the number of units in the update graph is known in advance.\\n\\n\\fDiscovering the Structure of a Reactive Environment by Exploration\\n(This is not necessary, as we show in Mozer & Bachrach, 1989.) A weight matrix is required for each action, with a potential non-zero connection between every pair of units.\\nAs in most connectionist learning procedures, the weight matrices are initialized to random values; the outcome of learning will be a set of matrices that represent the update\\ngraph connectivity.\\nIf the network is to behave as an update graph, the one-input-per-action constraint must\\nbe satisfied. In terms of the connectivity matrices, this means that each row of each\\nweight matrix should have connection strengths of zero except for one value which is 1.\\nTo achieve this property, additional constraints are placed on the weights. We have explored a combination of three constraints:\\n(1) l:w~j = 1,\\nj\\n\\n(2) l:Waij = 1,\\n\\nand (3) Waij ~ 0,\\n\\nj\\n\\nwhere waij is the connection strength to i from j for action a. Constraint 1 is satisfied by\\nintroducing an additional cost term to the error function. Constraints 2 and 3 are rigidly\\nenforced by renormalizing the Wai following each weight update. The normalization\\nprocedure finds the shortest distance projection from the updated weight vector to the hyperplane specified by constraint 2 that also satisfies constraint 3.\\nAt each time step t, the training procedure consists the following sequence of events:\\n1. An action. a (t), is selected at random.\\n2. The weight matrix for that action, Wa(t). is used to compute the activities at t, X(t),\\nfrom the previous activities X(t-l).\\n3. The selected action is performed on the environment and the resulting sensations are\\nobserved.\\n4. The observed sensations are compared with the sensations predicted by the network\\n(Le., the activities of units chosen to represent the sensations) to compute a measure of\\nerror. To this error is added the contribution of constraint 1.\\n5. The back propagation \\\"unfolding-in-time\\\" procedure (Rumelhart, Hinton. & Williams,\\n1986) is used to compute the derivative of the error with respect to weights at the\\ncurrent and earlier time steps, Wa(t-;)' for i =0 ... 't-l.\\n\\n6. The weight matrices for each action are updated using the overall error gradient and\\nthen are renormalized to enforce constraints 2 and 3.\\n7. The temporal record of unit activities, X(t-i) for i=O? .. 't, which is maintained to\\npermit back propagation in time, is updated to reflect the new weights. (See further\\nexplanation below.)\\n8. The activities of the output units at time t, which represent the predicted sensations,\\nare replaced by the actual observed sensations.\\nSteps 5-7 require further elaboration. The error measured at time t may be due to incorrect propagation of activities from time t-l, which would call for modification of the\\nweight matrix Wa(t). But the error may also be attributed to incorrect propagation of activities at earlier times. Thus. back propagation is usui to assign blame to the weights at\\nearlier times. One critical parameter of training is the amount of temporal history, 't, to\\nconsider. We have found that. for a particular problem, error propagation beyond a cer-\\n\\n443\\n\\n\\f444\\n\\nMozer and Bachrach\\n\\nlain critical number of steps does not improve learning performance, although any fewer\\ndoes indeed harm performance. In the results described below, we set 't for a particular\\nproblem to what appeared to be a safe limit: one less than the number of nodes in the update graph solution of the problem.\\nTo back propagate error in time, we maintain a temporal record of unit activities. However, a problem arises with these activities following a weight update: the activities are\\nno longer consistent with the weights - i.e., Equation I is violated. Because the error\\nderivatives computed by back propagation are exact only when Equation I is satisfied,\\nfuture weight updates based on the inconsistent activities are not assured of being correct.\\nEmpirically, we have found the algorithm extremely unstable if we do not address this\\nproblem.\\nIn most situations where back propagation is applied to temporally-extended sequences.\\nthe sequences are of finite length. Consequently. it is possible to wait until the end of the\\nsequence to update the weights, at which point consistency between activities and\\nweights no longer matters because the system starts fresh at the beginning of the next sequence. In the present situation. however, the sequence of actions does not tenninate.\\nWe thus were forced to consider alternative means of ensuring consistency. The most\\nsuccessful approach involved updating the activities after each weight change to force\\nconsistency (step 7 of the list above). To do this, we propagated the earliest activities in\\nthe temporal record. X(t--'t). forward again to time t, using the updated weight matrices.\\n\\n6 RESULTS\\nFigure 2 shows the weights in the update graph network for the three-room world after\\nthe robot has taken 6,000 steps. The Figure depicts a connectivity pattern identical to\\nthat of the update graph of Figure Ie. To explain the correspondence, think of the diagram as being in the shape of a person who has a head, left and right arms, left and right\\nlegs, and a heart. For the action U, the head - the output unit - receives input from\\nthe left leg, the left leg from the heart, and the heart from the head, thereby fonning a\\nthree-unit loop. The other three units - the left arm, right arm, and right leg - fonn a\\n\\nFlgure 2: Weights learned after 6,000 exploratory steps in the three-room world. Each large diagram\\nrepresents the weights corresponding to one of the three actic.lI. Each small diagram contained within a large\\ndiagram represents the connection strengths feeding into a particular Wlit for a particular action. There are six\\nWlits, hence six small diagrams. The output Wlit, which indicates the state of the light in the wrrent room, is the\\nprotruding \\\"head\\\" of the large diagram. A white square in a particular position of a small diagram represents the\\nstrength of connection from the unit in the homologous position in the large diagram to the unit represented by\\nthe small diagram. The area of the square is proportional to the cormection strength.\\n\\n\\fDiscovering the Structure of a Reactive Environment by Exploration\\n\\nsimilar loop. For the action D, the same two loops are present but in the reverse direction. These two loops also appear in Figure Ie. For the action T, the left and right anns,\\nheart, and left leg each keep their current value, while the head and the right leg exchange values. This corresponds to the exchange of values between the CUR and CUR\\nnodes of the Figure Ie.\\nIn addition to learning the update graph connectivity, the network has simultaneously\\nlearned the correct activity values associated with each node for the current state of the\\nenvironment. Armed with this infonnation, the network can predict the outcome of any\\nsequence of actions. Indeed, the prediction error drops to zero, causing learning to cease\\nand the network to become completely stable.\\nNow for the bad news: The network does not converge for every set of random initial\\nweights, and when it does, it requires on the order of 6,000 steps. However, when the\\nweight constraints are removed, that the network converges without fail and in about 300\\nsteps. In Mozer and Bachrach (1989), we consider why the weight constraints are hannful and suggest several remedies. Without weight constraints, the resulting weight matrix, which contains a collection of positive and negative weights of varying magnitudes,\\nis not readily interpreted. In the case of the n -room world, . one reason why the final\\nweights are difficult to interpret is because the net has discovered a solution that does not\\nsatisfy the RS update graph fonnalism; it has discovered the notion of complementation\\nlinks of the sort shown in Figure ld. With the use of complementation links, only three\\nunits are required, not six. Consequently, the three unnecessary units are either cut out of\\nthe solution or encode infonnation redundantly.\\nTable 1 compares the perfonnance of the RS algorithm against that of the connectionist\\nnetwork without weight constraints for several environments. Perfonnance is measured\\nin tenns of the median number of actions the robot must take before it is able to predict\\nthe outcome of subsequent actions. (Further details of the experiments can be found in\\nMozer and Bachrach, 1989.) In simple environments, the connectionist update graph can\\noutperfonn the RS algorithm. This result is quite surprising when considering that the action sequence used to train the network is generated at random, in contrast to the RS algorithm, which involves a strategy for exploring the environment. We conjecture that the\\nnetwork does as well as it does because it considers and updates many hypotheses in\\nparallel at each time step. In complex environments, however, the network does poorly.\\nBy \\\"complex\\\", we mean that the number of nodes in the update graph is quite large and\\nthe number of distinguishing environmental sensations is relatively small. For example,\\nthe network failed to learn a 32-room world, whereas the RS algorithm succeeded. An\\nintelligent exploration strategy seems necessary in this case: random actions will take\\ntoo long to search the state space. This is one direction our future work will take.\\nBeyond the potential speedups offered by connectionist learning algorithms, the connectionist approach has other benefits.\\nTable 1: Nwnber of Steps Required to Learn Update Graph\\nEnvironment\\nLittle Prince Wodd\\nCar Radio World\\nFour-Room World\\n32-Room World\\n\\nRS\\nAlgorithm\\n200\\n27,695\\n1,388\\n52,436\\n\\nConnectionist\\nUpdate Graph\\n91\\n8,167\\n1,308\\nfails\\n\\n445\\n\\n\\f446\\n\\nMozer and Bachrach\\n\\n? Perfonnance of the network appears insensitive to prior knowledge of the number of\\nnodes in the update graph being learned. In contrast, the RS algorithm requires an\\nupper bound on the update graph complexity, and performance degrades significantly if\\nthe upper bound isn't tight.\\n? The network is able to accommodate \\\"noisy\\\" environments, also in contrast to the RS\\nalgorithm.\\n? Owing learning, the network continually makes predictions about what sensations will\\nresult from a particular action, and these predictions improve with experience. The RS\\nalgorithm cannot make predictions until learning is complete; it could perhaps be\\nmodified to do so, but there would be an associated cost.\\n? Treating the update graph as matrices of connection strengths has suggested generalizations of the update graph formalism that don't arise from a more traditional analysis.\\nFirst, there is the fairly direct extension of allowing complementation links. Second,\\nbecause the connectionist network is a linear system. any rank-preserving linear\\ntransform of the weight matrices will produce an equivalent system, but one that does\\nnot have the local connectivity of the update graph (see Mozer & Bachrach, 1989).\\nThe linearity of the network also allows us to use tools of linear algebra to analyze the\\nresulting connectivity matrices.\\nThese benefits indicate that the connectionist approach to the environment-modeling\\nproblem is worthy of further study. We do not wish to claim that the connectionist approach supercedes the impressive work of Rivest and Schapire. However, it offers complementary strengths and alternative conceptualizations of the learning problem.\\nAcknowledgements\\nOur thanks to Rob Schapire, Paul Smolensky, and Rich Sutton for helpful discussions. This work\\nwas supported by a grant from the James S. McDonnell Foundation to Michael Mozer. grant 87-236 from the Sloan Foundation to Geoffrey Hinton. and grant AFOSR-87\\\"()()30 from the Air Force\\nOffice of Scientific Research. Bolling AFB, to Andrew Barto.\\n\\nReferences\\nMozer, M. C., & Bachrach, J. (1989). Discovering the structure of a reactive environment by\\nexploration (Teclmical Report CU-CS-451-89). Boulder, CO: University of Colorado,\\nDepartment of Computer Science.\\nRivest, R. L., & Schapire, R. E. (1987). Diversity-based inference of finite automata. In\\nProceedings of the Twenty-Eighth Annual Symposium on Foundations of Computer\\nScience (pp. 78-87).\\nRivest, R. L., & Schapire, R. E. (1987). A new approach to unsupervised learning in detenninistic\\nenvironments. In P. Langley (Ed.), Proceedings of the Fourth Inlernational Workslwp on\\nMachine Learning (pp. 364-375).\\nRumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning internal representations by\\nerror propagation. In D. E. Rumelhart & J. L. McClelland (Eds.), Parallel distributed\\nprocessing: Explorations in the microstructure of cognition. Volume I: Foundations (pp.\\n318-362). Cambridge, MA: MIT Press/Bradford Books.\\nSchapire, R. E. (1988). Diversity-based inference ofjiniJe automara. Unpublished master's thesis,\\nMassachusetts Instiblte of Technology, Cambridge, MA.\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Remove the columns\n",
        "papers = papers.drop(columns=['id', 'event_type', 'pdf_name'], axis=1).sample(100)\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euUYw-ke3Z_X"
      },
      "source": [
        "##### Remove punctuation/lower casing\n",
        "\n",
        "Next, let’s perform a simple preprocessing on the content of `paper_text` column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "8QoaLzTw3Z_Y",
        "outputId": "390bcf21-28fd-4e2f-a0dc-2cc587873818"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1661    332\\n\\nhormel\\n\\na sell-organizing associative...\n",
              "1819    conditional random fields for object\\nrecognit...\n",
              "1972    robust fisher discriminant analysis\\n\\nseung-j...\n",
              "1888    theories of access consciousness\\nmichael d co...\n",
              "6150    mocap-guided data augmentation\\nfor 3d pose es...\n",
              "Name: paper_text_processed, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_text_processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1661</th>\n",
              "      <td>332\\n\\nhormel\\n\\na sell-organizing associative...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1819</th>\n",
              "      <td>conditional random fields for object\\nrecognit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1972</th>\n",
              "      <td>robust fisher discriminant analysis\\n\\nseung-j...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1888</th>\n",
              "      <td>theories of access consciousness\\nmichael d co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6150</th>\n",
              "      <td>mocap-guided data augmentation\\nfor 3d pose es...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Load the regular expression library\n",
        "import re\n",
        "\n",
        "# Remove punctuation\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "\n",
        "# Convert the titles to lowercase\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text_processed'].map(lambda x: x.lower())\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers['paper_text_processed'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wND-00wu3Z_Y"
      },
      "source": [
        "** **\n",
        "#### Step 3: Exploratory Analysis <a class=\"anchor\\\" id=\"eda\"></a>\n",
        "** **\n",
        "\n",
        "To verify whether the preprocessing, we’ll make a simple word cloud using the `wordcloud` package to get a visual representation of most common words. It is key to understanding the data and ensuring we are on the right track, and if any more preprocessing is necessary before training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "vwdtZ8aa3Z_Z",
        "outputId": "b9607f1f-74aa-4e7e-d8c4-925f0ba5f7d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=400x200>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Import the wordcloud library\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Join the different processed titles together.\n",
        "long_string = ','.join(list(papers['paper_text_processed'].values))\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-mIE02u3Z_a"
      },
      "source": [
        "** **\n",
        "#### Step 4: Prepare text for LDA analysis <a class=\"anchor\\\" id=\"data_preparation\"></a>\n",
        "** **\n",
        "\n",
        "Next, let’s work to transform the textual data in a format that will serve as an input for training LDA model. We start by tokenizing the text and removing stopwords. Next, we convert the tokenized object into a corpus and dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq4r6jsG3Z_a",
        "outputId": "3ddbadf7-2e78-4414-ff7b-7fa7b196bd0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hormel', 'sell', 'organizing', 'associative', 'memory', 'system', 'lor', 'control', 'applications', 'michael', 'bormel', 'department', 'control', 'theory', 'robotics', 'technical', 'university', 'darmstadt', 'schlossgraben', 'darmstadt', 'gerany', 'abstract', 'chac', 'storage', 'scheme', 'used', 'basis', 'software', 'implementation', 'associative']\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc))\n",
        "             if word not in stop_words] for doc in texts]\n",
        "\n",
        "\n",
        "data = papers.paper_text_processed.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "# remove stop words\n",
        "data_words = remove_stopwords(data_words)\n",
        "\n",
        "print(data_words[:1][0][:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8UQhwY33Z_a",
        "outputId": "e71f1cd5-9e8c-4d4c-ad05-a0be957edf36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 3), (6, 1), (7, 1), (8, 1), (9, 6), (10, 2), (11, 2), (12, 1), (13, 3), (14, 1), (15, 1), (16, 1), (17, 1), (18, 2), (19, 1), (20, 1), (21, 1), (22, 1), (23, 3), (24, 3), (25, 3), (26, 5), (27, 1), (28, 1), (29, 1)]\n"
          ]
        }
      ],
      "source": [
        "import gensim.corpora as corpora\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_words)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_words\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1][0][:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on1fHkMm3Z_b"
      },
      "source": [
        "** **\n",
        "#### Step 5: LDA model tranining <a class=\"anchor\\\" id=\"train_model\"></a>\n",
        "** **\n",
        "\n",
        "To keep things simple, we'll keep all the parameters to default except for inputting the number of topics. For this tutorial, we will build a model with 10 topics where each topic is a combination of keywords, and each keyword contributes a certain weightage to the topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLxpRjbs3Z_b",
        "outputId": "e3d8c3c5-2d0e-413d-d7b9-5352da148090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.007*\"data\" + 0.006*\"model\" + 0.004*\"time\" + 0.004*\"learning\" + '\n",
            "  '0.004*\"set\" + 0.004*\"one\" + 0.004*\"two\" + 0.004*\"algorithm\" + '\n",
            "  '0.003*\"number\" + 0.003*\"using\"'),\n",
            " (1,\n",
            "  '0.008*\"model\" + 0.005*\"learning\" + 0.004*\"one\" + 0.004*\"function\" + '\n",
            "  '0.004*\"data\" + 0.004*\"using\" + 0.004*\"set\" + 0.003*\"used\" + 0.003*\"time\" + '\n",
            "  '0.003*\"given\"'),\n",
            " (2,\n",
            "  '0.008*\"model\" + 0.006*\"data\" + 0.005*\"learning\" + 0.005*\"using\" + '\n",
            "  '0.004*\"one\" + 0.004*\"set\" + 0.004*\"algorithm\" + 0.003*\"two\" + '\n",
            "  '0.003*\"figure\" + 0.003*\"models\"'),\n",
            " (3,\n",
            "  '0.006*\"model\" + 0.005*\"one\" + 0.004*\"learning\" + 0.004*\"algorithm\" + '\n",
            "  '0.004*\"using\" + 0.004*\"figure\" + 0.004*\"function\" + 0.003*\"data\" + '\n",
            "  '0.003*\"set\" + 0.003*\"two\"'),\n",
            " (4,\n",
            "  '0.009*\"model\" + 0.005*\"data\" + 0.005*\"learning\" + 0.004*\"set\" + '\n",
            "  '0.004*\"function\" + 0.004*\"using\" + 0.004*\"one\" + 0.004*\"time\" + '\n",
            "  '0.004*\"algorithm\" + 0.003*\"distribution\"'),\n",
            " (5,\n",
            "  '0.005*\"model\" + 0.005*\"set\" + 0.005*\"algorithm\" + 0.004*\"data\" + '\n",
            "  '0.004*\"learning\" + 0.004*\"time\" + 0.004*\"training\" + 0.004*\"function\" + '\n",
            "  '0.003*\"using\" + 0.003*\"input\"'),\n",
            " (6,\n",
            "  '0.007*\"model\" + 0.006*\"data\" + 0.006*\"learning\" + 0.005*\"state\" + '\n",
            "  '0.004*\"using\" + 0.004*\"function\" + 0.004*\"one\" + 0.004*\"figure\" + '\n",
            "  '0.004*\"number\" + 0.004*\"models\"'),\n",
            " (7,\n",
            "  '0.006*\"data\" + 0.005*\"model\" + 0.005*\"learning\" + 0.004*\"function\" + '\n",
            "  '0.004*\"set\" + 0.004*\"figure\" + 0.004*\"one\" + 0.003*\"models\" + 0.003*\"using\" '\n",
            "  '+ 0.003*\"two\"'),\n",
            " (8,\n",
            "  '0.005*\"data\" + 0.005*\"algorithm\" + 0.005*\"set\" + 0.004*\"model\" + '\n",
            "  '0.004*\"using\" + 0.003*\"one\" + 0.003*\"time\" + 0.003*\"learning\" + '\n",
            "  '0.003*\"number\" + 0.003*\"figure\"'),\n",
            " (9,\n",
            "  '0.008*\"model\" + 0.005*\"data\" + 0.004*\"one\" + 0.004*\"set\" + 0.004*\"using\" + '\n",
            "  '0.004*\"function\" + 0.003*\"figure\" + 0.003*\"models\" + 0.003*\"learning\" + '\n",
            "  '0.003*\"two\"')]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# number of topics\n",
        "num_topics = 10\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=num_topics)\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "915mQr7W3Z_b"
      },
      "source": [
        "** **\n",
        "#### Step 6: Analyzing our LDA model <a class=\"anchor\\\" id=\"results\"></a>\n",
        "** **\n",
        "\n",
        "Now that we have a trained model let’s visualize the topics for interpretability. To do so, we’ll use a popular visualization package, pyLDAvis which is designed to help interactively with:\n",
        "\n",
        "1. Better understanding and interpreting individual topics, and\n",
        "2. Better understanding the relationships between the topics.\n",
        "\n",
        "For (1), you can manually select each topic to view its top most frequent and/or “relevant” terms, using different values of the λ parameter. This can help when you’re trying to assign a human interpretable name or “meaning” to each topic.\n",
        "\n",
        "For (2), exploring the Intertopic Distance Plot can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyLDAvis\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KewwF014KyG",
        "outputId": "a6d225ac-016d-4596-d90f-eb9f1b8b044f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.10.1)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.5.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (75.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.16.0)\n",
            "Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-2.0 pyLDAvis-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./results', exist_ok=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3DRNyQL45uz",
        "outputId": "161f4169-9aa1-4023-e24d-6fdd583252ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "weswAQMZ3Z_b",
        "outputId": "b273b0c1-4079-4bc1-eec4-4d24a854a1d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "6      0.002665  0.006560       1        1  20.950405\n",
              "4      0.009912  0.002203       2        1  17.294550\n",
              "2      0.000382 -0.004567       3        1  14.694617\n",
              "5     -0.008764  0.001373       4        1   9.588772\n",
              "8     -0.005531 -0.000543       5        1   9.434646\n",
              "1      0.002733 -0.003569       6        1   7.603974\n",
              "0     -0.001849  0.002677       7        1   7.240271\n",
              "7      0.000757  0.002216       8        1   7.239707\n",
              "9      0.001794 -0.007407       9        1   4.437570\n",
              "3     -0.002098  0.001057      10        1   1.515489, topic_info=          Term         Freq        Total Category  logprob  loglift\n",
              "246      model  1624.000000  1624.000000  Default  30.0000  30.0000\n",
              "270        one   947.000000   947.000000  Default  29.0000  29.0000\n",
              "93        data  1298.000000  1298.000000  Default  28.0000  28.0000\n",
              "209   learning  1145.000000  1145.000000  Default  27.0000  27.0000\n",
              "153     figure   775.000000   775.000000  Default  26.0000  26.0000\n",
              "..         ...          ...          ...      ...      ...      ...\n",
              "385       time     8.952761   827.421506  Topic10  -6.0180  -0.3369\n",
              "665  inference     6.765091   435.079782  Topic10  -6.2982   0.0257\n",
              "717        log     7.113907   585.178894  Topic10  -6.2479  -0.2204\n",
              "474       case     6.874430   509.894154  Topic10  -6.2822  -0.1170\n",
              "747     models     7.012440   743.335346  Topic10  -6.2623  -0.4740\n",
              "\n",
              "[877 rows x 6 columns], token_table=       Topic      Freq       Term\n",
              "term                             \n",
              "3863       3  0.377490  acclaimed\n",
              "12133      1  0.197266     ackley\n",
              "12133      2  0.098633     ackley\n",
              "12133      3  0.295899     ackley\n",
              "12133      4  0.098633     ackley\n",
              "...      ...       ...        ...\n",
              "4184       6  0.138880         yn\n",
              "4184       7  0.046293         yn\n",
              "4184       8  0.092587         yn\n",
              "4184       9  0.030862         yn\n",
              "17296      5  0.380362       zijc\n",
              "\n",
              "[3868 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[7, 5, 3, 6, 9, 2, 1, 8, 10, 4])"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el3331359576286932805634280779\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el3331359576286932805634280779_data = {\"mdsDat\": {\"x\": [0.002664610554802297, 0.009911926351040042, 0.00038232120116852306, -0.008764344485744547, -0.005531321699311458, 0.0027329324817390657, -0.0018487668186586892, 0.0007567544450253958, 0.0017943510079689426, -0.002098463038029582], \"y\": [0.0065595537081451875, 0.002202715736003568, -0.004567120134858512, 0.0013728591801393435, -0.0005425497401717909, -0.0035687730410074872, 0.0026767920400986517, 0.0022163157468882296, -0.007407121181777219, 0.0010573276865400282], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [20.95040460701163, 17.294550468262056, 14.694616792156733, 9.588771744092478, 9.434645610130879, 7.60397359207676, 7.240270881377915, 7.239707393015518, 4.437569942010378, 1.5154889698656369]}, \"tinfo\": {\"Term\": [\"model\", \"one\", \"data\", \"learning\", \"figure\", \"function\", \"set\", \"algorithm\", \"two\", \"using\", \"training\", \"results\", \"number\", \"graph\", \"used\", \"network\", \"time\", \"input\", \"information\", \"given\", \"linear\", \"networks\", \"problem\", \"first\", \"log\", \"also\", \"models\", \"based\", \"vector\", \"approach\", \"jumps\", \"mjp\", \"berlinde\", \"bursting\", \"blanche\", \"sgcp\", \"inhomogeneous\", \"bursts\", \"arrival\", \"lengthscale\", \"chinese\", \"restaurant\", \"xo\", \"uniformization\", \"sudden\", \"mmpp\", \"novellino\", \"mmpps\", \"herten\", \"florianstimberg\", \"florian\", \"dqh\", \"mjps\", \"molecule\", \"stimberg\", \"join\", \"perkel\", \"polytrodes\", \"ruttor\", \"voice\", \"jump\", \"poisson\", \"pik\", \"states\", \"modulated\", \"state\", \"sampler\", \"lsp\", \"offline\", \"action\", \"gamma\", \"rate\", \"segments\", \"dirichlet\", \"number\", \"learning\", \"models\", \"figure\", \"pose\", \"et\", \"inference\", \"new\", \"neural\", \"weight\", \"data\", \"function\", \"human\", \"results\", \"model\", \"time\", \"given\", \"using\", \"images\", \"process\", \"one\", \"distribution\", \"since\", \"first\", \"case\", \"algorithm\", \"set\", \"two\", \"large\", \"log\", \"used\", \"functions\", \"performance\", \"linear\", \"network\", \"training\", \"approach\", \"different\", \"also\", \"information\", \"aoa\", \"lambon\", \"sse\", \"aw\", \"ellis\", \"morrison\", \"gerhand\", \"spelling\", \"ecoc\", \"hazards\", \"latency\", \"late\", \"acquisition\", \"correlates\", \"naming\", \"pgmm\", \"lateness\", \"spoken\", \"wingfield\", \"autoencoding\", \"ajk\", \"anova\", \"jared\", \"hazard\", \"loo\", \"staged\", \"inco\", \"assoon\", \"nataoqu\", \"carrojl\", \"ssm\", \"consistency\", \"nca\", \"rmse\", \"pecoc\", \"acoustic\", \"frequency\", \"ht\", \"gsn\", \"un\", \"survival\", \"sample\", \"ti\", \"model\", \"effects\", \"fisher\", \"distribution\", \"patterns\", \"posterior\", \"market\", \"function\", \"however\", \"space\", \"prior\", \"sampling\", \"xt\", \"learning\", \"set\", \"noise\", \"using\", \"models\", \"one\", \"time\", \"data\", \"parameters\", \"case\", \"matrix\", \"different\", \"random\", \"algorithm\", \"problem\", \"log\", \"state\", \"error\", \"training\", \"results\", \"given\", \"method\", \"also\", \"number\", \"two\", \"figure\", \"used\", \"information\", \"first\", \"stitching\", \"face\", \"dpm\", \"viewpoint\", \"faces\", \"yjl\", \"visibility\", \"anchor\", \"deformations\", \"dstitch\", \"ikl\", \"hedau\", \"te\", \"worker\", \"objectivity\", \"crbp\", \"stich\", \"scoreparts\", \"appearance\", \"dv\", \"irish\", \"norwich\", \"rescore\", \"rai\", \"tticedu\", \"uij\", \"acclaimed\", \"ackley\", \"bed\", \"akq\", \"aspect\", \"kq\", \"ijk\", \"cuboid\", \"confusion\", \"deformation\", \"deformable\", \"orthonormal\", \"reinforcement\", \"tensors\", \"rank\", \"norm\", \"schmidt\", \"gradient\", \"model\", \"methods\", \"learning\", \"small\", \"statistical\", \"jk\", \"using\", \"fig\", \"data\", \"two\", \"one\", \"section\", \"training\", \"approach\", \"note\", \"performance\", \"log\", \"linear\", \"matrix\", \"figure\", \"results\", \"variables\", \"models\", \"algorithm\", \"case\", \"non\", \"work\", \"set\", \"number\", \"ie\", \"also\", \"problem\", \"input\", \"given\", \"state\", \"method\", \"function\", \"time\", \"first\", \"information\", \"distribution\", \"nwrst\", \"unrevealed\", \"pzru\", \"transductive\", \"yga\", \"ul\", \"tiv\", \"fork\", \"ree\", \"hazoo\", \"frustrated\", \"disregarding\", \"rst\", \"mst\", \"rl\", \"pdfs\", \"hormel\", \"xs\", \"forks\", \"omv\", \"irregularity\", \"ygs\", \"bilmes\", \"webspam\", \"fcut\", \"ahs\", \"pxa\", \"trees\", \"eii\", \"ybit\", \"personalized\", \"mp\", \"hinge\", \"wta\", \"weighted\", \"submodular\", \"capability\", \"dbm\", \"signal\", \"attribute\", \"input\", \"circuit\", \"attributes\", \"organizing\", \"unblocking\", \"nodes\", \"clustering\", \"tree\", \"algorithm\", \"node\", \"training\", \"set\", \"based\", \"optimal\", \"neuron\", \"matrix\", \"time\", \"different\", \"algorithms\", \"unit\", \"also\", \"performance\", \"information\", \"graph\", \"neural\", \"function\", \"number\", \"two\", \"learning\", \"model\", \"data\", \"using\", \"method\", \"figure\", \"one\", \"first\", \"value\", \"see\", \"used\", \"problem\", \"distribution\", \"results\", \"given\", \"fcn\", \"shepherds\", \"rpn\", \"fcns\", \"ohem\", \"resnet\", \"conv\", \"anguelov\", \"cnn\", \"subnetwork\", \"ekpt\", \"img\", \"dilemma\", \"agnostic\", \"faster\", \"val\", \"rois\", \"lcls\", \"proposals\", \"trous\", \"zijc\", \"palm\", \"inserts\", \"roi\", \"overfeat\", \"metaaction\", \"convolutional\", \"ck\", \"queuing\", \"gy\", \"person\", \"sapalm\", \"rmm\", \"coco\", \"stride\", \"pooling\", \"activity\", \"object\", \"algorithm\", \"graph\", \"va\", \"table\", \"schedule\", \"proximal\", \"action\", \"regret\", \"maps\", \"labels\", \"set\", \"category\", \"problem\", \"class\", \"using\", \"problems\", \"data\", \"score\", \"large\", \"results\", \"layer\", \"solution\", \"time\", \"see\", \"also\", \"form\", \"based\", \"distribution\", \"let\", \"one\", \"approach\", \"training\", \"number\", \"two\", \"different\", \"figure\", \"model\", \"inference\", \"models\", \"network\", \"learning\", \"function\", \"state\", \"method\", \"used\", \"given\", \"log\", \"glued\", \"ii\", \"christoph\", \"interpolation\", \"bregler\", \"interpolant\", \"party\", \"bradley\", \"lip\", \"replicates\", \"jii\", \"carin\", \"qnn\", \"aekk\", \"uttering\", \"rij\", \"krs\", \"nee\", \"paired\", \"team\", \"ptn\", \"dangeimayr\", \"svm\", \"sapiro\", \"doubles\", \"arose\", \"discard\", \"vndfm\", \"iter\", \"gpc\", \"yn\", \"subspace\", \"bsvm\", \"fitc\", \"sparse\", \"krp\", \"tensor\", \"interpolated\", \"svms\", \"manifold\", \"patch\", \"xj\", \"margin\", \"demonstrated\", \"movement\", \"nonlinear\", \"procedure\", \"support\", \"convergence\", \"used\", \"orientation\", \"xn\", \"pf\", \"target\", \"linear\", \"model\", \"log\", \"rank\", \"dimension\", \"function\", \"given\", \"statistical\", \"one\", \"object\", \"mean\", \"learning\", \"methods\", \"using\", \"xi\", \"loss\", \"training\", \"two\", \"set\", \"inference\", \"problem\", \"time\", \"data\", \"vector\", \"input\", \"matrix\", \"image\", \"algorithm\", \"also\", \"figure\", \"models\", \"first\", \"number\", \"information\", \"distribution\", \"approach\", \"state\", \"prepyriform\", \"efh\", \"eeg\", \"hopfield\", \"rats\", \"electrodes\", \"olfactory\", \"resting\", \"cii\", \"qmax\", \"ramon\", \"bower\", \"nominal\", \"alp\", \"mutations\", \"nucleotides\", \"ancestral\", \"freeman\", \"psc\", \"exptl\", \"coalescent\", \"rat\", \"shunting\", \"hiv\", \"lb\", \"enfuvirtide\", \"epsp\", \"nucleotide\", \"ob\", \"jb\", \"treatment\", \"person\", \"cells\", \"lda\", \"fisher\", \"wia\", \"mutation\", \"group\", \"wave\", \"recombination\", \"gene\", \"data\", \"input\", \"factors\", \"value\", \"time\", \"similar\", \"samples\", \"continuous\", \"two\", \"networks\", \"problem\", \"brain\", \"order\", \"output\", \"number\", \"information\", \"also\", \"used\", \"model\", \"set\", \"one\", \"algorithm\", \"based\", \"learning\", \"figure\", \"results\", \"first\", \"probability\", \"using\", \"vector\", \"variables\", \"xi\", \"function\", \"case\", \"models\", \"distribution\", \"training\", \"state\", \"log\", \"given\", \"classier\", \"fgm\", \"scoring\", \"lmsr\", \"wrapper\", \"separable\", \"aggregation\", \"fs\", \"statistic\", \"fiedler\", \"myopic\", \"nba\", \"mim\", \"maker\", \"cliff\", \"aggregate\", \"agents\", \"establishing\", \"lter\", \"kbp\", \"markets\", \"classication\", \"synthetically\", \"storkey\", \"madelon\", \"msr\", \"informed\", \"albert\", \"cmin\", \"logop\", \"logarithmic\", \"market\", \"xuv\", \"price\", \"transduction\", \"trading\", \"opinion\", \"risk\", \"agent\", \"graph\", \"graphs\", \"functions\", \"network\", \"frfs\", \"optimal\", \"features\", \"data\", \"structure\", \"function\", \"feature\", \"figure\", \"models\", \"approach\", \"class\", \"set\", \"learning\", \"may\", \"given\", \"training\", \"networks\", \"two\", \"information\", \"one\", \"model\", \"linear\", \"used\", \"also\", \"first\", \"results\", \"number\", \"using\", \"algorithm\", \"time\", \"log\", \"distribution\", \"different\", \"problem\", \"regulatory\", \"lange\", \"dawta\", \"bamden\", \"lake\", \"dynamically\", \"wd\", \"nonnal\", \"netr\", \"majani\", \"mif\", \"ihe\", \"dyer\", \"inferencing\", \"descartes\", \"reggia\", \"winnas\", \"fmal\", \"oscillaiions\", \"suppressing\", \"suppress\", \"losers\", \"dawt\", \"taite\", \"saliency\", \"stan\", \"tenn\", \"unil\", \"intcuigeoce\", \"uic\", \"cxj\", \"pressure\", \"winner\", \"winning\", \"residual\", \"activated\", \"visual\", \"rpn\", \"shortcuts\", \"ijk\", \"inhibition\", \"activation\", \"units\", \"unit\", \"image\", \"inhibitory\", \"item\", \"dn\", \"network\", \"model\", \"part\", \"object\", \"labels\", \"pf\", \"question\", \"models\", \"method\", \"parts\", \"log\", \"networks\", \"figure\", \"given\", \"data\", \"also\", \"two\", \"one\", \"function\", \"set\", \"vector\", \"using\", \"training\", \"approach\", \"used\", \"time\", \"information\", \"learning\", \"number\", \"results\", \"input\", \"algorithm\", \"problem\", \"distribution\", \"different\", \"waij\", \"outperfonn\", \"rivest\", \"sensations\", \"bachrach\", \"scholars\", \"tch\", \"vero\", \"michi\", \"fsa\", \"terences\", \"tcw\", \"laminar\", \"room\", \"morphogenesis\", \"siet\", \"wander\", \"exploratory\", \"reactive\", \"knee\", \"jighu\", \"modalities\", \"workslwp\", \"transcribes\", \"rob\", \"nuptiis\", \"entrainer\", \"morris\", \"identied\", \"transcribe\", \"fgm\", \"etienne\", \"unigram\", \"manuscripts\", \"reconstructed\", \"lgn\", \"identication\", \"fs\", \"devices\", \"lter\", \"sparsified\", \"gaps\", \"scoring\", \"units\", \"neuron\", \"synapse\", \"weight\", \"memory\", \"graph\", \"environment\", \"therapy\", \"unit\", \"word\", \"cells\", \"linear\", \"residual\", \"one\", \"results\", \"representation\", \"figure\", \"features\", \"networks\", \"network\", \"feature\", \"local\", \"first\", \"information\", \"control\", \"three\", \"training\", \"vector\", \"two\", \"model\", \"algorithm\", \"new\", \"number\", \"learning\", \"function\", \"methods\", \"used\", \"using\", \"set\", \"based\", \"state\", \"given\", \"data\", \"input\", \"problem\", \"time\", \"inference\", \"log\", \"case\", \"models\"], \"Freq\": [1624.0, 947.0, 1298.0, 1145.0, 775.0, 888.0, 968.0, 880.0, 733.0, 981.0, 657.0, 631.0, 734.0, 430.0, 567.0, 444.0, 827.0, 474.0, 516.0, 616.0, 430.0, 388.0, 548.0, 499.0, 585.0, 574.0, 743.0, 447.0, 388.0, 488.0, 14.309411444779762, 2.8045089982796507, 2.145301341339689, 4.751783348636828, 1.5652007633951541, 2.042278598800135, 4.141726419488257, 7.654704954703154, 1.4989268628956398, 2.4945931657517555, 6.9070031284834466, 6.497346040194204, 16.526773999038717, 1.845237736854506, 1.3692967610124873, 1.330533678285898, 0.8656483704046319, 1.3014035707938394, 0.8504934794435096, 0.8530583026774705, 1.753105506006024, 4.823372765762192, 0.8494781290285655, 1.2645822301449365, 1.26307344532069, 2.5327355716126703, 0.8426687156856472, 0.8362540683647139, 1.2666037761760016, 1.711352965336419, 14.848061312997714, 40.32324687106823, 3.816963722614192, 102.36056687834107, 5.392755016211704, 241.5013055526949, 25.31895549466099, 9.2070118775011, 8.695679928971778, 91.39807294604852, 16.45766950872122, 76.77463599632364, 13.588279618926638, 27.875371849120707, 197.7570653456428, 288.05831423601956, 193.7683990248806, 200.69724198939318, 53.5182422508011, 73.13057775693919, 115.25611045307376, 84.84226343584177, 127.1406032242369, 70.03293413473827, 289.08037299162874, 205.27877530129533, 56.90484461404186, 151.55717593219683, 331.7743717185487, 189.36386884717243, 148.05551087887318, 217.24602893338422, 75.17212674601492, 74.7862506809962, 204.2335406638651, 142.42979637871454, 93.38377137415767, 118.13315116544855, 119.39899463095865, 174.87599541864077, 186.1663428558465, 151.1929429212689, 97.00432230950913, 126.85154387288271, 121.80198321517592, 87.53130489849057, 97.18058566197999, 98.64682711889512, 98.15877832053667, 112.11459481556658, 100.73155568758429, 100.15078560718037, 100.93412567812831, 99.13362124745119, 28.138936020770302, 1.8277530956861234, 7.463940028719764, 3.4796480702917423, 5.491678558002304, 3.3455779057278336, 2.0456246703851746, 3.6129115140115693, 8.94108558905217, 2.588881998287293, 7.9750523401569655, 3.9141964056614977, 10.271146580650921, 1.905569137923576, 16.129804954453135, 5.01433737652554, 1.131435373176272, 1.865314928113954, 1.0920296401060494, 2.1737427496297457, 1.919915761261651, 4.274230544307608, 1.0435019614732537, 13.402389872655261, 5.8529905644502636, 1.0404379031878064, 0.6958191708602771, 1.0125360337817737, 0.7015384308035316, 0.6922265608030258, 8.145527538542419, 19.026685154185202, 3.2781409897476155, 3.63013967580748, 6.605187216918226, 14.533872955949585, 55.368589558216705, 28.053732620190193, 11.717994612961679, 11.583197117190123, 22.638125822509437, 75.87015950016445, 39.119741675997794, 375.674573667788, 35.46062203478873, 27.4641949510559, 145.2185556347556, 31.30360479247183, 56.510876960340326, 20.651340323150944, 183.42576351647273, 88.4825589785821, 73.05720635993443, 77.06862056627145, 59.62238398872692, 52.00130622054278, 211.60502947255205, 184.25688751341588, 60.23644450586648, 182.61363440608096, 145.17358934467882, 175.05527200792244, 152.87507782216042, 216.75451976614454, 82.8117059501096, 102.85966549429003, 93.45618046402767, 98.0863599394973, 83.8198615080287, 148.35440230429415, 104.50586945334227, 106.86605250457218, 114.36403969491137, 75.84532153637149, 110.9742536701347, 107.1535065001761, 105.4155134648567, 87.61171573584102, 97.95081618114325, 110.49680195385643, 108.4373419599263, 106.73533563611339, 92.16666803458287, 88.3593658048993, 85.48470859647392, 8.218906534727978, 36.214563566833675, 6.91015542813548, 11.780621192676945, 16.31213130314425, 22.360256533740333, 2.6532965790665406, 2.4328823422511134, 3.3266635098435997, 1.4729989820859273, 14.235057156695154, 1.7409186445197178, 3.223360648720886, 21.45129181161052, 4.469658218321912, 5.884814587533217, 0.8408907092718915, 0.8382635397494116, 8.108530260460133, 1.0672511507611215, 1.7095624147138497, 1.684467416427129, 0.8077464734858678, 1.0640964718021197, 1.0927241553181055, 0.8055988815821812, 0.7884056931605308, 3.0087163909381656, 2.88887569063106, 4.248329709648964, 8.666880546241874, 13.186903105827422, 16.66972056486621, 13.612832372808533, 3.424858746091452, 5.608285229092687, 6.455539714137893, 6.895124944085081, 10.782098301384613, 7.469120071396961, 44.56780787418844, 34.20985284447643, 13.346284247881776, 40.33743359209773, 271.748694201282, 69.4451804013708, 192.80686305041243, 46.81716155646841, 39.19596569686615, 23.731006376369074, 161.11704232187478, 36.10094479265797, 199.84092410832824, 122.82701314591228, 151.8602359022543, 67.16470619189067, 111.50980117754473, 84.82405495555113, 53.712008379261995, 74.51637346257662, 97.46890173976814, 75.8985532600734, 78.07345345751119, 119.18847267001523, 100.67113217525527, 68.40069796665735, 111.66787369665659, 126.16848687397, 83.94771552134044, 61.287426304495405, 66.29744098369832, 128.69830304788007, 105.27387350709415, 60.273242486299736, 86.94347203086579, 84.15997603059111, 76.45450526859891, 89.02853490008356, 92.48026088937056, 74.07964001363337, 103.63382961066804, 97.32809764711388, 76.75158031856634, 76.35957043372265, 76.55174809508827, 3.2717544714860485, 1.2184221412009604, 1.2457219779811766, 2.0976932106928268, 1.697901172848128, 2.686876596487669, 0.6904352631302172, 3.2932092286123367, 2.9042011138654638, 13.263780841618804, 0.921150846821689, 0.6827076652441817, 4.9522736173600785, 1.1031910666732159, 3.1358820177268565, 4.379449039474619, 1.0190082744785296, 2.9034789013356703, 1.71858343682546, 1.7175180633022553, 0.6356290073289318, 1.9944169882273517, 1.6591547922852263, 1.4709146504303938, 0.834574984956375, 0.7645363679452175, 1.271335805716804, 14.494420157506251, 0.6249503857298242, 1.4085258825700848, 1.8556054037298226, 9.68758213065712, 8.451916107213497, 12.11203164790285, 21.494507595579528, 13.80556687163451, 5.638487255373278, 15.551304551528132, 23.779632733730274, 21.70878903508038, 75.88675277867812, 14.49848730092298, 21.426683949884254, 3.748435056551526, 4.390994959430532, 23.569893818045507, 20.512026328677653, 24.28501491829202, 109.58163422355454, 27.835136779654743, 82.29461510743972, 111.2033558567006, 56.618786120239015, 46.82630202709682, 20.56602060326796, 55.64732528156955, 89.96970343615787, 59.13418498209465, 41.24715639862658, 31.240279808454336, 64.97465710292072, 48.869175480711, 56.6499905772225, 49.19388602631453, 53.93697974012612, 81.57567742039723, 69.58319243087192, 68.63491823732213, 90.15078138314972, 112.90770429156751, 96.76308166211722, 78.17447828824416, 48.62889032705724, 65.80656467563625, 71.12031531397385, 48.12875764229011, 38.141963503860545, 37.8932840262318, 50.067150339414084, 46.20239708040231, 46.931162004418425, 47.34634380551228, 46.66433139256691, 17.05017636623671, 0.9485325362916002, 4.592543844773601, 0.9457690764062296, 1.1611482092865582, 8.76502579588294, 4.2462421768429826, 0.8797362940609093, 16.600192481847376, 3.583751725319147, 1.062087667104492, 1.2251235549366313, 1.026763270548666, 0.7929703999005016, 18.291844367390024, 1.803068957925237, 4.603372809640867, 0.5870633214458336, 2.541699975252755, 1.7698939468203378, 0.5777873660647813, 2.164891142722607, 0.5727216059500168, 10.085183387029419, 0.5482230510020921, 0.5994597877227622, 19.006451440262587, 4.322519666337188, 2.0061792617279837, 2.9165385724891015, 14.066839021632372, 7.1508847124922035, 4.642540647526717, 2.733952996636668, 1.9881209925192425, 9.707745071985297, 25.948908978784402, 38.89328689693474, 113.61576302171864, 59.646139710089194, 4.898743528764292, 34.90010968346794, 9.940859154264993, 11.604428690491494, 36.74341305153014, 15.23246007147601, 13.57884831897148, 35.44426241957979, 103.84750034370356, 19.715501610074515, 63.32163264009233, 40.306043548003885, 100.51859292344206, 31.629838917516775, 122.85663961933145, 24.08686672083283, 45.35301746591066, 64.21771457450053, 24.471596305684212, 27.120013639202266, 76.82179414565266, 38.07492164993573, 57.70535466991868, 33.190598417872856, 47.66104317108417, 59.0216717347772, 35.44336425347359, 78.82774819955877, 49.643479664986884, 60.67692523897735, 65.35335899740195, 64.48404081384983, 49.526275651080624, 64.76865552744474, 101.0851891514837, 45.145714332163685, 60.353366895875475, 44.94811245843625, 67.92738045944058, 60.03922197321014, 52.76642187252609, 44.3084875969344, 47.807686777895846, 49.065583068131275, 45.367537373764065, 0.45195501872373733, 23.444844990675108, 0.9861762991088268, 6.38108937978835, 1.4149145613514518, 0.5473062159434795, 10.293476284657174, 2.8951981864728813, 3.9490439627892235, 0.5614940007161103, 0.9785621789973605, 0.41799854683229426, 0.7145671490876653, 0.9618773011440055, 0.4030716023699216, 1.966803412981167, 0.37600841634135995, 0.4019725793466916, 1.6914866838705527, 0.8238846483214401, 0.8274161554982385, 0.2611447525482121, 35.54447353370348, 0.545624940625029, 0.3960528912806603, 0.3861755456232344, 0.39803861728910067, 0.6925874941369619, 1.0602234997220599, 1.5102608247985747, 9.311104830440865, 4.037591957450233, 1.5204241278894384, 1.9503506573824663, 28.263407642205216, 2.61584892054153, 13.023846189954835, 2.7347513638881167, 5.01723298806032, 11.778079209546677, 4.454861044322628, 14.584287711238355, 12.091125538331555, 4.530388980857036, 6.260349827747321, 14.42340295759825, 14.330139484584159, 14.420249639196099, 12.058149947425115, 60.113882513750305, 17.119637331977778, 11.580772339539466, 13.445469347945188, 16.071117076539185, 43.07546353698003, 138.5403480345949, 56.38978770436264, 21.317757737421005, 13.304877100398505, 79.29926453067871, 56.97143317911404, 20.155478513372856, 81.61379164571095, 26.37310495804141, 30.13538757025842, 92.10403993914773, 33.91272362618271, 76.59096785419614, 36.40368620914141, 27.91378453838501, 51.65373974840495, 54.89095188592228, 65.54879003067785, 36.82877415090743, 43.29874769315648, 57.82588365639286, 78.99780041771429, 33.645544332711985, 38.259268022288595, 36.72983126791017, 29.948542714745546, 54.75047026228412, 41.62674571919296, 49.72747033173049, 43.77990369822835, 36.19219477641716, 40.11639212604391, 35.44193268863292, 36.775532904370806, 34.78393738466753, 34.98830163043106, 1.108689169187607, 4.755853184010488, 4.508939019847134, 0.8451310963140879, 0.6640808042856299, 1.3213502644594914, 3.747242734396473, 9.75354820025814, 0.7974005551534216, 2.8293866316823637, 0.4699991060730652, 0.47019695641930404, 2.4597665200743886, 0.7612421021005172, 1.3556081909268969, 0.7420792846049293, 0.9154856056566689, 3.352558297937306, 0.899077864618721, 0.4528927050012821, 2.3662180443629253, 0.585131836969426, 2.1699231450716296, 4.150540675640174, 0.7438360013764926, 0.8557468811169152, 1.8715650786053772, 2.64447968002152, 0.5899835965987726, 6.6602750340872285, 6.6491910684457, 10.463682851104894, 18.858474000750206, 8.147995612599587, 13.110384744286819, 4.69846833850523, 3.9797981597508305, 14.369480699025724, 3.4466055699989706, 5.5650680669138115, 8.470357134898496, 119.72540838948291, 48.860906239826896, 12.717117441691908, 34.59664983943285, 74.50705006119797, 24.560981809912377, 27.681807658123876, 16.30828140364452, 64.48148374433212, 36.82593972099218, 47.84128980144824, 10.710683940311336, 27.187852521499014, 32.57578575825178, 58.21222056363351, 43.23943711436609, 46.512367470918996, 45.18861973707252, 99.94396019061384, 66.62232688322369, 65.3892920790507, 61.80488372691753, 37.23481467073578, 70.94325419340034, 53.56671718785431, 45.972000497169525, 38.99469704783884, 32.19220701869492, 57.88785311813435, 31.169655581802527, 30.75323758361386, 31.341955334272637, 48.504055417048825, 35.40794478748494, 40.74532650976688, 36.96818590679963, 37.9275479506035, 37.37561278759011, 35.2761377880288, 32.967472047710025, 1.6905567998431625, 1.7203360244460488, 14.282076034879598, 3.749796393233205, 2.8968980084844795, 7.288826699336006, 4.568047362886352, 2.8577878510987715, 3.646476891843974, 6.159024575892237, 1.9032745857771398, 0.7910539848321586, 1.3822120357629102, 1.74097221313174, 0.4851203721292866, 1.606634646036457, 11.533513526516598, 0.6432514982682727, 1.9162116045280297, 0.8812901763394044, 7.158331687053288, 2.7574647295217996, 0.4463254532774913, 1.1060243502931995, 0.7371291821597624, 4.16842984292348, 0.4671599881953159, 0.6530973579376095, 1.2408122307610014, 1.3927945917486004, 5.0214166798021465, 11.029641111108267, 7.997681046117368, 7.19027006393792, 3.166042501376121, 2.856289533371645, 5.077614970638127, 16.818137669540572, 8.3106195690097, 52.13988214932029, 14.909741552054637, 37.41186012974812, 46.0023637118195, 3.9164754846377363, 37.13397114680801, 31.007592564839936, 106.97005065803997, 32.477028882046696, 76.11088792521731, 33.01588948332867, 64.11360096391108, 59.89916885544231, 43.14227789483621, 30.752531389400502, 70.87035793589506, 79.51338060922404, 26.842525947990175, 48.395213678845735, 50.21153982089061, 34.03928573958905, 53.36089784752506, 41.469037644565006, 61.83880033432924, 86.71840275007061, 35.31090832693594, 42.284163390840895, 42.27012148739172, 38.64769079279207, 44.46752190171906, 47.99002316341257, 55.01869732135565, 50.70238881055406, 47.521003954905034, 40.24286002321639, 40.59657037645575, 37.15256143261532, 35.98592589938485, 6.581403950360043, 1.955274849771674, 1.7086368337107474, 0.7198480202248173, 4.159793560985771, 3.3439380017344544, 0.9783607310963593, 0.6588443650276404, 0.917660281600517, 0.5019517500879357, 0.37271592175649276, 0.36412971606787187, 0.49010780675994936, 0.36473965311823103, 0.23811915024318372, 0.35394631362798284, 0.46966261497986506, 0.3507259086802675, 0.23329367459769274, 0.34886276057158844, 0.962359931620347, 0.35120731628539686, 0.45772136377563644, 0.34774646216522775, 5.031285094347925, 0.2267508755493648, 0.33945484392803277, 0.3362618606845725, 0.22243626625414573, 0.21836159687115853, 0.9515324325213697, 0.569578647861566, 8.054702233812934, 1.1601833183732018, 5.599606652938712, 2.208443775613441, 16.304327343198075, 1.938872126459009, 1.0179166178320234, 5.854466166021667, 4.216524522921376, 12.991955953838431, 17.94828327117104, 16.862826290629243, 23.59512357464593, 4.3011804949067205, 4.5190081716091015, 4.6219608448461456, 28.914079269738444, 84.75246379166808, 12.31998568080888, 17.526704810471756, 17.726714647226455, 8.811071811676001, 6.173496227752354, 37.39131079225877, 25.892629911770776, 7.4103786797087965, 30.48050479475061, 22.305442276448606, 37.59288632997871, 31.515805441168993, 54.70175582072972, 28.487609403781864, 33.8464986216515, 40.638258696129405, 38.06889703031976, 38.89095146168725, 20.98647522488868, 38.60230088758031, 29.58251449335088, 24.128028513584493, 25.9402678365754, 32.25626541630963, 24.23246039189678, 36.297403539333736, 27.887638811008046, 25.90051231983276, 22.272077427056473, 27.81288129078152, 22.138294610073178, 21.82732308209352, 20.55169673923827, 0.14191680936685658, 0.10627680207783592, 0.4835373694187818, 0.36770573864992506, 0.3594406205546086, 0.06275380375187489, 0.1280647269129117, 0.09374168869669655, 0.3491993531736131, 0.31708088024738973, 0.15365180115781638, 0.1231131292264255, 0.34726411501577475, 1.417969951488331, 0.3370893917648903, 0.09232013403138131, 0.059052978846318514, 0.05900316439749089, 0.21244348659737985, 0.11442696632554895, 0.08841474679742457, 0.10995992736856781, 0.057867619721234706, 0.0864121484274613, 0.08473727387801795, 0.057276515464547366, 0.1432135503084341, 0.08491473827004116, 0.10685842774260856, 0.08787715611541441, 0.2622426840562209, 0.11220429337972576, 0.3207260376568882, 0.23587742878675433, 0.25797567215218625, 0.7130456393042449, 0.18332210831292056, 0.4548424274325313, 0.9798890994297621, 0.33300772412675006, 0.45681156643992765, 1.1373235745056036, 1.8618492134174716, 5.5924271313586145, 3.375368722763409, 0.9939977393391031, 5.767669104625484, 2.08999063957853, 9.144794356964097, 1.2756358298877482, 0.662924825261594, 5.004372452751399, 2.02446455171211, 3.036312279020535, 8.466485395740731, 1.2983984710395846, 16.56197873069704, 11.624192148644966, 4.184670352624574, 13.223645254587616, 5.70660797537583, 7.28217861510295, 8.071236701826631, 6.178669626272963, 4.492844326475543, 8.774083135419577, 8.988074930273092, 4.204994380178371, 4.6444709732916705, 10.86472321574039, 7.094825877723737, 11.700460551492084, 21.80716184138717, 13.315932624932902, 5.77013158527783, 11.489879925942791, 15.751012614609113, 12.905759209095635, 6.324332292218466, 8.947584419129347, 13.257063411702024, 12.78996182701457, 7.261106253720866, 9.58013113994037, 8.653196398332415, 12.825796229052804, 7.330742398176794, 7.759375000517508, 8.95276110828932, 6.765091070581031, 7.113906677910634, 6.874430434470051, 7.012439803019523], \"Total\": [1624.0, 947.0, 1298.0, 1145.0, 775.0, 888.0, 968.0, 880.0, 733.0, 981.0, 657.0, 631.0, 734.0, 430.0, 567.0, 444.0, 827.0, 474.0, 516.0, 616.0, 430.0, 388.0, 548.0, 499.0, 585.0, 574.0, 743.0, 447.0, 388.0, 488.0, 25.441650189398825, 5.259701853999213, 4.220059987320815, 9.482592269978728, 3.1267077615112537, 4.134999166880326, 8.534337388604746, 15.777184275507247, 3.103340079951175, 5.1809645830984135, 14.548229262500746, 13.822487712277198, 36.50145289876784, 4.104287087145673, 3.0744264132968646, 3.0600405937320767, 2.005572401046457, 3.0234025921147043, 2.000810172662178, 2.0121462625716617, 4.1375912248568065, 11.40999617046504, 2.0114195576839493, 3.000258701354916, 2.9988175863419495, 6.03089649437665, 2.013884003333444, 2.0010227097375983, 3.03161691080092, 4.104965866959641, 35.81053736482063, 99.45982721438888, 9.265733874065674, 267.9634238469813, 13.322513014017208, 677.812647103465, 69.0147202334909, 23.901262626717358, 22.59980718003159, 271.26370431791145, 44.70828898428213, 234.6642593920351, 36.67435024217528, 80.4333312652053, 734.1604468249081, 1145.1574594972892, 743.3353455766322, 775.4205905666649, 175.7596168286582, 257.7502977848113, 435.079782178152, 306.3624169763257, 492.62511059568715, 247.0738965574308, 1298.5163496625698, 888.8421319344037, 196.69933092864366, 631.3128006857479, 1624.9528696390046, 827.4215060953521, 616.7325944496828, 981.0266594659946, 273.990451848815, 272.78402267430357, 947.1392335734919, 612.0435849008182, 365.9385560361022, 499.8104772778709, 509.89415381870623, 880.9828385576483, 968.8947777560451, 733.8565497292022, 394.4819869522314, 585.1788940399881, 567.3512460517651, 345.2559585470968, 417.540676125722, 430.645880549254, 444.456797983114, 657.8102552386534, 488.34756513473747, 492.91769256941484, 574.0771123406969, 516.6968327889523, 57.20421989447512, 3.885266322332017, 16.24875765506763, 7.824586677282291, 12.520089821635553, 7.738813995775756, 4.772839276558272, 8.571062923851017, 21.261047041302156, 6.1796401249976665, 19.144855365811722, 9.631990641399833, 25.457687337585387, 4.739870387934912, 40.56578970981524, 12.661228174381161, 2.8647393666069974, 4.802355621599961, 2.819321986533681, 5.618759292820627, 5.015571430073771, 11.24096845959011, 2.7503171872520253, 35.35775383493285, 15.686320116047057, 2.7931302719645426, 1.8791275992053573, 2.7519271833210386, 1.9097796892425243, 1.8854196706426039, 22.502150485711166, 53.11513681493319, 8.998562304256765, 10.007694671672699, 18.523338312263178, 41.84690508648833, 173.1971324088586, 85.04313462336158, 34.05871468967725, 34.20635268549825, 70.37585839872415, 260.3472724868216, 130.70790846935432, 1624.9528696390046, 121.50414816598045, 91.66750313089118, 612.0435849008182, 108.312853809008, 216.31168789903347, 67.50413730622884, 888.8421319344037, 379.4093486106657, 305.0787759650675, 332.0425344602568, 244.29257349096812, 208.3030417266227, 1145.1574594972892, 968.8947777560451, 249.04385937190523, 981.0266594659946, 743.3353455766322, 947.1392335734919, 827.4215060953521, 1298.5163496625698, 378.3256173988277, 509.89415381870623, 449.67118526381097, 492.91769256941484, 403.28745151602976, 880.9828385576483, 548.2232459439705, 585.1788940399881, 677.812647103465, 367.226622060643, 657.8102552386534, 631.3128006857479, 616.7325944496828, 463.2858899266416, 574.0771123406969, 734.1604468249081, 733.8565497292022, 775.4205905666649, 567.3512460517651, 516.6968327889523, 499.8104772778709, 20.31897095747026, 91.53898311209014, 18.624131983346285, 33.4590196276162, 46.45880992940928, 65.34236923919995, 7.892285462368712, 7.250970311689576, 10.1329068821694, 4.522416003675861, 44.781547485123426, 5.494449876967893, 10.289672729451144, 68.4853678707699, 14.32465255784124, 18.902058469256147, 2.7118038385565915, 2.7204722994587165, 26.517442555474812, 3.52401781343674, 5.693155308239006, 5.61051494047638, 2.6942613337637593, 3.551008990439369, 3.6511528249421055, 2.705096532685024, 2.6490773720930925, 10.13859380248238, 9.800940368602234, 14.470018034568438, 29.675854814048915, 46.320656062320765, 59.13697182730627, 49.28919021775603, 11.857606157854612, 19.826497604542737, 22.985328361109538, 24.737799146949126, 40.07830274053594, 27.305600468049732, 194.15030600002365, 145.56782194203205, 52.54644711134519, 181.40472853209238, 1624.9528696390046, 352.5795044470304, 1145.1574594972892, 226.68703347824587, 184.5287036417243, 103.53092378365241, 981.0266594659946, 169.67465975394984, 1298.5163496625698, 733.8565497292022, 947.1392335734919, 359.1087797748779, 657.8102552386534, 488.34756513473747, 280.0527989225466, 417.540676125722, 585.1788940399881, 430.645880549254, 449.67118526381097, 775.4205905666649, 631.3128006857479, 383.0751929913918, 743.3353455766322, 880.9828385576483, 509.89415381870623, 334.7505646413609, 374.27022235737644, 968.8947777560451, 734.1604468249081, 328.3619423913063, 574.0771123406969, 548.2232459439705, 474.8067670439249, 616.7325944496828, 677.812647103465, 463.2858899266416, 888.8421319344037, 827.4215060953521, 499.8104772778709, 516.6968327889523, 612.0435849008182, 12.112439771966956, 4.6206041627627235, 4.7940836052520135, 8.18184409728642, 6.6366443371522585, 10.539362251451644, 2.715114232706583, 12.992639131370872, 11.497372720362927, 52.81487001626243, 3.69592363584091, 2.7595135187956847, 20.129427527916974, 4.493692838034426, 12.932398344710228, 18.314318154064182, 4.2836003583866935, 12.306040708220127, 7.332410586955474, 7.392821372043832, 2.741141404698728, 8.627192604788364, 7.24037290586662, 6.4725713454204845, 3.6728867666000315, 3.369213881650849, 5.631037993215587, 65.48930606210816, 2.82417817919788, 6.37077179648104, 8.49361057203862, 45.98632603549192, 40.349633389790064, 58.529279435736996, 108.67566029173153, 69.10458590617503, 26.956768692737892, 79.8120390533748, 126.8523551161222, 116.46942396286109, 474.8067670439249, 77.4443798378666, 122.64649120200733, 17.669840447354712, 21.150098085599048, 143.06613443412334, 122.51676965784615, 151.658380795358, 880.9828385576483, 180.2831664688601, 657.8102552386534, 968.8947777560451, 447.43043884998036, 355.8296896736186, 131.26630030772256, 449.67118526381097, 827.4215060953521, 492.91769256941484, 317.5183930731671, 224.42661532019528, 574.0771123406969, 417.540676125722, 516.6968327889523, 430.1687551563754, 492.62511059568715, 888.8421319344037, 734.1604468249081, 733.8565497292022, 1145.1574594972892, 1624.9528696390046, 1298.5163496625698, 981.0266594659946, 463.2858899266416, 775.4205905666649, 947.1392335734919, 499.8104772778709, 326.7317609781397, 326.2387541691223, 567.3512460517651, 548.2232459439705, 612.0435849008182, 631.3128006857479, 616.7325944496828, 57.72776722768391, 3.444425212848807, 16.696075705944107, 3.483236957773332, 4.288302394359356, 33.50022820228736, 16.79385923522655, 3.4808137506729664, 66.17745586739657, 14.780325695620137, 4.459400749256256, 5.211805907557494, 4.373535217610294, 3.399208395526024, 78.88116549950148, 7.847473246867336, 20.343587370477373, 2.628140855544332, 11.408765680837572, 8.031913870577785, 2.629073628856773, 9.854757731362806, 2.6180990453147466, 46.33230372421257, 2.548631397669227, 2.7978536377713548, 88.96181330107443, 20.347651459840037, 9.444739457200157, 14.031402934585213, 69.67415729094147, 35.00416698545485, 23.31517104840409, 13.356463371525809, 9.596812407719755, 51.99134939875111, 160.15512091560987, 252.48105955386302, 880.9828385576483, 430.1687551563754, 25.73857825068012, 249.52367695734057, 58.353666374879396, 70.13511434264561, 271.26370431791145, 96.28863061748008, 84.51275984461947, 264.01161186539895, 968.8947777560451, 133.19798001181428, 548.2232459439705, 319.12996342108175, 981.0266594659946, 240.9414231489836, 1298.5163496625698, 173.0582128462578, 394.4819869522314, 631.3128006857479, 179.61646907543118, 206.00849642825696, 827.4215060953521, 326.2387541691223, 574.0771123406969, 272.1194398271208, 447.43043884998036, 612.0435849008182, 300.3073207105568, 947.1392335734919, 488.34756513473747, 657.8102552386534, 734.1604468249081, 733.8565497292022, 492.91769256941484, 775.4205905666649, 1624.9528696390046, 435.079782178152, 743.3353455766322, 444.456797983114, 1145.1574594972892, 888.8421319344037, 677.812647103465, 463.2858899266416, 567.3512460517651, 616.7325944496828, 585.1788940399881, 2.6532017551684035, 140.64194808840702, 6.082168135015414, 39.87396896459949, 8.851751563973968, 3.4255496003803447, 65.69413793484532, 18.53464769505574, 25.340451386523213, 3.61632333489003, 6.317784896738851, 2.719934053769353, 4.655915217582216, 6.269883811621032, 2.631432900484237, 12.847001424365667, 2.4573345683501584, 2.630712548053669, 11.104062651316513, 5.423350707682679, 5.468834898276633, 1.729009046186432, 235.57871988923327, 3.6658576018499653, 2.6800662931164494, 2.6197427616876046, 2.7084740552341784, 4.717175429865111, 7.224520855729931, 10.401759451832552, 64.80424170292423, 28.233126999058314, 10.49837920133365, 13.546643546043981, 211.1032986313846, 18.368495823987203, 96.23682824075338, 19.331020279666372, 36.654795983387714, 90.48960726365182, 32.54609334282272, 114.89427755657503, 94.71443661237254, 33.46828992441831, 47.47511190738501, 116.96649237116998, 116.40797454490068, 117.93454101575604, 97.30885768494414, 567.3512460517651, 143.80997319607752, 94.20710160367777, 112.80194789476373, 139.09250935200714, 430.645880549254, 1624.9528696390046, 585.1788940399881, 194.15030600002365, 113.64423650131157, 888.8421319344037, 616.7325944496828, 184.5287036417243, 947.1392335734919, 252.48105955386302, 298.8815091367608, 1145.1574594972892, 352.5795044470304, 981.0266594659946, 397.35204679837585, 283.7878016887276, 657.8102552386534, 733.8565497292022, 968.8947777560451, 435.079782178152, 548.2232459439705, 827.4215060953521, 1298.5163496625698, 388.95111469494475, 474.8067670439249, 449.67118526381097, 343.14665344273527, 880.9828385576483, 574.0771123406969, 775.4205905666649, 743.3353455766322, 499.8104772778709, 734.1604468249081, 516.6968327889523, 612.0435849008182, 488.34756513473747, 677.812647103465, 5.19076420902818, 23.86477380979167, 23.25072820853621, 4.384937602407956, 3.5058540432569383, 7.068531606345073, 20.59485711358958, 53.80246695814949, 4.4341199114133705, 15.784522270426537, 2.6237658143403655, 2.6249017970425035, 13.923842462600367, 4.406736269617895, 7.877551417115949, 4.312745818112434, 5.331102321138397, 19.538925831112934, 5.253444313253939, 2.6548273245476115, 13.90988603068793, 3.4535755100015426, 12.8269282894701, 24.68509825857425, 4.443309676114485, 5.114678818342472, 11.196520923976598, 15.855610594070367, 3.5391439118757684, 40.178113585540466, 43.209959607903585, 69.67415729094147, 130.813613533174, 54.80464463498079, 91.66750313089118, 31.622621204208496, 26.79731405548473, 110.36642020456642, 22.997647394659396, 39.18600452525205, 63.37234959112859, 1298.5163496625698, 474.8067670439249, 101.9846638327224, 326.7317609781397, 827.4215060953521, 226.10174250397642, 265.2099583455404, 141.31806816245847, 733.8565497292022, 388.26944852973054, 548.2232459439705, 87.48628727636435, 281.3354161774916, 353.66565496968934, 734.1604468249081, 516.6968327889523, 574.0771123406969, 567.3512460517651, 1624.9528696390046, 968.8947777560451, 947.1392335734919, 880.9828385576483, 447.43043884998036, 1145.1574594972892, 775.4205905666649, 631.3128006857479, 499.8104772778709, 386.7072536434737, 981.0266594659946, 388.95111469494475, 383.0751929913918, 397.35204679837585, 888.8421319344037, 509.89415381870623, 743.3353455766322, 612.0435849008182, 657.8102552386534, 677.812647103465, 585.1788940399881, 616.7325944496828, 7.563711647665012, 8.02917016273647, 67.3924104301213, 17.731516465279846, 14.273698308196423, 36.10335394176805, 22.710609453547526, 14.572487800474061, 18.609262280405453, 32.124534261879525, 10.02329496300053, 4.2034731641011644, 7.431729993912081, 9.426275454609534, 2.629890454160531, 8.836136646697973, 63.48421845028076, 3.548996419690482, 10.63188403237564, 4.896146280749743, 40.1961719381872, 15.528122152719527, 2.5290466534383955, 6.270911187321082, 4.182494822296094, 23.78866137318704, 2.6661567401649044, 3.7295500964958364, 7.156976809053495, 8.071264808178062, 29.627230469141523, 67.50413730622884, 50.28065986359463, 45.15996718609321, 19.201825497558627, 17.32609120883834, 32.47408457006685, 118.61975714963809, 57.1515270603736, 430.1687551563754, 110.69165184155679, 345.2559585470968, 444.456797983114, 25.018131514665495, 355.8296896736186, 287.22694984695414, 1298.5163496625698, 314.6771987337355, 888.8421319344037, 322.30282894459947, 775.4205905666649, 743.3353455766322, 488.34756513473747, 319.12996342108175, 968.8947777560451, 1145.1574594972892, 275.22939282822506, 616.7325944496828, 657.8102552386534, 388.26944852973054, 733.8565497292022, 516.6968327889523, 947.1392335734919, 1624.9528696390046, 430.645880549254, 567.3512460517651, 574.0771123406969, 499.8104772778709, 631.3128006857479, 734.1604468249081, 981.0266594659946, 880.9828385576483, 827.4215060953521, 585.1788940399881, 612.0435849008182, 492.91769256941484, 548.2232459439705, 32.86405846686012, 10.043556906015914, 9.411281047741982, 3.9840749840942484, 23.750964423550926, 19.337168840043113, 5.690268767525806, 4.053689601771222, 5.704741712446955, 3.21490884803058, 2.4495184289041303, 2.4356488693349947, 3.286769311412259, 2.4520963729250096, 1.623205463174736, 2.4489691360367156, 3.256019320549288, 2.4409905715951643, 1.6378831898802262, 2.451938282062228, 6.7771208033429655, 2.479692948092273, 3.2418227187255253, 2.4795779629517765, 36.156017163599536, 1.644792418811267, 2.4757268717287477, 2.478097688377424, 1.6488262277969519, 1.6482730518903765, 7.190831637940673, 4.3151953812777, 67.15336326343208, 9.09998209630968, 49.95570735215139, 18.964852052987933, 173.38814792662947, 16.696075705944107, 8.229624606430624, 59.13697182730627, 41.49843070520253, 151.35148184288175, 219.99513582778977, 224.42661532019528, 343.14665344273527, 45.243422824369375, 48.07569732467136, 49.462574038955296, 444.456797983114, 1624.9528696390046, 161.02185199928584, 252.48105955386302, 264.01161186539895, 112.80194789476373, 72.19466075934959, 743.3353455766322, 463.2858899266416, 91.50618133106602, 585.1788940399881, 388.26944852973054, 775.4205905666649, 616.7325944496828, 1298.5163496625698, 574.0771123406969, 733.8565497292022, 947.1392335734919, 888.8421319344037, 968.8947777560451, 388.95111469494475, 981.0266594659946, 657.8102552386534, 488.34756513473747, 567.3512460517651, 827.4215060953521, 516.6968327889523, 1145.1574594972892, 734.1604468249081, 631.3128006857479, 474.8067670439249, 880.9828385576483, 548.2232459439705, 612.0435849008182, 492.91769256941484, 3.458445911251343, 2.5901483609139575, 12.496792526328475, 9.68952828841808, 9.730891477812413, 1.7120669790315268, 3.527463482178414, 2.6351923775645267, 9.885620753880575, 9.059452053469153, 4.391500434514137, 3.535633394551246, 9.986982467930028, 41.2753861758074, 9.879093958947918, 2.7075626326110815, 1.7433662716436897, 1.7422089908036071, 6.31034154678744, 3.428871558484874, 2.6566453419844427, 3.3073615921289647, 1.7408512210914018, 2.6227373016966737, 2.5747890034762584, 1.7421536276459129, 4.366012207298962, 2.5902851146035286, 3.266307409090489, 2.68869411594585, 8.02917016273647, 3.4338154857110643, 9.968223755877789, 7.295146317058383, 8.022112212718381, 22.896506459240722, 5.676500257947732, 14.572487800474061, 32.8363135783198, 10.63188403237564, 14.88491670524589, 39.64948405689675, 67.3924104301213, 219.99513582778977, 131.26630030772256, 35.53067560270713, 247.0738965574308, 81.4706113210944, 430.1687551563754, 48.078977307286024, 23.131143547273375, 224.42661532019528, 81.81237841433656, 130.813613533174, 430.645880549254, 49.95570735215139, 947.1392335734919, 631.3128006857479, 199.1845747887517, 775.4205905666649, 287.22694984695414, 388.26944852973054, 444.456797983114, 322.30282894459947, 219.7348518075204, 499.8104772778709, 516.6968327889523, 205.29607139360786, 231.8626384427692, 657.8102552386534, 388.95111469494475, 733.8565497292022, 1624.9528696390046, 880.9828385576483, 306.3624169763257, 734.1604468249081, 1145.1574594972892, 888.8421319344037, 352.5795044470304, 567.3512460517651, 981.0266594659946, 968.8947777560451, 447.43043884998036, 677.812647103465, 616.7325944496828, 1298.5163496625698, 474.8067670439249, 548.2232459439705, 827.4215060953521, 435.079782178152, 585.1788940399881, 509.89415381870623, 743.3353455766322], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -8.1755, -9.8052, -10.0731, -9.2779, -10.3884, -10.1223, -9.4153, -8.8011, -10.4316, -9.9223, -8.9039, -8.965, -8.0314, -10.2238, -10.5221, -10.5508, -10.9807, -10.5729, -10.9983, -10.9953, -10.275, -9.2629, -10.9995, -10.6016, -10.6028, -9.9071, -11.0076, -11.0152, -10.6, -10.2991, -8.1385, -7.1395, -9.4969, -6.2079, -9.1513, -5.3495, -7.6048, -8.6164, -8.6736, -6.3212, -8.0356, -6.4955, -8.2272, -7.5086, -5.5493, -5.1732, -5.5697, -5.5346, -6.8564, -6.5441, -6.0892, -6.3956, -5.9911, -6.5874, -5.1697, -5.512, -6.795, -5.8154, -5.0319, -5.5927, -5.8388, -5.4554, -6.5166, -6.5218, -5.5171, -5.8775, -6.2997, -6.0646, -6.0539, -5.6723, -5.6097, -5.8178, -6.2616, -5.9934, -6.034, -6.3644, -6.2598, -6.2448, -6.2498, -6.1169, -6.2239, -6.2297, -6.2219, -6.2399, -7.3075, -10.0415, -8.6345, -9.3977, -8.9414, -9.437, -9.9289, -9.3601, -8.454, -9.6934, -8.5683, -9.28, -8.3153, -9.9998, -7.864, -9.0323, -10.5211, -10.0212, -10.5566, -9.8682, -9.9923, -9.192, -10.602, -8.0492, -8.8777, -10.605, -11.0073, -10.6322, -10.9991, -11.0125, -8.5472, -7.6988, -9.4573, -9.3554, -8.7568, -7.9681, -6.6306, -7.3105, -8.1835, -8.1951, -7.525, -6.3156, -6.978, -4.7159, -7.0762, -7.3317, -5.6664, -7.2009, -6.6102, -7.6168, -5.4328, -6.1618, -6.3534, -6.2999, -6.5566, -6.6934, -5.2899, -5.4283, -6.5463, -5.4372, -5.6667, -5.4795, -5.615, -5.2659, -6.2281, -6.0113, -6.1071, -6.0588, -6.216, -5.645, -5.9954, -5.973, -5.9052, -6.3159, -5.9353, -5.9704, -5.9867, -6.1717, -6.0602, -5.9396, -5.9584, -5.9743, -6.121, -6.1632, -6.1963, -8.3753, -6.8922, -8.5487, -8.0153, -7.6898, -7.3744, -9.5059, -9.5926, -9.2797, -10.0944, -7.826, -9.9273, -9.3113, -7.4159, -8.9844, -8.7093, -10.655, -10.6581, -8.3888, -10.4166, -9.9455, -9.9603, -10.6952, -10.4196, -10.393, -10.6979, -10.7195, -9.3802, -9.4208, -9.0352, -8.3222, -7.9025, -7.6681, -7.8707, -9.2507, -8.7575, -8.6168, -8.5509, -8.1038, -8.4709, -6.6847, -6.9492, -7.8905, -6.7844, -4.8768, -6.2412, -5.22, -6.6355, -6.8131, -7.3149, -5.3996, -6.8954, -5.1842, -5.6709, -5.4588, -6.2746, -5.7676, -6.0411, -6.4981, -6.1707, -5.9022, -6.1523, -6.1241, -5.701, -5.8699, -6.2563, -5.7662, -5.6441, -6.0515, -6.3661, -6.2876, -5.6242, -5.8251, -6.3828, -6.0165, -6.049, -6.145, -5.9928, -5.9547, -6.1766, -5.8408, -5.9036, -6.1411, -6.1463, -6.1437, -8.8695, -9.8573, -9.8351, -9.314, -9.5254, -9.0664, -10.4253, -8.863, -8.9887, -7.4698, -10.137, -10.4365, -8.455, -9.9566, -8.9119, -8.5779, -10.036, -8.9889, -9.5133, -9.5139, -10.508, -9.3645, -9.5485, -9.6689, -10.2357, -10.3233, -9.8148, -7.3811, -10.5249, -9.7123, -9.4366, -7.784, -7.9204, -7.5606, -6.987, -7.4298, -8.3252, -7.3107, -6.886, -6.9771, -5.7256, -7.3808, -6.9902, -8.7335, -8.5753, -6.8949, -7.0338, -6.865, -5.3582, -6.7285, -5.6445, -5.3435, -6.0185, -6.2084, -7.0312, -6.0358, -5.5553, -5.975, -6.3352, -6.6131, -5.8808, -6.1657, -6.0179, -6.1591, -6.067, -5.6533, -5.8123, -5.826, -5.5533, -5.3283, -5.4826, -5.6959, -6.1706, -5.8681, -5.7904, -6.1809, -6.4135, -6.42, -6.1415, -6.2218, -6.2061, -6.1973, -6.2118, -7.2025, -10.0915, -8.5142, -10.0944, -9.8892, -7.8678, -8.5926, -10.1668, -7.2292, -8.7622, -9.9784, -9.8356, -10.0122, -10.2706, -7.1322, -9.4491, -8.5118, -10.5712, -9.1058, -9.4677, -10.5872, -9.2662, -10.596, -7.7276, -10.6397, -10.5503, -7.0938, -8.5748, -9.3424, -8.9682, -7.3948, -8.0714, -8.5034, -9.0329, -9.3514, -7.7657, -6.7825, -6.3778, -5.3058, -5.9502, -8.4496, -6.4861, -7.742, -7.5872, -6.4347, -7.3152, -7.4301, -6.4707, -5.3957, -7.0572, -5.8904, -6.3421, -5.4283, -6.5845, -5.2276, -6.857, -6.2241, -5.8763, -6.8411, -6.7383, -5.6971, -6.3991, -5.9833, -6.5364, -6.1745, -5.9607, -6.4707, -5.6714, -6.1338, -5.9331, -5.8588, -5.8722, -6.1361, -5.8678, -5.4227, -6.2287, -5.9384, -6.2331, -5.8202, -5.9436, -6.0727, -6.2474, -6.1714, -6.1455, -6.2238, -10.6171, -6.6683, -9.8368, -7.9696, -9.4758, -10.4256, -7.4914, -8.7598, -8.4494, -10.4001, -9.8446, -10.6952, -10.159, -9.8618, -10.7315, -9.1465, -10.801, -10.7343, -9.2973, -10.0166, -10.0123, -11.1656, -6.2521, -10.4287, -10.7491, -10.7744, -10.7441, -10.1902, -9.7644, -9.4106, -7.5917, -8.4273, -9.4039, -9.1549, -6.4813, -8.8613, -7.2561, -8.8169, -8.21, -7.3567, -8.3289, -7.143, -7.3304, -8.3121, -7.9887, -7.154, -7.1605, -7.1543, -7.3332, -5.7267, -6.9827, -7.3736, -7.2243, -7.0459, -6.0599, -4.8917, -5.7906, -6.7634, -7.2348, -5.4497, -5.7804, -6.8194, -5.4209, -6.5506, -6.4172, -5.3, -6.2991, -5.4844, -6.2282, -6.4938, -5.8783, -5.8176, -5.6401, -6.2166, -6.0548, -5.7655, -5.4535, -6.307, -6.1785, -6.2193, -6.4234, -5.8201, -6.0942, -5.9163, -6.0437, -6.2341, -6.1311, -6.255, -6.2181, -6.2737, -6.2679, -9.6707, -8.2145, -8.2678, -9.9422, -10.1832, -9.4952, -8.4529, -7.4963, -10.0003, -8.7338, -10.5289, -10.5285, -8.8738, -10.0467, -9.4696, -10.0722, -9.8622, -8.5642, -9.8803, -10.566, -8.9126, -10.3098, -8.9992, -8.3506, -10.0698, -9.9297, -9.1471, -8.8014, -10.3015, -7.8777, -7.8794, -7.426, -6.8369, -7.6761, -7.2005, -8.2267, -8.3927, -7.1088, -8.5365, -8.0574, -7.6373, -4.9887, -5.8849, -7.2309, -6.2301, -5.463, -6.5727, -6.4531, -6.9822, -5.6075, -6.1677, -5.906, -7.4026, -6.4711, -6.2903, -5.7098, -6.0071, -5.9342, -5.963, -5.1693, -5.5748, -5.5935, -5.6499, -6.1566, -5.512, -5.793, -5.9459, -6.1105, -6.3022, -5.7154, -6.3344, -6.3479, -6.3289, -5.8922, -6.207, -6.0665, -6.1638, -6.1382, -6.1529, -6.2107, -6.2784, -9.2488, -9.2313, -7.1148, -8.4521, -8.7102, -7.7875, -8.2547, -8.7238, -8.48, -7.9559, -9.1302, -10.0082, -9.4501, -9.2194, -10.4972, -9.2997, -7.3286, -10.215, -9.1235, -9.9002, -7.8055, -8.7595, -10.5805, -9.673, -10.0788, -8.3463, -10.5349, -10.1998, -9.558, -9.4425, -8.1601, -7.3732, -7.6947, -7.8011, -8.6213, -8.7243, -8.149, -6.9514, -7.6563, -5.8199, -7.0718, -6.1518, -5.9451, -8.4086, -6.1593, -6.3396, -5.1013, -6.2933, -5.4416, -6.2768, -5.6132, -5.6811, -6.0093, -6.3478, -5.513, -5.3979, -6.4838, -5.8944, -5.8576, -6.2463, -5.7967, -6.0489, -5.6493, -5.3111, -6.2096, -6.0294, -6.0297, -6.1193, -5.9791, -5.9028, -5.7661, -5.8478, -5.9126, -6.0789, -6.0701, -6.1588, -6.1907, -7.4001, -8.6138, -8.7486, -9.6131, -7.8589, -8.0772, -9.3062, -9.7016, -9.3703, -9.9736, -10.2713, -10.2946, -9.9975, -10.2929, -10.7193, -10.3229, -10.0401, -10.3321, -10.7398, -10.3374, -9.3227, -10.3307, -10.0658, -10.3406, -7.6687, -10.7682, -10.3648, -10.3742, -10.7875, -10.8059, -9.334, -9.8472, -7.1981, -9.1358, -7.5616, -8.492, -6.4929, -8.6222, -9.2666, -7.5171, -7.8453, -6.72, -6.3968, -6.4592, -6.1233, -7.8254, -7.776, -7.7535, -5.92, -4.8446, -6.7731, -6.4206, -6.4093, -7.1083, -7.4641, -5.6629, -6.0304, -7.2815, -5.8672, -6.1795, -5.6575, -5.8338, -5.2824, -5.9349, -5.7625, -5.5796, -5.6449, -5.6236, -6.2405, -5.631, -5.8972, -6.101, -6.0285, -5.8106, -6.0966, -5.6926, -5.9562, -6.0301, -6.181, -5.9588, -6.187, -6.2012, -6.2614, -10.1625, -10.4517, -8.9366, -9.2104, -9.2332, -10.9785, -10.2652, -10.5772, -9.2621, -9.3586, -10.083, -10.3046, -9.2676, -7.8607, -9.2974, -10.5925, -11.0393, -11.0401, -9.759, -10.3778, -10.6357, -10.4176, -11.0596, -10.6586, -10.6782, -11.0698, -10.1534, -10.6761, -10.4462, -10.6418, -9.5485, -10.3974, -9.3471, -9.6544, -9.5649, -8.5482, -9.9065, -8.9978, -8.2303, -9.3096, -8.9935, -8.0813, -7.5884, -6.4886, -6.9935, -8.216, -6.4577, -7.4728, -5.9968, -7.9665, -8.6211, -6.5997, -7.5047, -7.0993, -6.0739, -7.9488, -5.4029, -5.7569, -6.7785, -5.628, -6.4683, -6.2245, -6.1217, -6.3889, -6.7075, -6.0382, -6.0141, -6.7737, -6.6743, -5.8244, -6.2506, -5.7503, -5.1277, -5.621, -6.4573, -5.7685, -5.4531, -5.6523, -6.3656, -6.0186, -5.6254, -5.6613, -6.2274, -5.9503, -6.052, -5.6585, -6.2179, -6.1611, -6.018, -6.2982, -6.2479, -6.2822, -6.2623], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9875, 0.9342, 0.8864, 0.8721, 0.871, 0.8576, 0.84, 0.8398, 0.8353, 0.8321, 0.8181, 0.8081, 0.7706, 0.7636, 0.7542, 0.7302, 0.7228, 0.7201, 0.7075, 0.7049, 0.7043, 0.702, 0.701, 0.6991, 0.6983, 0.6954, 0.6918, 0.6905, 0.6903, 0.6881, 0.6826, 0.6602, 0.6761, 0.6007, 0.6586, 0.531, 0.5602, 0.609, 0.6079, 0.4751, 0.5636, 0.4457, 0.5701, 0.5033, 0.2513, 0.1829, 0.2185, 0.2114, 0.3739, 0.3033, 0.2346, 0.279, 0.2086, 0.3023, 0.0607, 0.0975, 0.3227, 0.1362, -0.0258, 0.0884, 0.1362, 0.0554, 0.2697, 0.269, 0.0288, 0.1051, 0.1973, 0.1206, 0.1113, -0.0539, -0.0865, -0.0167, 0.1602, 0.0341, 0.0244, 0.1907, 0.1052, 0.0893, 0.0527, -0.2064, -0.0156, -0.0307, -0.1753, -0.088, 1.0453, 1.0007, 0.9768, 0.9444, 0.9307, 0.9162, 0.9075, 0.8909, 0.8886, 0.8847, 0.8791, 0.8543, 0.8471, 0.8435, 0.8325, 0.8285, 0.8258, 0.8091, 0.8063, 0.8051, 0.7945, 0.7878, 0.7856, 0.7847, 0.7689, 0.7673, 0.7613, 0.7549, 0.7533, 0.7528, 0.7386, 0.7282, 0.745, 0.7407, 0.7236, 0.6972, 0.6144, 0.6457, 0.6878, 0.6719, 0.6206, 0.5218, 0.5484, 0.2903, 0.5233, 0.5495, 0.3162, 0.5135, 0.4125, 0.5704, 0.1767, 0.299, 0.3255, 0.2942, 0.3444, 0.3671, 0.0662, 0.095, 0.3354, 0.0736, 0.1216, 0.0664, 0.0661, -0.0354, 0.2356, 0.1539, 0.1838, 0.1403, 0.1838, -0.0267, 0.0973, 0.0544, -0.0247, 0.1775, -0.0248, -0.0188, -0.0117, 0.0893, -0.0135, -0.139, -0.1574, -0.2283, -0.0626, -0.0113, -0.0111, 1.0126, 0.9904, 0.9262, 0.8738, 0.871, 0.8453, 0.8276, 0.8256, 0.8039, 0.7959, 0.7716, 0.7684, 0.757, 0.7569, 0.753, 0.7508, 0.7468, 0.7405, 0.7328, 0.7232, 0.7147, 0.7145, 0.7131, 0.7126, 0.7113, 0.7064, 0.7057, 0.7029, 0.6961, 0.6921, 0.6869, 0.6613, 0.6514, 0.631, 0.6758, 0.6549, 0.6478, 0.6402, 0.6047, 0.6214, 0.4461, 0.4696, 0.5472, 0.4142, 0.1293, 0.293, 0.1361, 0.3404, 0.3685, 0.4446, 0.1112, 0.3701, 0.0462, 0.1302, 0.0872, 0.2412, 0.1429, 0.1672, 0.2663, 0.1943, 0.1253, 0.1818, 0.1668, 0.045, 0.0817, 0.1948, 0.0221, -0.0257, 0.1137, 0.2199, 0.1869, -0.101, -0.0245, 0.2225, 0.0302, 0.0437, 0.0915, -0.0178, -0.0742, 0.0845, -0.2314, -0.2225, 0.044, 0.0057, -0.1611, 1.0357, 1.0116, 0.9969, 0.9835, 0.9814, 0.9778, 0.9753, 0.9721, 0.9686, 0.9628, 0.9552, 0.9478, 0.9422, 0.9401, 0.9278, 0.9138, 0.9086, 0.9004, 0.8938, 0.8849, 0.8831, 0.88, 0.8712, 0.8629, 0.8628, 0.8614, 0.8564, 0.8365, 0.8363, 0.8354, 0.8235, 0.7871, 0.7814, 0.7692, 0.724, 0.734, 0.78, 0.709, 0.6704, 0.6647, 0.5109, 0.6691, 0.5999, 0.7941, 0.7725, 0.5412, 0.5573, 0.5128, 0.2602, 0.4763, 0.266, 0.1798, 0.2774, 0.3166, 0.491, 0.2551, 0.1257, 0.224, 0.3036, 0.3727, 0.1658, 0.1993, 0.134, 0.1762, 0.1326, -0.0438, -0.0116, -0.0249, -0.1972, -0.3221, -0.2521, -0.1851, 0.0905, -0.1221, -0.2445, 0.0042, 0.1968, 0.1917, -0.083, -0.1291, -0.2235, -0.2457, -0.2369, 1.1412, 1.0712, 1.07, 1.0571, 1.0543, 1.02, 0.9858, 0.9854, 0.9779, 0.9439, 0.926, 0.9129, 0.9116, 0.9053, 0.8993, 0.8901, 0.8748, 0.8619, 0.8592, 0.8483, 0.8456, 0.8452, 0.841, 0.836, 0.8242, 0.8202, 0.8174, 0.8117, 0.8116, 0.7899, 0.7608, 0.7726, 0.7469, 0.7745, 0.7865, 0.6826, 0.5408, 0.4903, 0.3126, 0.385, 0.7018, 0.3937, 0.5909, 0.5617, 0.3616, 0.5169, 0.5324, 0.3527, 0.1275, 0.4504, 0.2023, 0.2917, 0.0825, 0.3303, 0.0028, 0.3888, 0.1977, 0.0753, 0.3675, 0.3331, -0.016, 0.2127, 0.0634, 0.2568, 0.1214, 0.0219, 0.2239, -0.1254, 0.0746, -0.0226, -0.0581, -0.0711, 0.0629, -0.1218, -0.4165, 0.0951, -0.1501, 0.0694, -0.4641, -0.3341, -0.1922, 0.0136, -0.113, -0.1705, -0.1963, 0.8066, 0.7849, 0.7572, 0.7441, 0.743, 0.7425, 0.723, 0.7199, 0.7176, 0.7139, 0.7115, 0.7036, 0.7023, 0.7019, 0.7003, 0.6998, 0.6993, 0.6979, 0.6948, 0.6921, 0.688, 0.6863, 0.6852, 0.6716, 0.6645, 0.662, 0.6589, 0.658, 0.6575, 0.6468, 0.6363, 0.6317, 0.6443, 0.6384, 0.5657, 0.6275, 0.5765, 0.6208, 0.5878, 0.5375, 0.5878, 0.5124, 0.5181, 0.5767, 0.5505, 0.4835, 0.4818, 0.475, 0.4883, 0.3318, 0.4482, 0.4803, 0.4495, 0.4184, 0.2742, 0.1144, 0.2369, 0.3674, 0.4316, 0.1598, 0.1946, 0.3622, 0.1251, 0.3175, 0.2822, 0.0561, 0.235, 0.0264, 0.1863, 0.2574, 0.0321, -0.0165, -0.1169, 0.1072, 0.0379, -0.0844, -0.2231, 0.1289, 0.058, 0.0716, 0.1378, -0.2018, -0.0475, -0.1703, -0.2555, -0.0489, -0.3304, -0.1031, -0.2355, -0.0654, -0.3874, 1.0818, 1.0125, 0.9852, 0.9791, 0.9617, 0.9485, 0.9215, 0.9178, 0.9098, 0.9065, 0.9059, 0.9059, 0.892, 0.8696, 0.8657, 0.8656, 0.8637, 0.8628, 0.8602, 0.857, 0.8542, 0.8502, 0.8487, 0.8426, 0.8382, 0.8376, 0.8367, 0.8345, 0.834, 0.8283, 0.7539, 0.7296, 0.6887, 0.7195, 0.6807, 0.7189, 0.7184, 0.5868, 0.7275, 0.6737, 0.6131, 0.2417, 0.3516, 0.5436, 0.3801, 0.2181, 0.4057, 0.3658, 0.4662, 0.1936, 0.27, 0.1867, 0.5253, 0.2887, 0.2407, 0.0909, 0.1448, 0.1125, 0.0954, -0.1631, -0.0516, -0.0476, -0.0315, 0.1392, -0.1559, -0.047, 0.0057, 0.0747, 0.1396, -0.2046, 0.1015, 0.1033, 0.0856, -0.2828, -0.0418, -0.2783, -0.1812, -0.2277, -0.2723, -0.1832, -0.3034, 1.1273, 1.085, 1.0741, 1.0719, 1.0308, 1.0255, 1.0218, 0.9965, 0.9957, 0.9739, 0.9643, 0.9553, 0.9435, 0.9365, 0.9353, 0.9209, 0.9201, 0.9177, 0.9121, 0.9108, 0.9001, 0.8972, 0.891, 0.8904, 0.8897, 0.8839, 0.8839, 0.8833, 0.8733, 0.8686, 0.8506, 0.814, 0.7871, 0.7881, 0.8231, 0.8229, 0.77, 0.6721, 0.6974, 0.5153, 0.6209, 0.4033, 0.3574, 0.7712, 0.3657, 0.3995, 0.1292, 0.3546, 0.1679, 0.3471, 0.1328, 0.1071, 0.1991, 0.286, 0.0103, -0.0418, 0.298, 0.0806, 0.0529, 0.1914, 0.0044, 0.1031, -0.1033, -0.305, 0.1245, 0.029, 0.0169, 0.0658, -0.0275, -0.1021, -0.2553, -0.2295, -0.2316, -0.0514, -0.0875, 0.0403, -0.098, 1.5069, 1.4787, 1.4089, 1.404, 1.3729, 1.3602, 1.3544, 1.2982, 1.2878, 1.258, 1.2322, 1.2146, 1.212, 1.2095, 1.1957, 1.1808, 1.1788, 1.1749, 1.1662, 1.1651, 1.1631, 1.1605, 1.1574, 1.1507, 1.1429, 1.1335, 1.1281, 1.1177, 1.1119, 1.0937, 1.0926, 1.0901, 0.9943, 1.0554, 0.9266, 0.9648, 0.751, 0.962, 1.0251, 0.8024, 0.8284, 0.6598, 0.609, 0.5266, 0.4379, 0.7619, 0.7506, 0.7447, 0.3825, 0.1616, 0.5447, 0.4475, 0.4141, 0.5654, 0.656, 0.1254, 0.2307, 0.6015, 0.1602, 0.2582, 0.0885, 0.1411, -0.052, 0.1118, 0.0386, -0.0337, -0.0355, -0.1003, 0.1955, -0.1202, 0.0133, 0.1074, 0.0299, -0.1295, 0.0553, -0.3365, -0.1555, -0.0785, 0.0555, -0.3405, -0.0943, -0.2186, -0.0623, 0.9961, 0.996, 0.9373, 0.9179, 0.8909, 0.8832, 0.8736, 0.8533, 0.8462, 0.837, 0.8367, 0.8319, 0.8305, 0.8184, 0.8116, 0.8109, 0.8043, 0.8041, 0.7982, 0.7894, 0.7867, 0.7856, 0.7855, 0.7766, 0.7755, 0.7744, 0.7722, 0.7716, 0.7695, 0.7686, 0.7679, 0.7683, 0.7529, 0.7578, 0.7523, 0.7202, 0.7566, 0.7225, 0.6776, 0.726, 0.7056, 0.638, 0.6005, 0.5172, 0.5287, 0.613, 0.432, 0.5263, 0.3384, 0.56, 0.6372, 0.3862, 0.4903, 0.4263, 0.2603, 0.5394, 0.1431, 0.1947, 0.3266, 0.118, 0.2708, 0.2132, 0.1809, 0.235, 0.2995, 0.147, 0.1379, 0.3013, 0.279, 0.086, 0.1853, 0.0507, -0.1216, -0.0026, 0.2174, 0.0322, -0.097, -0.0428, 0.1686, 0.0398, -0.1146, -0.1381, 0.0684, -0.0697, -0.0771, -0.4281, 0.0186, -0.0683, -0.3369, 0.0257, -0.2204, -0.117, -0.474]}, \"token.table\": {\"Topic\": [3, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 5, 1, 4, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 7, 1, 2, 3, 4, 6, 9, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 6, 1, 2, 3, 6, 1, 2, 3, 4, 5, 7, 8, 1, 3, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 8, 9, 1, 1, 2, 3, 4, 5, 7, 8, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 1, 2, 3, 5, 6, 1, 3, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 6, 7, 2, 3, 4, 3, 4, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 4, 1, 2, 3, 5, 1, 3, 4, 5, 7, 8, 1, 2, 3, 4, 6, 7, 8, 2, 3, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 1, 2, 1, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 7, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 8, 1, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 7, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 3, 6, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 8, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 2, 3, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 7, 8, 1, 3, 5, 8, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 7, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 3, 4, 8, 1, 3, 9, 1, 2, 3, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 8, 1, 1, 1, 1, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 3, 4, 8, 1, 2, 3, 4, 5, 9, 1, 1, 2, 3, 4, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 1, 3, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 7, 1, 2, 3, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 7, 8, 1, 2, 3, 5, 6, 1, 2, 3, 4, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 7, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 6, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 1, 2, 3, 5, 6, 7, 8, 9, 1, 2, 3, 4, 6, 7, 8, 9, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 6, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 5, 6, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 9, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 2, 3, 4, 5, 8, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 3, 4, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 6, 1, 2, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 8, 9, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 7, 1, 2, 3, 4, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5], \"Freq\": [0.3774899180124281, 0.19726601528412263, 0.09863300764206132, 0.2958990229261839, 0.09863300764206132, 0.09863300764206132, 0.09863300764206132, 0.09863300764206132, 0.1433797789250919, 0.35844944731272976, 0.1911730385667892, 0.07168988946254595, 0.0477932596416973, 0.0477932596416973, 0.07168988946254595, 0.0477932596416973, 0.0477932596416973, 0.15712346321790407, 0.3928086580447602, 0.07856173160895204, 0.03928086580447602, 0.03928086580447602, 0.11784259741342806, 0.07856173160895204, 0.03928086580447602, 0.03928086580447602, 0.3354669222291207, 0.08478834298098656, 0.11796639023441607, 0.11059349084476508, 0.1363986387085436, 0.04423739633790603, 0.06266964481203355, 0.06635609450685905, 0.022118698168953014, 0.01474579877930201, 0.2636456106290713, 0.10545824425162852, 0.10545824425162852, 0.10545824425162852, 0.10545824425162852, 0.05272912212581426, 0.05272912212581426, 0.05272912212581426, 0.10545824425162852, 0.2774997607463155, 0.11232133173065151, 0.15196415469441085, 0.08589278308814527, 0.05946423444563904, 0.0660713716062656, 0.0660713716062656, 0.07267850876689215, 0.08589278308814527, 0.01321427432125312, 0.14985471499625827, 0.16234260791261312, 0.14361076853808086, 0.09990314333083886, 0.16234260791261312, 0.06243946458177428, 0.09990314333083886, 0.06868341103995171, 0.043707625207241994, 0.018731839374532284, 0.15949258870579572, 0.15949258870579572, 0.15949258870579572, 0.15949258870579572, 0.15949258870579572, 0.15949258870579572, 0.22746548812715167, 0.19247079764605143, 0.08748672620275065, 0.06998938096220052, 0.05249203572165039, 0.08748672620275065, 0.06998938096220052, 0.13997876192440104, 0.05249203572165039, 0.01749734524055013, 0.20477530191509363, 0.2520311408185768, 0.0630077852046442, 0.0630077852046442, 0.0315038926023221, 0.07875973150580524, 0.0630077852046442, 0.1890233556139326, 0.04725583890348315, 0.01575194630116105, 0.2263432628950336, 0.2263432628950336, 0.1131716314475168, 0.1131716314475168, 0.1131716314475168, 0.1131716314475168, 0.2263432628950336, 0.17612913507150188, 0.26419370260725283, 0.08806456753575094, 0.04403228376787547, 0.04403228376787547, 0.08806456753575094, 0.04403228376787547, 0.22016141883937737, 0.04403228376787547, 0.29418614090156453, 0.29418614090156453, 0.29680514064308067, 0.29680514064308067, 0.199379076530327, 0.398758153060654, 0.1382168284256495, 0.1382168284256495, 0.276433656851299, 0.06910841421282475, 0.1382168284256495, 0.1382168284256495, 0.06910841421282475, 0.06910841421282475, 0.26812885579404533, 0.26812885579404533, 0.26812885579404533, 0.19864178090745935, 0.1679941918531656, 0.14302208225337074, 0.12486054799897445, 0.1294009315625735, 0.062430273999487224, 0.0703759452357856, 0.05788989043588815, 0.0317826849451935, 0.01475624658169698, 0.20786197410866633, 0.13857464940577757, 0.1511723448063028, 0.12912637785538364, 0.09448271550393925, 0.08503444395354533, 0.0755861724031514, 0.05668962930236355, 0.04409193390183831, 0.01574711925065654, 0.22692531134537566, 0.22692531134537566, 0.22692531134537566, 0.22692531134537566, 0.17593455274360362, 0.17070877394923917, 0.15154758503656945, 0.11322520721123006, 0.10103172335771297, 0.0731609031211025, 0.08187053444504327, 0.0731609031211025, 0.04877393541406833, 0.012193483853517083, 0.1875784668463203, 0.1875784668463203, 0.1875784668463203, 0.1875784668463203, 0.13791257680201233, 0.13791257680201233, 0.27582515360402465, 0.13791257680201233, 0.13791257680201233, 0.13791257680201233, 0.2872891431800004, 0.2872891431800004, 0.17792061308505155, 0.3558412261701031, 0.08896030654252578, 0.08896030654252578, 0.08896030654252578, 0.08896030654252578, 0.08896030654252578, 0.08896030654252578, 0.12236859471054637, 0.4894743788421855, 0.052443683447377015, 0.03496245563158468, 0.03496245563158468, 0.08740613907896169, 0.06992491126316935, 0.052443683447377015, 0.03496245563158468, 0.01748122781579234, 0.1131330818846487, 0.1508441091795316, 0.3016882183590632, 0.1131330818846487, 0.0754220545897658, 0.0754220545897658, 0.0377110272948829, 0.0377110272948829, 0.0754220545897658, 0.206819911085527, 0.15562686378712923, 0.17405636081455242, 0.07986115378550052, 0.10238609459679554, 0.07167026621775688, 0.05938393486614141, 0.08805204135324417, 0.04914532540646186, 0.014334053243551375, 0.3222334562880821, 0.13478971456978386, 0.13478971456978386, 0.30327685778201374, 0.1010922859273379, 0.06739485728489193, 0.1010922859273379, 0.06739485728489193, 0.033697428642445966, 0.06739485728489193, 0.36338170793937774, 0.29192211005389423, 0.11161727737354779, 0.12878916620024747, 0.18889077709369625, 0.07727349972014848, 0.02575783324004949, 0.03434377765339932, 0.11161727737354779, 0.02575783324004949, 0.00858594441334983, 0.29352653832310205, 0.11414920934787301, 0.130456239254712, 0.17122381402180953, 0.065228119627356, 0.032614059813678, 0.04892108972051701, 0.11414920934787301, 0.024460544860258504, 0.0081535149534195, 0.1779752340125605, 0.355950468025121, 0.1779752340125605, 0.12780227777441266, 0.383406833323238, 0.12780227777441266, 0.12780227777441266, 0.20553101476470448, 0.10276550738235224, 0.20553101476470448, 0.10276550738235224, 0.10276550738235224, 0.10276550738235224, 0.10276550738235224, 0.2509992919290757, 0.2509992919290757, 0.2509992919290757, 0.16985880575166268, 0.16091886860683832, 0.15197893146201397, 0.127394104313747, 0.10727924573789221, 0.06704952858618264, 0.08269441858962526, 0.07598946573100698, 0.04022971715170958, 0.015644890003442616, 0.10203102583947417, 0.20406205167894834, 0.3060930775184225, 0.10203102583947417, 0.10203102583947417, 0.10203102583947417, 0.10203102583947417, 0.10203102583947417, 0.4739269124157019, 0.13811443319303832, 0.13811443319303832, 0.13811443319303832, 0.27622886638607663, 0.13811443319303832, 0.13811443319303832, 0.13811443319303832, 0.6396504414705281, 0.21581203299952828, 0.16185902474964622, 0.16185902474964622, 0.05395300824988207, 0.05395300824988207, 0.16185902474964622, 0.05395300824988207, 0.05395300824988207, 0.05395300824988207, 0.2514679806962572, 0.1257339903481286, 0.14859471586597015, 0.10287326483028703, 0.06858217655352468, 0.06858217655352468, 0.1257339903481286, 0.0571518137946039, 0.04572145103568312, 0.01143036275892078, 0.22594398244748137, 0.11297199122374069, 0.22594398244748137, 0.11297199122374069, 0.11297199122374069, 0.11297199122374069, 0.11297199122374069, 0.19050559725885421, 0.19050559725885421, 0.19050559725885421, 0.09525279862942711, 0.09525279862942711, 0.19050559725885421, 0.09525279862942711, 0.09525279862942711, 0.5272819770844388, 0.10545639541688775, 0.10545639541688775, 0.10545639541688775, 0.10545639541688775, 0.10545639541688775, 0.5070613273129685, 0.06338266591412106, 0.12676533182824212, 0.06338266591412106, 0.06338266591412106, 0.06338266591412106, 0.06338266591412106, 0.22257860607812352, 0.11128930303906176, 0.11128930303906176, 0.22257860607812352, 0.07419286869270784, 0.03709643434635392, 0.07419286869270784, 0.07419286869270784, 0.03709643434635392, 0.36765597261969457, 0.5303858952840839, 0.23338176974335473, 0.20200270826525663, 0.16474007276001512, 0.08237003638000756, 0.07844765369524528, 0.06471931429857736, 0.06864169698333963, 0.0607969316138151, 0.029417870135716985, 0.013728339396667925, 0.13513718450087195, 0.1726752913066697, 0.21021339811246748, 0.05255334952811687, 0.15015242722319105, 0.09009145633391463, 0.08258383497275508, 0.05255334952811687, 0.03003048544463821, 0.015015242722319105, 0.2140450006978767, 0.10702250034893834, 0.1146669646595768, 0.1146669646595768, 0.09173357172766144, 0.07644464310638453, 0.14524482190213062, 0.06880017879574608, 0.04586678586383072, 0.02293339293191536, 0.481158213394607, 0.137473775255602, 0.068736887627801, 0.068736887627801, 0.068736887627801, 0.068736887627801, 0.068736887627801, 0.068736887627801, 0.16441505361269754, 0.16441505361269754, 0.16441505361269754, 0.16441505361269754, 0.16441505361269754, 0.22552389650672555, 0.22552389650672555, 0.22552389650672555, 0.22552389650672555, 0.14203742121802782, 0.11621243554202276, 0.1291249283800253, 0.1807748997320354, 0.14203742121802782, 0.06456246419001264, 0.10329994270402024, 0.05164997135201012, 0.05164997135201012, 0.01291249283800253, 0.1474371627566489, 0.09829144183776593, 0.19658288367553187, 0.09829144183776593, 0.19658288367553187, 0.09829144183776593, 0.04914572091888297, 0.04914572091888297, 0.04914572091888297, 0.18174413764924624, 0.14414190227354012, 0.15354246111746664, 0.09400558843926529, 0.12534078458568707, 0.08460502959533876, 0.06893743152212789, 0.09713910805390748, 0.040735754990348295, 0.012534078458568706, 0.1287985746331683, 0.06439928731658415, 0.1287985746331683, 0.1287985746331683, 0.1287985746331683, 0.1287985746331683, 0.06439928731658415, 0.19319786194975244, 0.06439928731658415, 0.13221022251802914, 0.13221022251802914, 0.13221022251802914, 0.13221022251802914, 0.13221022251802914, 0.13221022251802914, 0.13221022251802914, 0.26442044503605827, 0.38024397496024337, 0.20405369868808781, 0.11427007126532918, 0.10610792331780566, 0.17140510689799376, 0.11427007126532918, 0.05713503563266459, 0.08978362742275864, 0.09794577537028215, 0.04081073973761756, 0.016324295895047025, 0.13972380052077454, 0.2794476010415491, 0.13972380052077454, 0.13972380052077454, 0.21155240582310347, 0.12088708904177341, 0.10577620291155174, 0.060443544520886704, 0.2568850642137685, 0.060443544520886704, 0.060443544520886704, 0.04533265839066503, 0.07555443065110838, 0.015110886130221676, 0.07189131512607683, 0.21567394537823048, 0.14378263025215365, 0.07189131512607683, 0.07189131512607683, 0.07189131512607683, 0.14378263025215365, 0.07189131512607683, 0.07189131512607683, 0.22461035654060926, 0.07487011884686975, 0.07487011884686975, 0.07487011884686975, 0.22461035654060926, 0.07487011884686975, 0.07487011884686975, 0.07487011884686975, 0.07487011884686975, 0.16866810833274112, 0.08433405416637056, 0.2530021624991117, 0.08433405416637056, 0.08433405416637056, 0.08433405416637056, 0.08433405416637056, 0.08433405416637056, 0.08433405416637056, 0.15061619869066817, 0.35771347189033686, 0.0941351241816676, 0.056481074509000556, 0.056481074509000556, 0.07530809934533408, 0.07530809934533408, 0.07530809934533408, 0.03765404967266704, 0.01882702483633352, 0.16982966376535613, 0.16982966376535613, 0.18398213574580247, 0.08491483188267807, 0.07076235990223172, 0.07783859589245488, 0.11321977584357075, 0.056609887921785375, 0.0495336519315622, 0.007076235990223172, 0.1948405526149072, 0.1948405526149072, 0.15587244209192577, 0.121775345384317, 0.08280723486133557, 0.06332317959984485, 0.0730652072305902, 0.05358115196909948, 0.04383912433835412, 0.01948405526149072, 0.1786367241728003, 0.11909114944853354, 0.05954557472426677, 0.05954557472426677, 0.23818229889706707, 0.05954557472426677, 0.05954557472426677, 0.05954557472426677, 0.05954557472426677, 0.1438717947478908, 0.18497802181871675, 0.16442490828330378, 0.0719358973739454, 0.11304212444477134, 0.12331868121247783, 0.051382783838532425, 0.08221245414165189, 0.041106227070825944, 0.020553113535412972, 0.20233400525551795, 0.13488933683701196, 0.10116700262775898, 0.07868544648825698, 0.21357478332526894, 0.044963112279003986, 0.08992622455800797, 0.05620389034875498, 0.06744466841850598, 0.011240778069750997, 0.21097623313613104, 0.4219524662722621, 0.1587128727212142, 0.10580858181414281, 0.3174257454424284, 0.052904290907071404, 0.052904290907071404, 0.10580858181414281, 0.052904290907071404, 0.052904290907071404, 0.08115369683146129, 0.18259581787078788, 0.2840379389101145, 0.12173054524719193, 0.060865272623595965, 0.10144212103932661, 0.04057684841573064, 0.04057684841573064, 0.08115369683146129, 0.02028842420786532, 0.13906597322119787, 0.13906597322119787, 0.13906597322119787, 0.13906597322119787, 0.13906597322119787, 0.13906597322119787, 0.13906597322119787, 0.2225616951801177, 0.16711379880306415, 0.1540219343807043, 0.07470063817464158, 0.09472348964413314, 0.0608386640803782, 0.09241316062842257, 0.08240173489367679, 0.04235603195469368, 0.010011425734745779, 0.3084684409865371, 0.2125109206551486, 0.1062554603275743, 0.2125109206551486, 0.1062554603275743, 0.1062554603275743, 0.1062554603275743, 0.1062554603275743, 0.2125109206551486, 0.2255298851337752, 0.1127649425668876, 0.1252943806298751, 0.20047100900780018, 0.10023550450390009, 0.1127649425668876, 0.050117752251950046, 0.050117752251950046, 0.03758831418896253, 0.012529438062987511, 0.08701202647963638, 0.13051803971945455, 0.2610360794389091, 0.13051803971945455, 0.04350601323981819, 0.13051803971945455, 0.04350601323981819, 0.04350601323981819, 0.08701202647963638, 0.10087510360588099, 0.1513126554088215, 0.302625310817643, 0.1513126554088215, 0.050437551802940496, 0.1513126554088215, 0.050437551802940496, 0.050437551802940496, 0.10087510360588099, 0.09868836372706362, 0.19737672745412724, 0.29606509118119084, 0.09868836372706362, 0.09868836372706362, 0.09868836372706362, 0.17927417306201918, 0.1493951442183493, 0.08963708653100959, 0.11951611537467945, 0.11951611537467945, 0.1493951442183493, 0.08963708653100959, 0.05975805768733972, 0.05975805768733972, 0.02987902884366986, 0.21317861955800527, 0.15227044254143232, 0.1827245310497188, 0.06090817701657293, 0.06090817701657293, 0.0913622655248594, 0.06090817701657293, 0.0913622655248594, 0.030454088508286466, 0.030454088508286466, 0.20287362678895435, 0.19881615425317525, 0.11766670353759352, 0.11969543980548307, 0.10143681339447717, 0.06897703310824448, 0.06289082430457585, 0.0750632419119131, 0.04260346162568041, 0.01217241760733726, 0.22864798160843472, 0.22864798160843472, 0.1759878073514901, 0.12319146514604307, 0.19358658808663912, 0.07919451330817055, 0.09679329404331956, 0.11439207477846856, 0.07039512294059604, 0.07919451330817055, 0.03519756147029802, 0.01759878073514901, 0.34811438939011763, 0.1367592244032605, 0.08702859734752941, 0.08702859734752941, 0.08702859734752941, 0.062163283819663864, 0.08702859734752941, 0.04973062705573109, 0.03729797029179832, 0.012432656763932773, 0.3623827146302306, 0.23200962072498663, 0.2369112324304441, 0.12580803377340824, 0.0767919167188336, 0.09639836354066346, 0.06045321103397539, 0.06045321103397539, 0.06698869330791868, 0.03594515250668807, 0.009803223410914929, 0.14152114272271804, 0.10108653051622717, 0.14152114272271804, 0.16173844882596347, 0.1213038366194726, 0.08086922441298174, 0.08086922441298174, 0.0606519183097363, 0.10108653051622717, 0.020217306103245434, 0.37312509864715865, 0.05369377756204697, 0.10738755512409394, 0.37585644293432874, 0.10738755512409394, 0.05369377756204697, 0.10738755512409394, 0.05369377756204697, 0.05369377756204697, 0.05369377756204697, 0.4382122417308588, 0.17528489669234354, 0.08764244834617177, 0.08764244834617177, 0.22112074589935796, 0.22112074589935796, 0.22112074589935796, 0.2837670105375451, 0.2837670105375451, 0.3042501329581661, 0.155141635511174, 0.10342775700744934, 0.155141635511174, 0.05171387850372467, 0.10342775700744934, 0.05171387850372467, 0.10342775700744934, 0.10342775700744934, 0.155141635511174, 0.14110311661378375, 0.4233093498413512, 0.14110311661378375, 0.04703437220459458, 0.04703437220459458, 0.04703437220459458, 0.04703437220459458, 0.04703437220459458, 0.04703437220459458, 0.1720376223971966, 0.04300940559929915, 0.21504702799649578, 0.0860188111985983, 0.0860188111985983, 0.04300940559929915, 0.21504702799649578, 0.0860188111985983, 0.04300940559929915, 0.13168274698031904, 0.2880560090194479, 0.11522240360777916, 0.0823017168626994, 0.0823017168626994, 0.09876206023523929, 0.0823017168626994, 0.06584137349015952, 0.03292068674507976, 0.01646034337253988, 0.08380552926838987, 0.2514165878051696, 0.16761105853677974, 0.08380552926838987, 0.041902764634194935, 0.041902764634194935, 0.20951382317097467, 0.041902764634194935, 0.041902764634194935, 0.35408530784839465, 0.35408530784839465, 0.22424537650418191, 0.22424537650418191, 0.22424537650418191, 0.22424537650418191, 0.14147209854764592, 0.14147209854764592, 0.14147209854764592, 0.14147209854764592, 0.14147209854764592, 0.14147209854764592, 0.15974326290725696, 0.3993581572681424, 0.07987163145362848, 0.07987163145362848, 0.07987163145362848, 0.07987163145362848, 0.07987163145362848, 0.19551569815366682, 0.19551569815366682, 0.19551569815366682, 0.19551569815366682, 0.22904196152457648, 0.22904196152457648, 0.22904196152457648, 0.22904196152457648, 0.24958933554897986, 0.14559377907023827, 0.1663928903659866, 0.0831964451829933, 0.12479466777448993, 0.020799111295748324, 0.0831964451829933, 0.0831964451829933, 0.020799111295748324, 0.020799111295748324, 0.17862691576962397, 0.26794037365443596, 0.08931345788481199, 0.08931345788481199, 0.08931345788481199, 0.17862691576962397, 0.08931345788481199, 0.2396340426143393, 0.2069566731669294, 0.166109961357667, 0.07624719537728977, 0.07624719537728977, 0.07352408125667229, 0.04629294005049736, 0.06263162477420231, 0.04629294005049736, 0.010892456482469969, 0.2817697967943323, 0.2817697967943323, 0.2817697967943323, 0.2832198473770366, 0.15130923353019765, 0.09699309841679336, 0.10863227022680856, 0.08147420267010642, 0.06595530692341948, 0.06983503086009121, 0.08535392660677815, 0.03879723936671734, 0.015518895746686938, 0.29122123892831214, 0.29122123892831214, 0.29122123892831214, 0.0873944600215183, 0.0873944600215183, 0.3932750700968323, 0.12016738252958765, 0.03277292250806936, 0.07647015251882851, 0.054621537513448934, 0.054621537513448934, 0.06554584501613872, 0.010924307502689787, 0.06457332860136275, 0.1076222143356046, 0.3443910858739347, 0.1291466572027255, 0.043048885734241836, 0.1076222143356046, 0.043048885734241836, 0.043048885734241836, 0.06457332860136275, 0.16669172953184208, 0.18630252124147056, 0.15688633367702784, 0.06863777098369968, 0.10785935440295664, 0.0490269792740712, 0.12747014611258511, 0.058832375128885436, 0.0490269792740712, 0.00980539585481424, 0.19015946208470763, 0.0887410823061969, 0.12677297472313842, 0.07606378483388306, 0.22819135450164918, 0.0887410823061969, 0.0887410823061969, 0.03803189241694153, 0.07606378483388306, 0.012677297472313843, 0.19054954882656266, 0.10393611754176145, 0.06929074502784097, 0.051968058770880725, 0.29448566636832413, 0.06929074502784097, 0.08661343128480122, 0.051968058770880725, 0.08661343128480122, 0.01732268625696024, 0.2870892827915022, 0.2870892827915022, 0.27226540417571704, 0.27226540417571704, 0.16133894998774045, 0.1520309336422939, 0.16444162210288932, 0.10859352403020991, 0.09928550768476335, 0.08687481922416794, 0.058950770187828246, 0.10238817979991222, 0.04343740961208397, 0.01861603269089313, 0.1705968051608985, 0.12881799573373967, 0.1705968051608985, 0.10792859102016027, 0.10444702356789703, 0.08703918630658086, 0.05222351178394852, 0.10792859102016027, 0.048741944331685286, 0.020889404713579408, 0.12454587208040736, 0.12454587208040736, 0.12454587208040736, 0.12454587208040736, 0.12454587208040736, 0.12454587208040736, 0.12454587208040736, 0.24909174416081473, 0.21790199175919345, 0.21790199175919345, 0.12451542386239627, 0.031128855965599066, 0.0933865678967972, 0.031128855965599066, 0.031128855965599066, 0.1867731357935944, 0.031128855965599066, 0.15912806331336385, 0.14734079936422578, 0.21217075108448513, 0.0942981115931045, 0.1119790075168116, 0.08840447961853548, 0.07661721566939741, 0.04714905579655225, 0.04714905579655225, 0.011787263949138062, 0.25921416382960943, 0.1379896295013344, 0.15346510196877375, 0.08511509857091654, 0.0838254758652966, 0.06448113528099737, 0.06963962610347717, 0.08253585315967664, 0.049005662813558004, 0.016765095173059318, 0.23608948864510818, 0.17006446215961182, 0.15405839513282482, 0.09603640216072197, 0.0620235097287996, 0.07202730162054148, 0.0780295767555866, 0.0780295767555866, 0.03601365081027074, 0.01800682540513537, 0.18545285318534158, 0.2945427668237778, 0.08727193091074897, 0.05454495681921811, 0.04363596545537449, 0.032726974091530865, 0.1418168877299671, 0.08727193091074897, 0.04363596545537449, 0.010908991363843622, 0.14763804725518587, 0.29527609451037173, 0.14763804725518587, 0.07381902362759293, 0.07381902362759293, 0.14763804725518587, 0.07381902362759293, 0.48337302824524814, 0.24168651412262407, 0.49698176449754256, 0.07696665703471198, 0.15393331406942395, 0.15393331406942395, 0.23089997110413593, 0.15393331406942395, 0.07696665703471198, 0.07696665703471198, 0.07696665703471198, 0.13638079702997305, 0.13638079702997305, 0.13638079702997305, 0.2727615940599461, 0.13638079702997305, 0.13638079702997305, 0.21314169997133525, 0.1653685603225877, 0.15434398963441517, 0.06614742412903508, 0.12127027756989765, 0.06982228102509258, 0.05879771033692007, 0.09554627929749511, 0.04042342585663255, 0.014699427584230018, 0.15353965852221674, 0.1023597723481445, 0.204719544696289, 0.1023597723481445, 0.1023597723481445, 0.05117988617407225, 0.15353965852221674, 0.05117988617407225, 0.05117988617407225, 0.23095070595957568, 0.31755722069441655, 0.09238028238383027, 0.06351144413888331, 0.06351144413888331, 0.09815405003281967, 0.05773767648989392, 0.034642605893936355, 0.02886883824494696, 0.017321302946968178, 0.23982606360842065, 0.23982606360842065, 0.11991303180421033, 0.03997101060140344, 0.07994202120280688, 0.03997101060140344, 0.03997101060140344, 0.15988404240561377, 0.03997101060140344, 0.2705683608564267, 0.2705683608564267, 0.13724492532667878, 0.06862246266333939, 0.13724492532667878, 0.13724492532667878, 0.13724492532667878, 0.06862246266333939, 0.06862246266333939, 0.20586738799001816, 0.06862246266333939, 0.3311458554329679, 0.1103819518109893, 0.1103819518109893, 0.1103819518109893, 0.1103819518109893, 0.1103819518109893, 0.1103819518109893, 0.23063713187611243, 0.205885829918676, 0.11700615470788144, 0.09225485275044498, 0.06750355079300852, 0.08887967521079455, 0.05512789981429029, 0.08550449767114413, 0.04275224883557206, 0.01462576933848518, 0.25488336354952673, 0.1477164947843848, 0.12744168177476337, 0.08978845761403782, 0.08399565389700313, 0.06951364460441638, 0.06661724274589903, 0.10716686876514192, 0.04054962601924289, 0.011585607434069396, 0.35787547149534266, 0.17893773574767133, 0.11183608484229457, 0.06710165090537674, 0.06710165090537674, 0.04473443393691783, 0.06710165090537674, 0.06710165090537674, 0.022367216968458916, 0.12610504572581657, 0.10088403658065326, 0.1513260548709799, 0.07566302743548994, 0.12610504572581657, 0.10088403658065326, 0.12610504572581657, 0.07566302743548994, 0.05044201829032663, 0.025221009145163314, 0.17357727890745392, 0.23669628941925533, 0.09467851576770213, 0.06311901051180142, 0.07889876313975178, 0.12623802102360285, 0.12623802102360285, 0.047339257883851064, 0.047339257883851064, 0.015779752627950356, 0.20951889264561765, 0.4190377852912353, 0.23997434436242504, 0.17025206863550424, 0.14430889627199883, 0.07620806881779714, 0.07945096536323532, 0.09242255154498802, 0.05350779299972991, 0.07782951709051623, 0.05188634472701082, 0.014593034454471793, 0.1922751635683755, 0.2884127453525633, 0.09613758178418776, 0.09613758178418776, 0.09613758178418776, 0.1922751635683755, 0.09613758178418776, 0.09613758178418776, 0.176401134959053, 0.12678831575181934, 0.22050141869881626, 0.11025070934940813, 0.12678831575181934, 0.07166296107711528, 0.049612819207233656, 0.06615042560964488, 0.03307521280482244, 0.01653760640241122, 0.139480144201056, 0.17667484932133762, 0.14645415141110882, 0.11390878443086241, 0.139480144201056, 0.05811672675044, 0.0534673886104048, 0.12088279164091521, 0.030220697910228803, 0.0209220216301584, 0.1626138891283786, 0.17164799407995518, 0.1445456792252254, 0.1445456792252254, 0.11744336437049566, 0.03613641980630635, 0.04517052475788294, 0.13551157427364882, 0.027102314854729765, 0.018068209903153176, 0.12685017756352623, 0.13591090453234952, 0.14497163150117284, 0.12685017756352623, 0.12685017756352623, 0.08154654271940973, 0.12685017756352623, 0.07248581575058642, 0.04530363484411651, 0.009060726968823303, 0.1468053050608934, 0.35233273214614413, 0.11744424404871472, 0.08808318303653603, 0.05872212202435736, 0.05872212202435736, 0.08808318303653603, 0.08808318303653603, 0.02936106101217868, 0.07126871095228521, 0.14253742190457042, 0.14253742190457042, 0.07126871095228521, 0.21380613285685562, 0.07126871095228521, 0.14253742190457042, 0.07126871095228521, 0.07126871095228521, 0.16969403735347305, 0.3676704142658583, 0.08484701867673652, 0.028282339558912174, 0.08484701867673652, 0.1131293582356487, 0.05656467911782435, 0.028282339558912174, 0.028282339558912174, 0.16182172096961353, 0.48546516290884056, 0.09467030778378192, 0.17040655401080745, 0.1514724924540511, 0.246142800237833, 0.1325384308972947, 0.03786812311351277, 0.07573624622702554, 0.03786812311351277, 0.018934061556756386, 0.018934061556756386, 0.18200184229396393, 0.18200184229396393, 0.36400368458792787, 0.18200184229396393, 0.4997975388487005, 0.14870023581225944, 0.17348360844763602, 0.12391686317688287, 0.1982669810830126, 0.0991334905415063, 0.07435011790612972, 0.07435011790612972, 0.04956674527075315, 0.024783372635376576, 0.024783372635376576, 0.08102054037015266, 0.32408216148061064, 0.12153081055522898, 0.08102054037015266, 0.08102054037015266, 0.04051027018507633, 0.16204108074030532, 0.08102054037015266, 0.04051027018507633, 0.04051027018507633, 0.2280534161879196, 0.2280534161879196, 0.2280534161879196, 0.23344848172919286, 0.23344848172919286, 0.23344848172919286, 0.2108540558975333, 0.2319394614872866, 0.15814054192314997, 0.08961297375645165, 0.07907027096157498, 0.047442162576944995, 0.06325621676925999, 0.07116324386541749, 0.031628108384629997, 0.015814054192314998, 0.14110486464481245, 0.3292446841712291, 0.14110486464481245, 0.05879369360200519, 0.047034954881604155, 0.03527621616120311, 0.07055243232240622, 0.11758738720401038, 0.047034954881604155, 0.011758738720401039, 0.28978237867355944, 0.17793654830832598, 0.12201363312570924, 0.07625852070356827, 0.08134242208380615, 0.050839013802378846, 0.07117461932333038, 0.0660907179430925, 0.04575511242214096, 0.01016780276047577, 0.17616488233218852, 0.17616488233218852, 0.17616488233218852, 0.17616488233218852, 0.17616488233218852, 0.17616488233218852, 0.30615611905263146, 0.30615611905263146, 0.19186145489699719, 0.16140725570699763, 0.18272519513999733, 0.09136259756999866, 0.07613549797499888, 0.08831717765099871, 0.07309007805599893, 0.07004465813699898, 0.048726718703999285, 0.015227099594999776, 0.20619737136868088, 0.14220508370253854, 0.12087432114715775, 0.06399228766614234, 0.09954355859177698, 0.1635358462579193, 0.07821279603639619, 0.07110254185126927, 0.04266152511076156, 0.014220508370253853, 0.1690989526687692, 0.11836926686813844, 0.28746821953690765, 0.06763958106750768, 0.11836926686813844, 0.06763958106750768, 0.01690989526687692, 0.05072968580063076, 0.10145937160126152, 0.17864501003806593, 0.17864501003806593, 0.31262876756661534, 0.04466125250951648, 0.06699187876427472, 0.04466125250951648, 0.02233062625475824, 0.06699187876427472, 0.06699187876427472, 0.1661097359631171, 0.17776655953947618, 0.16902394185720687, 0.06994094145815456, 0.10199720629314207, 0.0874261768226932, 0.08451197092860344, 0.06411252966997502, 0.06994094145815456, 0.011656823576359094, 0.2737321665551477, 0.16788906215382393, 0.1423407266086768, 0.07664500663544135, 0.11314262884279438, 0.07664500663544135, 0.04379714664882363, 0.04379714664882363, 0.05109667109029424, 0.014599048882941211, 0.19187207231756806, 0.19187207231756806, 0.19187207231756806, 0.19187207231756806, 0.5321618395807067, 0.26431933799422325, 0.1838743220829379, 0.09423559006750569, 0.09423559006750569, 0.10342930617165258, 0.08504187396335879, 0.06435601272902827, 0.055162296624881375, 0.03907329344262431, 0.016089003182257068, 0.19160171635973064, 0.170312636764205, 0.14708818629635886, 0.11031613972226915, 0.08322094750978198, 0.06773798053121789, 0.08322094750978198, 0.07935020576514097, 0.046448900935692275, 0.0174183378508846, 0.3750717221291907, 0.3750717221291907, 0.24097296765360168, 0.144583780592161, 0.144583780592161, 0.0722918902960805, 0.04819459353072034, 0.0722918902960805, 0.09638918706144067, 0.09638918706144067, 0.09638918706144067, 0.02409729676536017, 0.1768212814281367, 0.11051330089258543, 0.19892394160665378, 0.11051330089258543, 0.08841064071406834, 0.04420532035703417, 0.06630798053555126, 0.08841064071406834, 0.08841064071406834, 0.022102660178517086, 0.46869485208551714, 0.11717371302137929, 0.11717371302137929, 0.11717371302137929, 0.11717371302137929, 0.16848959524748963, 0.12004883661383636, 0.16006511548511515, 0.16006511548511515, 0.08424479762374482, 0.08003255774255758, 0.1031998770890874, 0.05897135833662137, 0.04633463869305965, 0.014742839584155343, 0.38195651986106605, 0.29192395868066495, 0.29192395868066495, 0.29192395868066495, 0.20692130793569447, 0.10346065396784723, 0.20692130793569447, 0.10346065396784723, 0.10346065396784723, 0.15519098095177086, 0.051730326983923616, 0.051730326983923616, 0.15047411019772974, 0.07523705509886487, 0.20063214693030634, 0.12539509183144146, 0.10031607346515317, 0.15047411019772974, 0.025079018366288293, 0.10031607346515317, 0.050158036732576586, 0.025079018366288293, 0.17564952049574734, 0.17564952049574734, 0.35129904099149467, 0.36481153372308694, 0.16640424258380088, 0.16640424258380088, 0.24960636387570131, 0.04160106064595022, 0.10400265161487555, 0.08320212129190044, 0.02080053032297511, 0.06240159096892533, 0.10400265161487555, 0.13841748400613688, 0.13841748400613688, 0.13841748400613688, 0.13841748400613688, 0.13841748400613688, 0.13841748400613688, 0.13841748400613688, 0.3635944263574734, 0.09955669002438042, 0.19911338004876084, 0.19911338004876084, 0.09955669002438042, 0.07466751751828532, 0.04977834501219021, 0.17422420754266574, 0.04977834501219021, 0.04977834501219021, 0.37641456471304025, 0.15828332498565842, 0.15828332498565842, 0.15828332498565842, 0.15828332498565842, 0.1448842476412686, 0.1738610971695223, 0.23181479622602974, 0.06761264889925868, 0.13522529779851736, 0.06761264889925868, 0.038635799371004954, 0.057953699056507435, 0.057953699056507435, 0.009658949842751239, 0.49743848245402167, 0.16581282748467388, 0.16581282748467388, 0.4188711229654884, 0.1396237076551628, 0.08377422459309768, 0.08377422459309768, 0.08377422459309768, 0.02792474153103256, 0.11169896612413024, 0.05584948306206512, 0.02792474153103256, 0.5502787710615408, 0.07861125300879153, 0.07861125300879153, 0.039305626504395764, 0.039305626504395764, 0.07861125300879153, 0.039305626504395764, 0.20424226374357238, 0.20424226374357238, 0.20424226374357238, 0.20424226374357238, 0.2916411370164805, 0.10794320342252703, 0.10794320342252703, 0.2806523288985703, 0.08635456273802163, 0.17270912547604325, 0.10794320342252703, 0.04317728136901081, 0.04317728136901081, 0.021588640684505406, 0.021588640684505406, 0.10888207826948049, 0.10888207826948049, 0.21776415653896097, 0.10888207826948049, 0.10888207826948049, 0.16332311740422073, 0.05444103913474024, 0.05444103913474024, 0.05444103913474024, 0.17423476064171065, 0.15908391189025756, 0.18938560939316373, 0.08711738032085532, 0.13256992657521463, 0.05302797063008585, 0.06439110719367568, 0.056815682817949124, 0.06817881938153895, 0.0151508487514531, 0.21051776722978507, 0.08420710689191403, 0.16841421378382807, 0.08420710689191403, 0.042103553445957016, 0.042103553445957016, 0.042103553445957016, 0.08420710689191403, 0.16841421378382807, 0.042103553445957016, 0.5147652269045893, 0.10013034499771852, 0.10013034499771852, 0.20026068999543703, 0.10013034499771852, 0.10013034499771852, 0.10013034499771852, 0.10013034499771852, 0.10013034499771852, 0.19913263983221274, 0.09956631991610637, 0.19913263983221274, 0.09956631991610637, 0.09956631991610637, 0.09956631991610637, 0.19913263983221274, 0.2458920893940486, 0.1419583196501724, 0.13181843967516008, 0.07604909981259235, 0.11407364971888853, 0.07858406980634543, 0.06844418983133312, 0.07858406980634543, 0.048164429881308486, 0.01520981996251847, 0.20764139776088245, 0.4152827955217649, 0.10382069888044122, 0.10382069888044122, 0.10382069888044122, 0.15670006080888474, 0.41786682882369264, 0.05223335360296158, 0.05223335360296158, 0.05223335360296158, 0.10446670720592316, 0.05223335360296158, 0.05223335360296158, 0.05223335360296158, 0.3490718952155155, 0.2171293100279222, 0.15588770976363647, 0.12805061873441567, 0.10021352770519487, 0.1336180369402598, 0.055674182058441594, 0.08351127308766239, 0.0668090184701299, 0.04453934564675328, 0.016702254617532476, 0.22505746231814844, 0.22505746231814844, 0.22505746231814844, 0.3804971099210294, 0.18246628669164264, 0.25545280136829973, 0.12772640068414987, 0.05473988600749279, 0.03649325733832853, 0.05473988600749279, 0.14597302935331413, 0.07298651467665707, 0.03649325733832853, 0.018246628669164267, 0.2514937990505067, 0.18512737985662298, 0.16853577505815204, 0.07859181220328335, 0.05938048033136963, 0.08033829691891185, 0.062000207404812414, 0.06985938862514074, 0.031436724881313334, 0.01397187772502815, 0.3860285025928365, 0.19301425129641825, 0.19301425129641825, 0.21311501780432685, 0.1398567304340895, 0.1431866525872821, 0.08657797598300777, 0.11654727536174124, 0.08657797598300777, 0.08657797598300777, 0.07325828737023735, 0.03995906583831128, 0.013319688612770428, 0.13102435541161958, 0.13102435541161958, 0.21837392568603264, 0.04367478513720653, 0.13102435541161958, 0.08734957027441306, 0.13102435541161958, 0.04367478513720653, 0.04367478513720653, 0.04367478513720653, 0.22988725649420705, 0.13235932949666465, 0.1764791059955529, 0.07198489849818604, 0.09056164649771792, 0.09985002049748387, 0.06037443099847862, 0.08127327249795198, 0.03947558949900525, 0.018576747999531882, 0.19731298088317184, 0.07892519235326874, 0.19731298088317184, 0.07892519235326874, 0.1183877885299031, 0.15785038470653748, 0.03946259617663437, 0.07892519235326874, 0.03946259617663437, 0.03946259617663437, 0.225587022285004, 0.169190266713753, 0.056396755571251, 0.056396755571251, 0.056396755571251, 0.112793511142502, 0.056396755571251, 0.225587022285004, 0.056396755571251, 0.21389415294561584, 0.17293569812624257, 0.16383381927749296, 0.10012066733624571, 0.06826409136562207, 0.07281503078999688, 0.07736597021437168, 0.07281503078999688, 0.04550939424374805, 0.01820375769749922, 0.2170276496529307, 0.18285006703042195, 0.16576127571916754, 0.0666462861138921, 0.07689956090064475, 0.09569723134302456, 0.059810769589390356, 0.06835516524501754, 0.05126637393376316, 0.01196215391787807, 0.2025163980902416, 0.2025163980902416, 0.1012581990451208, 0.06750546603008054, 0.06750546603008054, 0.06750546603008054, 0.06750546603008054, 0.16876366507520132, 0.03375273301504027, 0.24779263814681637, 0.24779263814681637, 0.12389631907340819, 0.12389631907340819, 0.12389631907340819, 0.12749962930783276, 0.3824988879234983, 0.19124944396174914, 0.06374981465391638, 0.06374981465391638, 0.06374981465391638, 0.06374981465391638, 0.06374981465391638, 0.06374981465391638, 0.4032757365259033, 0.2079018183618551, 0.1902830201955962, 0.12333158716381235, 0.08104647156479097, 0.0951415100977981, 0.09866526973104987, 0.08104647156479097, 0.07047519266503563, 0.035237596332517815, 0.017618798166258907, 0.3765491447275928, 0.1673551754344857, 0.12551638157586428, 0.08367758771724285, 0.12551638157586428, 0.04183879385862142, 0.04183879385862142, 0.04183879385862142, 0.09405670687856019, 0.09405670687856019, 0.09405670687856019, 0.09405670687856019, 0.09405670687856019, 0.09405670687856019, 0.09405670687856019, 0.18811341375712037, 0.239091748462948, 0.239091748462948, 0.239091748462948, 0.3110508096092957, 0.3110508096092957, 0.3110508096092957, 0.21217287884601144, 0.21217287884601144, 0.10608643942300572, 0.10608643942300572, 0.21217287884601144, 0.17681588509255178, 0.08840794254627589, 0.20996886354740524, 0.07735694972799141, 0.11050992818284487, 0.13261191381941384, 0.055254964091422436, 0.07735694972799141, 0.055254964091422436, 0.011050992818284486, 0.27415488505328, 0.27415488505328, 0.13707744252664, 0.13707744252664, 0.20115305702068267, 0.1183253276592251, 0.1183253276592251, 0.07099519659553506, 0.16565545872291515, 0.08282772936145757, 0.10649279489330259, 0.07099519659553506, 0.04733013106369004, 0.01183253276592251, 0.16892884096942326, 0.1372546832876564, 0.14781273584824534, 0.08446442048471163, 0.10558052560588954, 0.12669663072706744, 0.06334831536353372, 0.10558052560588954, 0.042232210242355815, 0.021116105121177908, 0.16295297501691045, 0.3110920432141017, 0.07406953409859565, 0.04444172045915739, 0.04444172045915739, 0.07406953409859565, 0.07406953409859565, 0.16295297501691045, 0.04444172045915739, 0.029627813639438262, 0.1990239272610891, 0.24877990907636138, 0.0746339727229084, 0.04975598181527228, 0.04975598181527228, 0.0746339727229084, 0.0746339727229084, 0.17414593635345296, 0.04975598181527228, 0.02487799090763614, 0.1423262199076705, 0.2068177883033337, 0.1734600805124734, 0.12453544241921169, 0.08895388744229406, 0.082282345884122, 0.06004387402354849, 0.07116310995383525, 0.037805402162974976, 0.013343083116344108, 0.23616663660834913, 0.14533331483590717, 0.1198999847396234, 0.09083332177244198, 0.09083332177244198, 0.07993332315974894, 0.08356665603064661, 0.09809998751423733, 0.043599994450772145, 0.014533331483590716, 0.22082329613037394, 0.18401941344197828, 0.11710326309944072, 0.07026195785966444, 0.10037422551380634, 0.10037422551380634, 0.08364518792817195, 0.07360776537679131, 0.03345807517126878, 0.013383230068507512, 0.17184110654114013, 0.1841154712940787, 0.11046928277644723, 0.12274364752938581, 0.09819491802350865, 0.0736461885176315, 0.08592055327057006, 0.0736461885176315, 0.04909745901175432, 0.02454872950587716, 0.35741683785737816, 0.35741683785737816, 0.20289847380174758, 0.1899475073888701, 0.15972858575882257, 0.10576622570516629, 0.09497375369443505, 0.06475483206438752, 0.04748687684721752, 0.06475483206438752, 0.056120854455802525, 0.012950966412877506, 0.1957005416642594, 0.14464822644749606, 0.1957005416642594, 0.0680697536223511, 0.09643215096499738, 0.09643215096499738, 0.07374223309088035, 0.06523351388808646, 0.045379835748234065, 0.017017438405587774, 0.3034710793272499, 0.20231405288483326, 0.10115702644241663, 0.10115702644241663, 0.10115702644241663, 0.10115702644241663, 0.13455817162614617, 0.13455817162614617, 0.13455817162614617, 0.13455817162614617, 0.13455817162614617, 0.13455817162614617, 0.13455817162614617, 0.5703745351495447, 0.4971613188207491, 0.3267930504086494, 0.3307531728020895, 0.302355812070822, 0.302355812070822, 0.2043136180766623, 0.2313913264964609, 0.16738947023148237, 0.06954047844175554, 0.06215564887271956, 0.08554094250800018, 0.061540246408633224, 0.053540014375510904, 0.05230920944733824, 0.01353885420989931, 0.26098584058249935, 0.19506673651784745, 0.1506722378620615, 0.059192664874381296, 0.08071727028324722, 0.059192664874381296, 0.055156801360218936, 0.08071727028324722, 0.04977565000800245, 0.009417014866378843, 0.3753045686455159, 0.15012182745820635, 0.07506091372910317, 0.07506091372910317, 0.07506091372910317, 0.07506091372910317, 0.07506091372910317, 0.07506091372910317, 0.33330459121688416, 0.1012238575880997, 0.1012238575880997, 0.2024477151761994, 0.1012238575880997, 0.1012238575880997, 0.1012238575880997, 0.1012238575880997, 0.1012238575880997, 0.1292187666670696, 0.3876563000012088, 0.1292187666670696, 0.1292187666670696, 0.1292187666670696, 0.18957301285686914, 0.10531834047603841, 0.18957301285686914, 0.10531834047603841, 0.08425467238083073, 0.1263820085712461, 0.06319100428562305, 0.06319100428562305, 0.042127336190415365, 0.021063668095207683, 0.17396475625875518, 0.1304735671940664, 0.108727972661722, 0.217455945323444, 0.108727972661722, 0.08698237812937759, 0.043491189064688796, 0.0652367835970332, 0.043491189064688796, 0.021745594532344398, 0.16814733444852545, 0.25222100167278816, 0.08407366722426272, 0.04203683361213136, 0.04203683361213136, 0.08407366722426272, 0.04203683361213136, 0.16814733444852545, 0.04203683361213136, 0.04203683361213136, 0.2225341241697791, 0.2225341241697791, 0.2225341241697791, 0.2225341241697791, 0.2225341241697791, 0.11195151849130851, 0.26122020981305316, 0.11195151849130851, 0.07463434566087233, 0.07463434566087233, 0.07463434566087233, 0.14926869132174467, 0.07463434566087233, 0.03731717283043617, 0.03731717283043617, 0.1269429987885893, 0.2538859975771786, 0.1269429987885893, 0.1269429987885893, 0.1269429987885893, 0.1269429987885893, 0.1269429987885893, 0.1269429987885893, 0.19953518352824054, 0.2993027752923608, 0.09976759176412027, 0.09976759176412027, 0.09976759176412027, 0.09976759176412027, 0.19953518352824054, 0.14790788107221906, 0.39442101619258413, 0.07395394053610953, 0.049302627024073016, 0.049302627024073016, 0.09860525404814603, 0.049302627024073016, 0.07395394053610953, 0.024651313512036508, 0.024651313512036508, 0.523620606938505, 0.237898509389873, 0.237898509389873, 0.237898509389873, 0.237898509389873, 0.111128863277076, 0.333386589831228, 0.111128863277076, 0.111128863277076, 0.111128863277076, 0.111128863277076, 0.38012514926416013, 0.17529277404762056, 0.17529277404762056, 0.17529277404762056, 0.17529277404762056, 0.17529277404762056, 0.17529277404762056, 0.22049387127097844, 0.16874530964615697, 0.13499624771692556, 0.07874781116820659, 0.10124718578769418, 0.05849837401066775, 0.04949862416287271, 0.10349712324964294, 0.06524818639651403, 0.017999499695590077, 0.1905892938015536, 0.16740951482568894, 0.13907867385518774, 0.06696380593027558, 0.10044570889541338, 0.07726592991954875, 0.0952946469007768, 0.08756805390882191, 0.05666168194100241, 0.01802871698122804, 0.2578025302982025, 0.16848511822638432, 0.14615576520842977, 0.10961682390632232, 0.08119764733801654, 0.06698805905386365, 0.048718588402809924, 0.07104794142076447, 0.03653894130210744, 0.014209588284152894, 0.2590154511881199, 0.10665342107746113, 0.12950772559405996, 0.1599801316161917, 0.06856291354979645, 0.03809050752766469, 0.0990353195719282, 0.07618101505532938, 0.03809050752766469, 0.022854304516598815, 0.2774491755187074, 0.1566771814693877, 0.09792323841836731, 0.10445145431292513, 0.0946591304710884, 0.06854626689285712, 0.06854626689285712, 0.07507448278741494, 0.039169295367346924, 0.019584647683673462, 0.2329668422329079, 0.14976439857829793, 0.13312390984737593, 0.15531122815527193, 0.12757708027040193, 0.055468295769739974, 0.04992146619276598, 0.055468295769739974, 0.02218731830789599, 0.01664048873092199, 0.19571368242211762, 0.16076481056102518, 0.11183638995549577, 0.16775458493324366, 0.11183638995549577, 0.048928420605529405, 0.05591819497774789, 0.10484661558327729, 0.03494887186109243, 0.013979548744436972, 0.19273713522211383, 0.2409214190276423, 0.1445528514165854, 0.07629178269208672, 0.0843224966596748, 0.08030713967588077, 0.0722764257082927, 0.060230354756910576, 0.03613821285414635, 0.012046070951382114, 0.21545776663719382, 0.2872770221829251, 0.07181925554573128, 0.07181925554573128, 0.14363851109146256, 0.07181925554573128, 0.07181925554573128, 0.20911092435346706, 0.16131414164410315, 0.18222523407944988, 0.10455546217673353, 0.09260626649939255, 0.06572057622537536, 0.059745978386704875, 0.06572057622537536, 0.04182218487069341, 0.014936494596676219, 0.13679131241473133, 0.17953859754433485, 0.16243968349249344, 0.09404402728512778, 0.09404402728512778, 0.1196923983628899, 0.04274728512960354, 0.09404402728512778, 0.051296742155524246, 0.017098914051841416, 0.2466888435570053, 0.2466888435570053, 0.2466888435570053, 0.13739300164815538, 0.12365370148333985, 0.23356810280186416, 0.09617510115370877, 0.13739300164815538, 0.10991440131852431, 0.06182685074166992, 0.04808755057685438, 0.034348250412038844, 0.013739300164815539, 0.17823675912270034, 0.17823675912270034, 0.3564735182454007, 0.17823675912270034, 0.1678254964093348, 0.15711323068107938, 0.19282078310859743, 0.10355190203980233, 0.0892688810687951, 0.0821273705832915, 0.07498586009778789, 0.06784434961228428, 0.053561328641277064, 0.014283020971007216, 0.49861077041059465, 0.06306915738546058, 0.2522766295418423, 0.12613831477092116, 0.06306915738546058, 0.06306915738546058, 0.06306915738546058, 0.18920747215638173, 0.06306915738546058, 0.06306915738546058, 0.23187084103131111, 0.23187084103131111, 0.2696958149356983, 0.14983100829761017, 0.14302050792044607, 0.09534700528029738, 0.08853650490313328, 0.054484003017312786, 0.07900180437510354, 0.06538080362077535, 0.03813880211211895, 0.014983100829761016, 0.08255975004428101, 0.16511950008856202, 0.16511950008856202, 0.24767925013284303, 0.08255975004428101, 0.08255975004428101, 0.08255975004428101, 0.2825542065821205, 0.2825542065821205, 0.2825542065821205, 0.17030980492549225, 0.13862425982307508, 0.15050633923648152, 0.08317455589384505, 0.15446703237428366, 0.10297802158285578, 0.06337109020483432, 0.05544970392923003, 0.07129247648043861, 0.011882079413406436, 0.13961944221154673, 0.13961944221154673, 0.27923888442309347, 0.06980972110577337, 0.06980972110577337, 0.06980972110577337, 0.06980972110577337, 0.06980972110577337, 0.3982334861667355, 0.13274449538891184, 0.08849633025927456, 0.08849633025927456, 0.13274449538891184, 0.04424816512963728, 0.08849633025927456, 0.04424816512963728, 0.23319251023793378, 0.23319251023793378, 0.1942232460239109, 0.04855581150597772, 0.1942232460239109, 0.04855581150597772, 0.14566743451793315, 0.04855581150597772, 0.1942232460239109, 0.09711162301195544, 0.04855581150597772, 0.13526635497802353, 0.13526635497802353, 0.13526635497802353, 0.27053270995604706, 0.13526635497802353, 0.13526635497802353, 0.21538543940400598, 0.18476692105735806, 0.16048326857553386, 0.07496257940041384, 0.08340906722017878, 0.08657650015259064, 0.06862771353559013, 0.06546028060317828, 0.04328825007629532, 0.017948786617000497, 0.2155564996727355, 0.2155564996727355, 0.09238135700260093, 0.06158757133506729, 0.06158757133506729, 0.06158757133506729, 0.06158757133506729, 0.1539689283376682, 0.06158757133506729, 0.030793785667533644, 0.1911026594278076, 0.14613732779773522, 0.12646499520957855, 0.1320856616633376, 0.1067926626214219, 0.06463766421822904, 0.07587899712574714, 0.10398232939454237, 0.03934466517631333, 0.014051666134397617, 0.1883872308723579, 0.15284247033040357, 0.15284247033040357, 0.10307980557166753, 0.11018875768005838, 0.0781984731922995, 0.09597085346327666, 0.06398056897551778, 0.042653712650345184, 0.010663428162586296, 0.16978081997617128, 0.11318721331744751, 0.11318721331744751, 0.22637442663489502, 0.11318721331744751, 0.056593606658723754, 0.11318721331744751, 0.056593606658723754, 0.056593606658723754, 0.23642310226734237, 0.13907241309843668, 0.13907241309843668, 0.10430430982382752, 0.083443447859062, 0.11821155113367118, 0.06953620654921834, 0.055628965239374675, 0.03476810327460917, 0.013907241309843669, 0.12127190386578854, 0.12127190386578854, 0.2829677756868399, 0.08084793591052569, 0.12127190386578854, 0.08084793591052569, 0.08084793591052569, 0.12127190386578854, 0.040423967955262846, 0.040423967955262846, 0.3860782706852903, 0.220547284996291, 0.14420399403603643, 0.16399669909980613, 0.0904809374343758, 0.08765340813955155, 0.07634329096025458, 0.09330846672920004, 0.06786070307578185, 0.04806799801201214, 0.011310117179296975, 0.3923674490216668, 0.2701713862938549, 0.0900571287646183, 0.0900571287646183, 0.0900571287646183, 0.0900571287646183, 0.1801142575292366, 0.0900571287646183, 0.0900571287646183, 0.0900571287646183, 0.10147382891184589, 0.20294765782369178, 0.10147382891184589, 0.10147382891184589, 0.20294765782369178, 0.10147382891184589, 0.10147382891184589, 0.1797393485208139, 0.21938773422393465, 0.14537741424477596, 0.0713670942656173, 0.10044257711457248, 0.08193999711978282, 0.07401031997915868, 0.0713670942656173, 0.04493483713020348, 0.013216128567706906, 0.14904809317499618, 0.17388944203749554, 0.21115146533124457, 0.08073438380312292, 0.06831370937187324, 0.09936539544999745, 0.06210337215624841, 0.06831370937187324, 0.07452404658749809, 0.018631011646874522, 0.16392335230043695, 0.14206690532704536, 0.20763624624722013, 0.09835401138026216, 0.07649756440687057, 0.08742578789356636, 0.07649756440687057, 0.06556934092017477, 0.07649756440687057, 0.010928223486695796, 0.18266470003611981, 0.1522205833634332, 0.1522205833634332, 0.10655440835440323, 0.12177646669074656, 0.1522205833634332, 0.045666175009029954, 0.06088823334537328, 0.01522205833634332, 0.12290261561859947, 0.1843539234278992, 0.15362826952324934, 0.061451307809299735, 0.0921769617139496, 0.12290261561859947, 0.030725653904649868, 0.0921769617139496, 0.0921769617139496, 0.17541777667037922, 0.2862079514095661, 0.1384877184239836, 0.07386011649279126, 0.06462760193119235, 0.07386011649279126, 0.07386011649279126, 0.04616257280799453, 0.03693005824639563, 0.018465029123197814, 0.2730104368581385, 0.10920417474325542, 0.16380626211488314, 0.21840834948651083, 0.05460208737162771, 0.05460208737162771, 0.10920417474325542, 0.05460208737162771, 0.10797189827688465, 0.37790164396909626, 0.16195784741532698, 0.053985949138442325, 0.10797189827688465, 0.053985949138442325, 0.053985949138442325, 0.053985949138442325, 0.053985949138442325, 0.2323126956157756, 0.1700433132857739, 0.17962321825962033, 0.11735383592961861, 0.07663923979077134, 0.06945431106038652, 0.0574794298430785, 0.0550844535996169, 0.031134691165000856, 0.011974881217308022, 0.49655292874106377, 0.08611514273427856, 0.12917271410141784, 0.1578777616795107, 0.12917271410141784, 0.20093533304664998, 0.05741009515618571, 0.14352523789046429, 0.04305757136713928, 0.028705047578092856, 0.014352523789046428, 0.11773556033897396, 0.11773556033897396, 0.11773556033897396, 0.23547112067794793, 0.11773556033897396, 0.11773556033897396, 0.11773556033897396, 0.11773556033897396, 0.2038971882068684, 0.06205566597600342, 0.1329764270914359, 0.10638114167314872, 0.12411133195200684, 0.11524623681257778, 0.07978585625486154, 0.07978585625486154, 0.07978585625486154, 0.00886509513942906, 0.07898127940095169, 0.39490639700475844, 0.07898127940095169, 0.07898127940095169, 0.07898127940095169, 0.07898127940095169, 0.07898127940095169, 0.07898127940095169, 0.07898127940095169, 0.43169813145570696, 0.10792453286392674, 0.10792453286392674, 0.10792453286392674, 0.10792453286392674, 0.4021724259964649, 0.14076034909876273, 0.0904887958492046, 0.07038017454938136, 0.060325863899469735, 0.07038017454938136, 0.08043448519929298, 0.050271553249558114, 0.030162931949734868, 0.010054310649911623, 0.49974445324068, 0.17310572054927642, 0.192339689499196, 0.07693587579967841, 0.0577019068497588, 0.192339689499196, 0.07693587579967841, 0.07693587579967841, 0.07693587579967841, 0.0577019068497588, 0.019233968949919602, 0.30723781136051675, 0.1763772620773337, 0.14792931658098957, 0.07965424738976361, 0.13086054928318308, 0.022758356397075317, 0.05120630189341946, 0.03413753459561297, 0.045516712794150635, 0.011379178198537659, 0.23577089382153482, 0.263508646035833, 0.16180355458340626, 0.05547550442859643, 0.0647214218333625, 0.050852545726213394, 0.05547550442859643, 0.0647214218333625, 0.04160662832144732, 0.00924591740476607, 0.19264986035403464, 0.19264986035403464, 0.19264986035403464, 0.19264986035403464, 0.23173921726434243, 0.23173921726434243, 0.23173921726434243, 0.23173921726434243, 0.24357856494163696, 0.2214350590378518, 0.06643051771135554, 0.06643051771135554, 0.04428701180757036, 0.06643051771135554, 0.06643051771135554, 0.15500454132649624, 0.04428701180757036, 0.02214350590378518, 0.24394465043964164, 0.23189800103521488, 0.12950148109758752, 0.06625657172434711, 0.07529155877766718, 0.07529155877766718, 0.06625657172434711, 0.06324490937324043, 0.033128285862173555, 0.012046649404426747, 0.24049199781947128, 0.1706717403880119, 0.13705457940249438, 0.09050774111485478, 0.08533587019400594, 0.05947651558976171, 0.08274993473358151, 0.07499212835230824, 0.041374967366790756, 0.015515612762546534, 0.1696389211658945, 0.1915278142195583, 0.15322225137564666, 0.08390742337237793, 0.11491668853173498, 0.07843520010896197, 0.08755557221465522, 0.06566667916099142, 0.04012963726505031, 0.014592595369109205, 0.21997047791665117, 0.13696275040093375, 0.14941390952829137, 0.08300772751571742, 0.13281236402514787, 0.07885734113993155, 0.0705565683883598, 0.0705565683883598, 0.05395502288521632, 0.012451159127357612, 0.2319428725184597, 0.13744762815908723, 0.11167619787925837, 0.08590476759942951, 0.09449524435937247, 0.12026667463920132, 0.08590476759942951, 0.07731429083948657, 0.042952383799714756, 0.017180953519885904, 0.27494278904137975, 0.20895651967144863, 0.10631121176266684, 0.07331807707770127, 0.08798169249324152, 0.06598626936993114, 0.062320365516046076, 0.06598626936993114, 0.047656750100505824, 0.014663615415540255, 0.17530380200193318, 0.08765190100096659, 0.08765190100096659, 0.08765190100096659, 0.2629557030028998, 0.08765190100096659, 0.08765190100096659, 0.08765190100096659, 0.08765190100096659, 0.15684012356862242, 0.08554915831015769, 0.1853565096720083, 0.11406554441354358, 0.17109831662031538, 0.09980735136185064, 0.07129096525846473, 0.08554915831015769, 0.042774579155078846, 0.014258193051692947, 0.19035130865993866, 0.19035130865993866, 0.19035130865993866, 0.19035130865993866, 0.19035130865993866, 0.18285430418005946, 0.18285430418005946, 0.18285430418005946, 0.18285430418005946, 0.18285430418005946, 0.17758715199663447, 0.17758715199663447, 0.17758715199663447, 0.17758715199663447, 0.17758715199663447, 0.2085904382027214, 0.2085904382027214, 0.2085904382027214, 0.2085904382027214, 0.19005960070269093, 0.06335320023423031, 0.19005960070269093, 0.06335320023423031, 0.06335320023423031, 0.06335320023423031, 0.19005960070269093, 0.12670640046846063, 0.06335320023423031, 0.21478054330192314, 0.21478054330192314, 0.21478054330192314, 0.21478054330192314, 0.21478054330192314, 0.20777159754237673, 0.13851439836158447, 0.22162303737853517, 0.06925719918079223, 0.05540575934463379, 0.08310863901695069, 0.05540575934463379, 0.06925719918079223, 0.08310863901695069, 0.013851439836158448, 0.10587904563504441, 0.10587904563504441, 0.10587904563504441, 0.10587904563504441, 0.21175809127008882, 0.10587904563504441, 0.10587904563504441, 0.2816101008734053, 0.21820664062120515, 0.2082881569566049, 0.16861422229820397, 0.09174597389755217, 0.07438862748450176, 0.0719090065683517, 0.0595109019876014, 0.05455166015530129, 0.03967393465840093, 0.01487772549690035, 0.16997140349599027, 0.09786232322496409, 0.23177918658544128, 0.08756102604338893, 0.12361556617890201, 0.10816362040653926, 0.07725972886181376, 0.05665713449866342, 0.02575324295393792, 0.015451945772362752, 0.28955498355371223, 0.28955498355371223, 0.28955498355371223, 0.3281283660302192, 0.1619334793395887, 0.09801236907396159, 0.09375096172291977, 0.07670533231875254, 0.06392111026562712, 0.05965970291458531, 0.06818251761666894, 0.03409125880833447, 0.012784222053125424, 0.28523720259357976, 0.28523720259357976, 0.28523720259357976, 0.3169400554900533, 0.15847002774502664, 0.15847002774502664, 0.15847002774502664, 0.15847002774502664, 0.15847002774502664, 0.07655794553044454, 0.33175109729859303, 0.10207726070725939, 0.07655794553044454, 0.10207726070725939, 0.051038630353629696, 0.15311589106088908, 0.07655794553044454, 0.025519315176814848, 0.025519315176814848, 0.24931089804866716, 0.12465544902433358, 0.12465544902433358, 0.12465544902433358, 0.12465544902433358, 0.12465544902433358, 0.12465544902433358, 0.08697639228733588, 0.08697639228733588, 0.17395278457467175, 0.2609291768620076, 0.17395278457467175, 0.08697639228733588, 0.08697639228733588, 0.08697639228733588, 0.08697639228733588, 0.24925061085709405, 0.0726980948333191, 0.19732340026186612, 0.12462530542854702, 0.1557816317856838, 0.031156326357136756, 0.09346897907141027, 0.031156326357136756, 0.02077088423809117, 0.010385442119045586, 0.1825702691604677, 0.12171351277364513, 0.1825702691604677, 0.060856756386822565, 0.060856756386822565, 0.030428378193411282, 0.060856756386822565, 0.12171351277364513, 0.212998647353879, 0.030428378193411282, 0.19960925121483877, 0.09980462560741939, 0.2744627204204033, 0.04990231280370969, 0.07485346920556454, 0.09980462560741939, 0.07485346920556454, 0.024951156401854847, 0.04990231280370969, 0.2765239463938612, 0.2765239463938612, 0.2765239463938612, 0.2765239463938612, 0.20081876341289293, 0.14057313438902505, 0.170695948900959, 0.09538891262112413, 0.09538891262112413, 0.0853479744504795, 0.05020469085322323, 0.0853479744504795, 0.05522515993854555, 0.02008187634128929, 0.37115924408232737, 0.16014186214211362, 0.16014186214211362, 0.18015959490987782, 0.08007093107105681, 0.08007093107105681, 0.06005319830329261, 0.08007093107105681, 0.06005319830329261, 0.12010639660658522, 0.020017732767764203, 0.17910325755901346, 0.11940217170600899, 0.08955162877950673, 0.059701085853004494, 0.2686548863385202, 0.059701085853004494, 0.08955162877950673, 0.059701085853004494, 0.08955162877950673, 0.4340752637942859, 0.1446917545980953, 0.07234587729904765, 0.07234587729904765, 0.07234587729904765, 0.07234587729904765, 0.24162460821939843, 0.13010555827198378, 0.1486920665965529, 0.055759524973707335, 0.09293254162284556, 0.037173016649138226, 0.18586508324569112, 0.07434603329827645, 0.037173016649138226, 0.018586508324569113, 0.2407681260935843, 0.16948808876324684, 0.1599840837858685, 0.07444803898946356, 0.1013760530920355, 0.05068802654601775, 0.07286403815990052, 0.0696960365007744, 0.04118402156863942, 0.019008009954756655, 0.23351752684561783, 0.15567835123041188, 0.15567835123041188, 0.07783917561520594, 0.07783917561520594, 0.15567835123041188, 0.07783917561520594, 0.07783917561520594, 0.07783917561520594, 0.1938968731067763, 0.24447866609115276, 0.10959388479948227, 0.05901209181510584, 0.08430298830729405, 0.07587268947656464, 0.05058179298437643, 0.14331508012239988, 0.03372119532291762, 0.008430298830729405, 0.32008213240099215, 0.16004106620049607, 0.08002053310024804, 0.08002053310024804, 0.08002053310024804, 0.08002053310024804, 0.08002053310024804, 0.15465035538578698, 0.15465035538578698, 0.15465035538578698, 0.23197553307868046, 0.07732517769289349, 0.07732517769289349, 0.07732517769289349, 0.07732517769289349, 0.08578105628510492, 0.1286715844276574, 0.1286715844276574, 0.08578105628510492, 0.21445264071276232, 0.08578105628510492, 0.1286715844276574, 0.1286715844276574, 0.04289052814255246, 0.1998462248914432, 0.3996924497828864, 0.0999231124457216, 0.0999231124457216, 0.0999231124457216, 0.3883813386844072, 0.17266570744288975, 0.08633285372144488, 0.1079160671518061, 0.06474964029108367, 0.2158321343036122, 0.08633285372144488, 0.1079160671518061, 0.06474964029108367, 0.08633285372144488, 0.19662215553018947, 0.09831107776509473, 0.09831107776509473, 0.04915553888254737, 0.24577769441273684, 0.04915553888254737, 0.09831107776509473, 0.04915553888254737, 0.04915553888254737, 0.2422751408649755, 0.1453650845189853, 0.1453650845189853, 0.0969100563459902, 0.0969100563459902, 0.0484550281729951, 0.07268254225949265, 0.07268254225949265, 0.0484550281729951, 0.02422751408649755, 0.17968294183835937, 0.11978862789223958, 0.05989431394611979, 0.05989431394611979, 0.29947156973059896, 0.05989431394611979, 0.11978862789223958, 0.05989431394611979, 0.11978862789223958, 0.149035534956937, 0.149035534956937, 0.149035534956937, 0.24839255826156165, 0.09935702330462466, 0.04967851165231233, 0.04967851165231233, 0.04967851165231233, 0.3298569804242881, 0.138289568161667, 0.1659474817940004, 0.138289568161667, 0.0829737408970002, 0.1106316545293336, 0.1106316545293336, 0.0553158272646668, 0.0553158272646668, 0.138289568161667, 0.1497999177309381, 0.29191778839875115, 0.0998666118206254, 0.06913842356812527, 0.11523070594687546, 0.06529740003656276, 0.08834354122593785, 0.06913842356812527, 0.03841023531562515, 0.011523070594687546, 0.36224156115419853, 0.17387594935401532, 0.08693797467700766, 0.07244831223083971, 0.07244831223083971, 0.04346898733850383, 0.1014276371231756, 0.04346898733850383, 0.028979324892335883, 0.014489662446167942, 0.17344748397443996, 0.21869465370690255, 0.10557672937574605, 0.10180613189804084, 0.10180613189804084, 0.09426493694263041, 0.10557672937574605, 0.04901776721016781, 0.033935377299346946, 0.011311792433115649, 0.18829880639699714, 0.24560713877869192, 0.11052321245041136, 0.08596249857254216, 0.10233630782445496, 0.08596249857254216, 0.08596249857254216, 0.049121427755738384, 0.03684107081680379, 0.012280356938934596, 0.1142721094223469, 0.17140816413352036, 0.17140816413352036, 0.08570408206676018, 0.19997619148910709, 0.08570408206676018, 0.028568027355586726, 0.05713605471117345, 0.028568027355586726, 0.028568027355586726, 0.2727874643836009, 0.2727874643836009, 0.2727874643836009, 0.32560079220967836, 0.15423195420458446, 0.08568441900254693, 0.06854753520203755, 0.17136883800509387, 0.03427376760101877, 0.10282130280305632, 0.03427376760101877, 0.017136883800509387, 0.017136883800509387, 0.20933860621805989, 0.09515391191729994, 0.24740017098497985, 0.05709234715037997, 0.07612312953383996, 0.05709234715037997, 0.07612312953383996, 0.09515391191729994, 0.03806156476691998, 0.01903078238345999, 0.15601686597784514, 0.22535769530133187, 0.12712485375972568, 0.08089763421073452, 0.13868165864697346, 0.08089763421073452, 0.08089763421073452, 0.057784024436238944, 0.04622721954899115, 0.011556804887247788, 0.3675832318524128, 0.1335461952252329, 0.14838466136136988, 0.14838466136136988, 0.07419233068068494, 0.08903079681682192, 0.08903079681682192, 0.04451539840841096, 0.20773852590591782, 0.04451539840841096, 0.029676932272273974, 0.21998905192327628, 0.13644890562329795, 0.18657299340328493, 0.10303284710330661, 0.08910948938664355, 0.08075547475664571, 0.047339416236654386, 0.08354014629997833, 0.036200730063323944, 0.013923357716663054, 0.2053710638107304, 0.19004486501888485, 0.15632722767682464, 0.1164791108180262, 0.1164791108180262, 0.05210907589227488, 0.05210907589227488, 0.0582395554090131, 0.03984811685879844, 0.015326198791845552, 0.3817381877948061, 0.10906805365565889, 0.1363350670695736, 0.10906805365565889, 0.05453402682782944, 0.05453402682782944, 0.08180104024174417, 0.05453402682782944, 0.02726701341391472, 0.02726701341391472, 0.20640839682470463, 0.10320419841235232, 0.20640839682470463, 0.10320419841235232, 0.10320419841235232, 0.10320419841235232, 0.10320419841235232, 0.13849128831810523, 0.055396515327242095, 0.11079303065448419, 0.11079303065448419, 0.13849128831810523, 0.11079303065448419, 0.055396515327242095, 0.19388780364534733, 0.027698257663621047, 0.027698257663621047, 0.1919713102704248, 0.1899071026331084, 0.13314139260690752, 0.11456352387105996, 0.10733879714045257, 0.06811885203144105, 0.06915095585009925, 0.07327937112473204, 0.04025204892766972, 0.01341734964255657, 0.4836760345731609, 0.2903242016315757, 0.12151222538371927, 0.12151222538371927, 0.24302445076743853, 0.12151222538371927, 0.12151222538371927, 0.12151222538371927, 0.12151222538371927, 0.12151222538371927, 0.15592197561764204, 0.2338829634264631, 0.07796098780882102, 0.15592197561764204, 0.07796098780882102, 0.15592197561764204, 0.07796098780882102, 0.07796098780882102, 0.3693358698172141, 0.3693358698172141, 0.18131314928245137, 0.11824770605377265, 0.18919632968603622, 0.18919632968603622, 0.07883180403584843, 0.07094862363226359, 0.06306544322867874, 0.06306544322867874, 0.023649541210754528, 0.015766360807169685, 0.19017987001689632, 0.19460265769170787, 0.1459519932687809, 0.08845575349623085, 0.10172411652066547, 0.061919027447361594, 0.11056969187028856, 0.04865066442292697, 0.039805089073303886, 0.013268363024434628, 0.25414102577052566, 0.14210035849534766, 0.1502984561008485, 0.07924827685317466, 0.10110987046784353, 0.06831748004584023, 0.06831748004584023, 0.07924827685317466, 0.04645588643117136, 0.013663496009168045, 0.14116378651658035, 0.18527746980301169, 0.20733431144622738, 0.10587283988743526, 0.06617052492964703, 0.07058189325829017, 0.07940462991557644, 0.0749932615869333, 0.04411368328643136, 0.017645473314572543, 0.1698959052991722, 0.1698959052991722, 0.13591672423933776, 0.10193754317950332, 0.13106255551650428, 0.09222920573383633, 0.05339585595116841, 0.08252086828816936, 0.038833349782667934, 0.014562506168500475, 0.17700346354537352, 0.23928245997800493, 0.15078072820531818, 0.08522388985517984, 0.08194604793767292, 0.059001154515124506, 0.07866820602016601, 0.06555683835013834, 0.045889786845096835, 0.013111367670027668, 0.14211052216850542, 0.19895473103590758, 0.13737350476288857, 0.12789946995165488, 0.09947736551795379, 0.1326364873572717, 0.056844208867402164, 0.047370174056168474, 0.042633156650551625, 0.014211052216850541, 0.06718210251371914, 0.1343642050274383, 0.20154630754115743, 0.20154630754115743, 0.1343642050274383, 0.06718210251371914, 0.06718210251371914, 0.06718210251371914, 0.06718210251371914, 0.11667164374878905, 0.4666865749951562, 0.11667164374878905, 0.11667164374878905, 0.20823114296288583, 0.41646228592577167, 0.20823114296288583, 0.12308633327276217, 0.43080216645466757, 0.06154316663638108, 0.06154316663638108, 0.06154316663638108, 0.12308633327276217, 0.06154316663638108, 0.06154316663638108, 0.06154316663638108, 0.177760787909582, 0.355521575819164, 0.088880393954791, 0.088880393954791, 0.088880393954791, 0.088880393954791, 0.0444401969773955, 0.0444401969773955, 0.0444401969773955, 0.3580212530855756, 0.35703081232572487, 0.16818806861625057, 0.13573072204118466, 0.06491469315013179, 0.0781926985672042, 0.051636687733059385, 0.05458735560351992, 0.04721068592736858, 0.026556010834144828, 0.01475333935230268, 0.3806489652044691, 0.18286077740214693, 0.11195557800131445, 0.05970964160070104, 0.05224593640061341, 0.05970964160070104, 0.06344149420074485, 0.04478223120052578, 0.02985482080035052, 0.01492741040017526, 0.214946725975902, 0.1612100444819265, 0.107473362987951, 0.0537366814939755, 0.107473362987951, 0.0537366814939755, 0.0537366814939755, 0.214946725975902, 0.0537366814939755, 0.15715712205026694, 0.13548027762954046, 0.21134923310208312, 0.10838422210363237, 0.09212658878808751, 0.10838422210363237, 0.07044974436736104, 0.0596113221569978, 0.03793447773627133, 0.010838422210363236, 0.3687582360427179, 0.33346476443064715, 0.09843018153754982, 0.1476452723063247, 0.39372072615019926, 0.09843018153754982, 0.04921509076877491, 0.04921509076877491, 0.04921509076877491, 0.04921509076877491, 0.04921509076877491, 0.15946645872163875, 0.15946645872163875, 0.15946645872163875, 0.20840253148964166, 0.10420126574482083, 0.10420126574482083, 0.10420126574482083, 0.20840253148964166, 0.10420126574482083, 0.10420126574482083, 0.10420126574482083, 0.10420126574482083, 0.216094462114296, 0.162070846585722, 0.14935940763782224, 0.0858022128983234, 0.09533579210924824, 0.054023615528574, 0.07309077395042364, 0.10169151158319811, 0.05084575579159906, 0.012711438947899764, 0.2460039341394985, 0.10129573758685233, 0.08682491793158771, 0.20259147517370465, 0.1591790162079108, 0.028941639310529236, 0.05788327862105847, 0.08682491793158771, 0.028941639310529236, 0.014470819655264618, 0.20297252318932268, 0.06765750772977422, 0.13531501545954844, 0.06765750772977422, 0.2706300309190969, 0.06765750772977422, 0.06765750772977422, 0.06765750772977422, 0.06765750772977422, 0.17709692589725431, 0.14167754071780345, 0.17709692589725431, 0.10625815553835258, 0.10625815553835258, 0.14167754071780345, 0.07083877035890172, 0.07083877035890172, 0.03541938517945086, 0.03541938517945086, 0.32526392424779127, 0.16958560085741126, 0.19502344098602295, 0.12718920064305844, 0.08479280042870563, 0.11023064055731731, 0.11870992060018788, 0.06783424034296451, 0.07631352038583507, 0.042396400214352814, 0.008479280042870563, 0.2951105724740004, 0.1475552862370002, 0.1475552862370002, 0.1475552862370002, 0.1475552862370002, 0.1475552862370002, 0.1475552862370002, 0.2557695267888388, 0.3268166175635162, 0.05683767261974195, 0.05683767261974195, 0.08525650892961292, 0.0994659270845484, 0.04262825446480646, 0.04262825446480646, 0.04262825446480646, 0.15706002654822568, 0.11885623630676538, 0.12310110188914987, 0.10612163955961194, 0.12310110188914987, 0.15281516096584122, 0.09338704281245852, 0.08489731164768956, 0.025469193494306867, 0.01697946232953791, 0.1636893573959395, 0.1364077978299496, 0.1364077978299496, 0.08184467869796976, 0.1364077978299496, 0.1364077978299496, 0.05456311913197984, 0.10912623826395967, 0.02728155956598992, 0.1970128594871599, 0.14072347106225708, 0.14072347106225708, 0.1688681652747085, 0.08443408263735425, 0.028144694212451415, 0.08443408263735425, 0.11257877684980566, 0.05628938842490283, 0.028144694212451415, 0.18835887869685122, 0.19236651441380548, 0.1322519786594913, 0.08015271433908562, 0.14026725009339983, 0.06812980718822279, 0.06812980718822279, 0.06812980718822279, 0.05209926432040565, 0.016030542867817126, 0.40329443757822603, 0.15816811489340302, 0.20849433326857672, 0.1294102758218752, 0.09346297698246542, 0.10784189651822933, 0.11503135628611129, 0.07189459767881956, 0.0647051379109376, 0.028757839071527822, 0.014378919535763911, 0.28348982350979346, 0.28348982350979346, 0.28348982350979346, 0.2828347536090979, 0.2828347536090979, 0.2828347536090979, 0.1943696415412311, 0.09718482077061555, 0.2915544623118467, 0.09718482077061555, 0.09718482077061555, 0.09718482077061555, 0.09718482077061555, 0.18438785428045568, 0.18438785428045568, 0.18438785428045568, 0.18438785428045568, 0.18438785428045568, 0.40392177805208423, 0.1350834211563811, 0.1143013563630917, 0.207820647932894, 0.103910323966447, 0.1454744535530258, 0.1350834211563811, 0.0727372267765129, 0.0311730971899341, 0.041564129586578805, 0.010391032396644701, 0.14649009475841404, 0.10986757106881052, 0.2563576658272246, 0.10986757106881052, 0.10986757106881052, 0.07324504737920702, 0.03662252368960351, 0.03662252368960351, 0.03662252368960351, 0.22771260413427172, 0.22771260413427172, 0.22771260413427172, 0.08646351599144149, 0.3026223059700452, 0.12969527398716224, 0.043231757995720745, 0.08646351599144149, 0.043231757995720745, 0.12969527398716224, 0.08646351599144149, 0.043231757995720745, 0.043231757995720745, 0.20270622432169486, 0.1768288339827551, 0.14232564686416874, 0.09919666296593578, 0.11213535813540566, 0.0819450694066426, 0.06469347584734941, 0.060380577457526126, 0.04312898389823295, 0.021564491949116475, 0.19126616203075028, 0.2983752127679704, 0.08415711129353012, 0.09180775777476013, 0.07650646481230011, 0.09180775777476013, 0.09180775777476013, 0.030602585924920045, 0.038253232406150056, 0.007650646481230011, 0.22842045874768407, 0.18491179993860138, 0.11723166401336166, 0.1087716470227067, 0.09306018689720462, 0.07009728363685543, 0.09064303918558891, 0.058011545078776906, 0.03867436338585127, 0.01087716470227067, 0.36830862876923676, 0.173149267416395, 0.23086568988852665, 0.057716422472131663, 0.057716422472131663, 0.057716422472131663, 0.057716422472131663, 0.057716422472131663, 0.173149267416395, 0.057716422472131663, 0.17026186367277965, 0.16874166846141556, 0.17026186367277965, 0.12465600733185653, 0.09273190789321034, 0.0790501509909334, 0.05776741803183595, 0.0760097605682052, 0.04560585634092312, 0.016722147325005144, 0.3719277674873075, 0.3812810376979389, 0.3812810376979389, 0.10415676365011678, 0.15623514547517517, 0.20831352730023356, 0.10415676365011678, 0.10415676365011678, 0.10415676365011678, 0.05207838182505839, 0.15623514547517517, 0.12222183509114512, 0.12222183509114512, 0.12222183509114512, 0.24444367018229024, 0.12222183509114512, 0.12222183509114512, 0.12222183509114512, 0.12222183509114512, 0.16199968857919556, 0.27771375185004954, 0.09257125061668318, 0.06942843796251238, 0.06942843796251238, 0.06942843796251238, 0.16199968857919556, 0.06942843796251238, 0.04628562530834159, 0.023142812654170796, 0.13846910332200232, 0.13846910332200232, 0.13846910332200232, 0.1582504037965741, 0.11209403602257331, 0.07912520189828705, 0.0989065023728588, 0.0725314350734298, 0.05275013459885803, 0.019781300474571762, 0.13742701734333024, 0.18323602312444032, 0.09161801156222016, 0.21377536031184705, 0.13742701734333024, 0.04580900578111008, 0.07634834296851681, 0.061078674374813444, 0.030539337187406722, 0.015269668593703361, 0.2490066542329752, 0.1245033271164876, 0.1245033271164876, 0.1245033271164876, 0.2490066542329752, 0.1245033271164876, 0.1245033271164876, 0.27388609788357915, 0.2057622842716604, 0.14716772649893592, 0.16760768851267702, 0.09402382526320906, 0.08721050459196203, 0.07494652738371736, 0.08721050459196203, 0.07222119911521856, 0.04633058056447983, 0.01635196961099288, 0.36967257468162157, 0.18976480286788977, 0.09488240143394489, 0.09488240143394489, 0.28464720430183466, 0.09488240143394489, 0.09488240143394489, 0.11693734309462919, 0.3508120292838876, 0.0877030073209719, 0.0877030073209719, 0.058468671547314596, 0.1461716788682865, 0.058468671547314596, 0.0877030073209719, 0.029234335773657298, 0.23640552302707593, 0.14184331381624554, 0.14184331381624554, 0.18912441842166072, 0.09456220921083036, 0.04728110460541518, 0.04728110460541518, 0.04728110460541518, 0.04728110460541518, 0.48729534692245424, 0.30095632616904716, 0.20063755077936477, 0.10031877538968238, 0.10031877538968238, 0.10031877538968238, 0.10031877538968238, 0.403535342730886, 0.21833417542785835, 0.1247623859587762, 0.12921818545730393, 0.13812978445435936, 0.07574859147497126, 0.053469593982332655, 0.0846601904720267, 0.07574859147497126, 0.07574859147497126, 0.02227899749263861, 0.23636886244932767, 0.1318210963659712, 0.14545776150727857, 0.11363887617756138, 0.05000110551812701, 0.07272888075363929, 0.07272888075363929, 0.06818332570653683, 0.08181999084784419, 0.027273330282614732, 0.21642191470521596, 0.21642191470521596, 0.21642191470521596, 0.21642191470521596, 0.21503433869054855, 0.16215704229123334, 0.12866808790500037, 0.08812882733219203, 0.08460367423890434, 0.10575459279863043, 0.07931594459897283, 0.0740282149590413, 0.04582699021273986, 0.015863188919794564, 0.22119684302781364, 0.186539273152488, 0.1641137867625714, 0.07950854265515882, 0.10295336933552617, 0.07848920236470806, 0.05912173684614374, 0.05606371597479148, 0.03975427132757941, 0.013251423775859804, 0.11655655455330861, 0.07770436970220575, 0.11655655455330861, 0.1554087394044115, 0.19426092425551436, 0.07770436970220575, 0.11655655455330861, 0.07770436970220575, 0.038852184851102874, 0.1274295519770257, 0.1274295519770257, 0.1274295519770257, 0.1274295519770257, 0.2548591039540514, 0.1274295519770257, 0.1274295519770257, 0.1274295519770257, 0.20812179323010355, 0.16833380334787787, 0.10712151122137682, 0.11630335504035198, 0.11324274043402693, 0.06121229212650104, 0.10712151122137682, 0.0703941359454762, 0.027545531456925468, 0.01530307303162526, 0.21927809875667825, 0.19839447030366128, 0.1775108418506443, 0.08092406025544079, 0.07570315314218655, 0.06526133891567805, 0.08092406025544079, 0.05481952468916956, 0.03393589623615259, 0.010441814226508488, 0.22110747790875981, 0.1748291685790194, 0.14140594517420685, 0.0745594983645818, 0.08484356710452412, 0.0874145842895097, 0.07970153273455295, 0.06427542962463947, 0.053991360884697165, 0.017997120294899055, 0.37947893615425937, 0.11954922901263086, 0.11954922901263086, 0.35864768703789257, 0.11954922901263086, 0.05977461450631543, 0.05977461450631543, 0.029887307253157715, 0.029887307253157715, 0.05977461450631543, 0.12670600990905742, 0.3801180297271723, 0.12670600990905742, 0.12670600990905742, 0.12670600990905742, 0.22492886893574265, 0.12111554481155373, 0.17878961376943647, 0.09227851033261238, 0.07497628964524755, 0.06920888274945927, 0.07497628964524755, 0.05767406895788273, 0.09227851033261238, 0.017302220687364818, 0.21199126783983002, 0.21199126783983002, 0.21199126783983002, 0.21199126783983002, 0.4872147698225096, 0.2436073849112548, 0.2891472139976819, 0.2891472139976819, 0.13044812578075582, 0.08696541718717055, 0.1739308343743411, 0.08696541718717055, 0.13044812578075582, 0.08696541718717055, 0.13044812578075582, 0.08696541718717055, 0.04348270859358527, 0.17573862340333907, 0.17573862340333907, 0.17573862340333907, 0.17573862340333907, 0.17573862340333907, 0.15449810386524768, 0.15449810386524768, 0.15449810386524768, 0.15449810386524768, 0.15449810386524768, 0.15449810386524768, 0.28331604825655443, 0.15380014048212953, 0.10927904718467099, 0.08499481447696633, 0.09308955871286788, 0.06071058176926166, 0.072852698123114, 0.08499481447696633, 0.036426349061557, 0.024284232707704665, 0.14722707878699995, 0.13802538636281245, 0.17483215605956243, 0.19323554090793743, 0.10121861666606247, 0.04600846212093748, 0.08281523181768748, 0.06441184696931247, 0.03680676969674999, 0.018403384848374994, 0.12649172799969094, 0.2529834559993819, 0.22136052399945913, 0.06324586399984547, 0.031622931999922735, 0.031622931999922735, 0.15811465999961366, 0.06324586399984547, 0.06324586399984547, 0.3546952085559716, 0.3071234847068723, 0.2084780168802596, 0.08934772152011126, 0.163804156120204, 0.1042390084401298, 0.08934772152011126, 0.04467386076005563, 0.07445643460009271, 0.07445643460009271, 0.11913029536014835, 0.014891286920018544, 0.21978065218513573, 0.10989032609256787, 0.10989032609256787, 0.10989032609256787, 0.10989032609256787, 0.10989032609256787, 0.10989032609256787, 0.10989032609256787, 0.10989032609256787, 0.2444617842389393, 0.22001560581504537, 0.12223089211946965, 0.08556162448362875, 0.04889235684778786, 0.08556162448362875, 0.07333853527168178, 0.04889235684778786, 0.04889235684778786, 0.02444617842389393, 0.2297805031303876, 0.16832757787458627, 0.17634317682099512, 0.09351532104143681, 0.08817158841049756, 0.07214039051767983, 0.06412479157127096, 0.05878105894033171, 0.034734262101105105, 0.010687465261878493, 0.17521991007836427, 0.16061825090516724, 0.3066348426371375, 0.08760995503918213, 0.0730082958659851, 0.05840663669278809, 0.014601659173197022, 0.043804977519591067, 0.0730082958659851, 0.014601659173197022, 0.14011785571028462, 0.07005892785514231, 0.14011785571028462, 0.14011785571028462, 0.14011785571028462, 0.07005892785514231, 0.07005892785514231, 0.21017678356542693, 0.07005892785514231, 0.10251279458493556, 0.13668372611324742, 0.15376919187740334, 0.20502558916987113, 0.15376919187740334, 0.05125639729246778, 0.08542732882077964, 0.05125639729246778, 0.05125639729246778, 0.017085465764155927, 0.20384945957281295, 0.19881613958336078, 0.13086631972575646, 0.07801645983650866, 0.08556643982068692, 0.0905997598101391, 0.07801645983650866, 0.07549979984178258, 0.04529987990506955, 0.012583299973630429, 0.15666576597895948, 0.18277672697545272, 0.15666576597895948, 0.069629229323982, 0.10444384398597298, 0.13055480498246622, 0.069629229323982, 0.069629229323982, 0.04351826832748874, 0.0174073073309955, 0.1698385761543838, 0.22291313120262876, 0.12737893211578785, 0.07430437706754292, 0.0955341990868409, 0.12737893211578785, 0.06368946605789393, 0.0849192880771919, 0.04245964403859595, 0.010614911009648988, 0.46573488587282674, 0.08218850927167531, 0.13698084878612551, 0.027396169757225104, 0.05479233951445021, 0.08218850927167531, 0.05479233951445021, 0.05479233951445021, 0.027396169757225104, 0.027396169757225104, 0.2437827138013671, 0.0812609046004557, 0.1625218092009114, 0.2437827138013671, 0.0812609046004557, 0.0812609046004557, 0.0812609046004557, 0.0812609046004557, 0.18242652476420038, 0.24963629704574788, 0.15362233664353717, 0.12001745050276341, 0.057608376241326434, 0.048006980201105366, 0.057608376241326434, 0.07681116832176858, 0.03840558416088429, 0.014402094060331608, 0.218771989664449, 0.2386603523612171, 0.15910690157414473, 0.03977672539353618, 0.07955345078707236, 0.03977672539353618, 0.03977672539353618, 0.15910690157414473, 0.03977672539353618, 0.15696685298826119, 0.15696685298826119, 0.15696685298826119, 0.15696685298826119, 0.15696685298826119, 0.15696685298826119, 0.1506785581987498, 0.1506785581987498, 0.1506785581987498, 0.3013571163974996, 0.1506785581987498, 0.2318251245358701, 0.11591256226793505, 0.11591256226793505, 0.2318251245358701, 0.11591256226793505, 0.11591256226793505, 0.13773605249992607, 0.1530400583332512, 0.33668812833315265, 0.04591201749997536, 0.09182403499995072, 0.06121602333330048, 0.01530400583332512, 0.06121602333330048, 0.0765200291666256, 0.15431088671389792, 0.23146633007084688, 0.13887979804250813, 0.06172435468555917, 0.09258653202833875, 0.13887979804250813, 0.046293266014169374, 0.09258653202833875, 0.030862177342779584, 0.3803621127320197], \"Term\": [\"acclaimed\", \"ackley\", \"ackley\", \"ackley\", \"ackley\", \"ackley\", \"ackley\", \"ackley\", \"acoustic\", \"acoustic\", \"acoustic\", \"acoustic\", \"acoustic\", \"acoustic\", \"acoustic\", \"acoustic\", \"acoustic\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"activated\", \"activated\", \"activated\", \"activated\", \"activated\", \"activated\", \"activated\", \"activated\", \"activated\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"aekk\", \"aekk\", \"aekk\", \"aekk\", \"aekk\", \"aekk\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"agents\", \"agents\", \"agents\", \"agents\", \"agents\", \"agents\", \"agents\", \"agents\", \"agents\", \"agents\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregation\", \"aggregation\", \"aggregation\", \"aggregation\", \"aggregation\", \"aggregation\", \"aggregation\", \"aggregation\", \"aggregation\", \"agnostic\", \"agnostic\", \"ahs\", \"ahs\", \"ajk\", \"ajk\", \"akq\", \"akq\", \"akq\", \"akq\", \"akq\", \"akq\", \"akq\", \"akq\", \"albert\", \"albert\", \"albert\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"alp\", \"alp\", \"alp\", \"alp\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"ancestral\", \"ancestral\", \"ancestral\", \"ancestral\", \"anchor\", \"anchor\", \"anchor\", \"anchor\", \"anchor\", \"anchor\", \"anguelov\", \"anguelov\", \"anova\", \"anova\", \"anova\", \"anova\", \"anova\", \"anova\", \"anova\", \"anova\", \"aoa\", \"aoa\", \"aoa\", \"aoa\", \"aoa\", \"aoa\", \"aoa\", \"aoa\", \"aoa\", \"aoa\", \"appearance\", \"appearance\", \"appearance\", \"appearance\", \"appearance\", \"appearance\", \"appearance\", \"appearance\", \"appearance\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"arrival\", \"aspect\", \"aspect\", \"aspect\", \"aspect\", \"aspect\", \"aspect\", \"aspect\", \"aspect\", \"aspect\", \"assoon\", \"attribute\", \"attribute\", \"attribute\", \"attribute\", \"attribute\", \"attribute\", \"attribute\", \"attribute\", \"attribute\", \"attribute\", \"attributes\", \"attributes\", \"attributes\", \"attributes\", \"attributes\", \"attributes\", \"attributes\", \"attributes\", \"attributes\", \"attributes\", \"autoencoding\", \"autoencoding\", \"autoencoding\", \"aw\", \"aw\", \"aw\", \"aw\", \"bachrach\", \"bachrach\", \"bachrach\", \"bachrach\", \"bachrach\", \"bachrach\", \"bachrach\", \"bamden\", \"bamden\", \"bamden\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"bed\", \"bed\", \"bed\", \"bed\", \"bed\", \"bed\", \"bed\", \"bed\", \"berlinde\", \"bilmes\", \"bilmes\", \"bilmes\", \"bilmes\", \"bilmes\", \"bilmes\", \"bilmes\", \"blanche\", \"bradley\", \"bradley\", \"bradley\", \"bradley\", \"bradley\", \"bradley\", \"bradley\", \"bradley\", \"bradley\", \"brain\", \"brain\", \"brain\", \"brain\", \"brain\", \"brain\", \"brain\", \"brain\", \"brain\", \"brain\", \"bregler\", \"bregler\", \"bregler\", \"bregler\", \"bregler\", \"bregler\", \"bregler\", \"bsvm\", \"bsvm\", \"bsvm\", \"bsvm\", \"bsvm\", \"bsvm\", \"bsvm\", \"bsvm\", \"bursting\", \"bursting\", \"bursting\", \"bursting\", \"bursting\", \"bursting\", \"bursts\", \"bursts\", \"bursts\", \"bursts\", \"bursts\", \"bursts\", \"bursts\", \"capability\", \"capability\", \"capability\", \"capability\", \"capability\", \"capability\", \"capability\", \"capability\", \"capability\", \"carin\", \"carrojl\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"chinese\", \"chinese\", \"chinese\", \"chinese\", \"chinese\", \"chinese\", \"chinese\", \"chinese\", \"christoph\", \"christoph\", \"christoph\", \"christoph\", \"christoph\", \"cii\", \"cii\", \"cii\", \"cii\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"ck\", \"ck\", \"ck\", \"ck\", \"ck\", \"ck\", \"ck\", \"ck\", \"ck\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classication\", \"classication\", \"classication\", \"classication\", \"classication\", \"classication\", \"classication\", \"classication\", \"classication\", \"classier\", \"classier\", \"classier\", \"classier\", \"classier\", \"classier\", \"classier\", \"classier\", \"cliff\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"cmin\", \"cmin\", \"cmin\", \"cmin\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"coalescent\", \"coalescent\", \"coalescent\", \"coalescent\", \"coalescent\", \"coalescent\", \"coalescent\", \"coalescent\", \"coalescent\", \"coco\", \"coco\", \"coco\", \"coco\", \"coco\", \"coco\", \"coco\", \"coco\", \"coco\", \"confusion\", \"confusion\", \"confusion\", \"confusion\", \"confusion\", \"confusion\", \"confusion\", \"confusion\", \"confusion\", \"consistency\", \"consistency\", \"consistency\", \"consistency\", \"consistency\", \"consistency\", \"consistency\", \"consistency\", \"consistency\", \"consistency\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"conv\", \"conv\", \"conv\", \"conv\", \"conv\", \"conv\", \"conv\", \"conv\", \"conv\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"correlates\", \"correlates\", \"crbp\", \"crbp\", \"crbp\", \"crbp\", \"crbp\", \"crbp\", \"crbp\", \"crbp\", \"cuboid\", \"cuboid\", \"cuboid\", \"cuboid\", \"cuboid\", \"cuboid\", \"cuboid\", \"cuboid\", \"cuboid\", \"cuboid\", \"cxj\", \"cxj\", \"cxj\", \"cxj\", \"cxj\", \"cxj\", \"cxj\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"dawt\", \"dawta\", \"dawta\", \"dawta\", \"dawta\", \"dawta\", \"dawta\", \"dawta\", \"dawta\", \"dbm\", \"dbm\", \"dbm\", \"dbm\", \"dbm\", \"dbm\", \"dbm\", \"dbm\", \"dbm\", \"dbm\", \"deformable\", \"deformable\", \"deformable\", \"deformable\", \"deformable\", \"deformable\", \"deformable\", \"deformable\", \"deformable\", \"deformation\", \"deformation\", \"deformation\", \"deformation\", \"deformation\", \"deformation\", \"deformation\", \"deformation\", \"deformation\", \"deformations\", \"deformations\", \"deformations\", \"deformations\", \"deformations\", \"deformations\", \"demonstrated\", \"demonstrated\", \"demonstrated\", \"demonstrated\", \"demonstrated\", \"demonstrated\", \"demonstrated\", \"demonstrated\", \"demonstrated\", \"demonstrated\", \"devices\", \"devices\", \"devices\", \"devices\", \"devices\", \"devices\", \"devices\", \"devices\", \"devices\", \"devices\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"dilemma\", \"dilemma\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dirichlet\", \"dirichlet\", \"dirichlet\", \"dirichlet\", \"dirichlet\", \"dirichlet\", \"dirichlet\", \"dirichlet\", \"dirichlet\", \"dirichlet\", \"disregarding\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"dn\", \"dn\", \"dn\", \"dn\", \"dn\", \"dn\", \"dn\", \"dn\", \"dn\", \"dn\", \"doubles\", \"dpm\", \"dpm\", \"dpm\", \"dpm\", \"dpm\", \"dpm\", \"dpm\", \"dpm\", \"dpm\", \"dqh\", \"dqh\", \"dqh\", \"dqh\", \"dstitch\", \"dstitch\", \"dstitch\", \"dv\", \"dv\", \"dyer\", \"dynamically\", \"dynamically\", \"dynamically\", \"dynamically\", \"dynamically\", \"dynamically\", \"dynamically\", \"dynamically\", \"dynamically\", \"ecoc\", \"ecoc\", \"ecoc\", \"ecoc\", \"ecoc\", \"ecoc\", \"ecoc\", \"ecoc\", \"ecoc\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"effects\", \"effects\", \"effects\", \"effects\", \"effects\", \"effects\", \"effects\", \"effects\", \"effects\", \"effects\", \"efh\", \"efh\", \"efh\", \"efh\", \"efh\", \"efh\", \"efh\", \"efh\", \"efh\", \"eii\", \"eii\", \"ekpt\", \"ekpt\", \"ekpt\", \"ekpt\", \"electrodes\", \"electrodes\", \"electrodes\", \"electrodes\", \"electrodes\", \"electrodes\", \"ellis\", \"ellis\", \"ellis\", \"ellis\", \"ellis\", \"ellis\", \"ellis\", \"enfuvirtide\", \"enfuvirtide\", \"enfuvirtide\", \"enfuvirtide\", \"entrainer\", \"entrainer\", \"entrainer\", \"entrainer\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"epsp\", \"epsp\", \"epsp\", \"epsp\", \"epsp\", \"epsp\", \"epsp\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"establishing\", \"establishing\", \"establishing\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"etienne\", \"etienne\", \"etienne\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"faces\", \"faces\", \"faces\", \"faces\", \"faces\", \"faces\", \"faces\", \"faces\", \"faces\", \"factors\", \"factors\", \"factors\", \"factors\", \"factors\", \"factors\", \"factors\", \"factors\", \"factors\", \"factors\", \"faster\", \"faster\", \"faster\", \"faster\", \"faster\", \"faster\", \"faster\", \"faster\", \"faster\", \"faster\", \"fcn\", \"fcn\", \"fcn\", \"fcn\", \"fcn\", \"fcn\", \"fcn\", \"fcn\", \"fcn\", \"fcn\", \"fcns\", \"fcns\", \"fcut\", \"fcut\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"fgm\", \"fgm\", \"fgm\", \"fgm\", \"fgm\", \"fgm\", \"fgm\", \"fgm\", \"fiedler\", \"fiedler\", \"fiedler\", \"fiedler\", \"fiedler\", \"fiedler\", \"fiedler\", \"fiedler\", \"fiedler\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"fisher\", \"fisher\", \"fisher\", \"fisher\", \"fisher\", \"fisher\", \"fisher\", \"fisher\", \"fisher\", \"fisher\", \"fitc\", \"fitc\", \"fitc\", \"fitc\", \"fitc\", \"fitc\", \"fitc\", \"florian\", \"florian\", \"florianstimberg\", \"fork\", \"fork\", \"fork\", \"fork\", \"fork\", \"fork\", \"fork\", \"fork\", \"forks\", \"forks\", \"forks\", \"forks\", \"forks\", \"forks\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"freeman\", \"freeman\", \"freeman\", \"freeman\", \"freeman\", \"freeman\", \"freeman\", \"freeman\", \"freeman\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frfs\", \"frfs\", \"frfs\", \"frfs\", \"frfs\", \"frfs\", \"frfs\", \"frfs\", \"frfs\", \"frustrated\", \"frustrated\", \"fs\", \"fs\", \"fs\", \"fs\", \"fs\", \"fs\", \"fs\", \"fs\", \"fs\", \"fsa\", \"fsa\", \"fsa\", \"fsa\", \"fsa\", \"fsa\", \"fsa\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"gamma\", \"gamma\", \"gamma\", \"gamma\", \"gamma\", \"gamma\", \"gamma\", \"gamma\", \"gamma\", \"gaps\", \"gaps\", \"gaps\", \"gaps\", \"gaps\", \"gaps\", \"gaps\", \"gaps\", \"gaps\", \"gaps\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gerhand\", \"gerhand\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"gpc\", \"gpc\", \"gpc\", \"gpc\", \"gpc\", \"gpc\", \"gpc\", \"gpc\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graphs\", \"graphs\", \"graphs\", \"graphs\", \"graphs\", \"graphs\", \"graphs\", \"graphs\", \"graphs\", \"graphs\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"gsn\", \"gsn\", \"gsn\", \"gsn\", \"gsn\", \"gsn\", \"gsn\", \"gsn\", \"gsn\", \"gy\", \"gy\", \"gy\", \"gy\", \"gy\", \"gy\", \"gy\", \"gy\", \"gy\", \"hazard\", \"hazard\", \"hazard\", \"hazard\", \"hazard\", \"hazard\", \"hazard\", \"hazard\", \"hazard\", \"hazards\", \"hazards\", \"hazoo\", \"hazoo\", \"hazoo\", \"hazoo\", \"hazoo\", \"hazoo\", \"hazoo\", \"hazoo\", \"hazoo\", \"hazoo\", \"hedau\", \"hedau\", \"hedau\", \"hedau\", \"herten\", \"hinge\", \"hinge\", \"hinge\", \"hinge\", \"hinge\", \"hinge\", \"hinge\", \"hinge\", \"hinge\", \"hinge\", \"hiv\", \"hiv\", \"hiv\", \"hiv\", \"hiv\", \"hiv\", \"hiv\", \"hiv\", \"hiv\", \"hiv\", \"hopfield\", \"hopfield\", \"hopfield\", \"hormel\", \"hormel\", \"hormel\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"ht\", \"ht\", \"ht\", \"ht\", \"ht\", \"ht\", \"ht\", \"ht\", \"ht\", \"ht\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"identication\", \"identication\", \"identication\", \"identication\", \"identication\", \"identication\", \"identied\", \"identied\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ii\", \"ii\", \"ii\", \"ii\", \"ii\", \"ii\", \"ii\", \"ii\", \"ii\", \"ii\", \"ijk\", \"ijk\", \"ijk\", \"ijk\", \"ijk\", \"ijk\", \"ijk\", \"ijk\", \"ijk\", \"ikl\", \"ikl\", \"ikl\", \"ikl\", \"ikl\", \"ikl\", \"ikl\", \"ikl\", \"ikl\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"img\", \"img\", \"img\", \"img\", \"inco\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"informed\", \"informed\", \"inhibition\", \"inhibition\", \"inhibition\", \"inhibition\", \"inhibition\", \"inhibition\", \"inhibition\", \"inhibition\", \"inhibition\", \"inhibition\", \"inhibitory\", \"inhibitory\", \"inhibitory\", \"inhibitory\", \"inhibitory\", \"inhibitory\", \"inhibitory\", \"inhibitory\", \"inhibitory\", \"inhibitory\", \"inhomogeneous\", \"inhomogeneous\", \"inhomogeneous\", \"inhomogeneous\", \"inhomogeneous\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"inserts\", \"interpolant\", \"interpolant\", \"interpolant\", \"interpolated\", \"interpolated\", \"interpolated\", \"interpolated\", \"interpolated\", \"interpolated\", \"interpolated\", \"interpolated\", \"interpolation\", \"interpolation\", \"interpolation\", \"interpolation\", \"interpolation\", \"interpolation\", \"interpolation\", \"interpolation\", \"interpolation\", \"interpolation\", \"irish\", \"irish\", \"irish\", \"irregularity\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"iter\", \"iter\", \"iter\", \"iter\", \"iter\", \"iter\", \"iter\", \"jared\", \"jb\", \"jb\", \"jb\", \"jb\", \"jb\", \"jb\", \"jb\", \"jb\", \"jb\", \"jighu\", \"jii\", \"jii\", \"jii\", \"jii\", \"jk\", \"jk\", \"jk\", \"jk\", \"jk\", \"jk\", \"jk\", \"jk\", \"jk\", \"jk\", \"join\", \"join\", \"join\", \"jump\", \"jump\", \"jump\", \"jump\", \"jump\", \"jump\", \"jump\", \"jump\", \"jump\", \"jumps\", \"jumps\", \"jumps\", \"jumps\", \"jumps\", \"jumps\", \"jumps\", \"kbp\", \"kbp\", \"kbp\", \"kbp\", \"knee\", \"kq\", \"kq\", \"kq\", \"kq\", \"kq\", \"kq\", \"kq\", \"kq\", \"kq\", \"kq\", \"krp\", \"krp\", \"krp\", \"krp\", \"krp\", \"krp\", \"krp\", \"krp\", \"krp\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"lake\", \"lake\", \"lake\", \"lake\", \"lake\", \"lake\", \"lake\", \"lake\", \"lake\", \"lake\", \"lambon\", \"laminar\", \"laminar\", \"laminar\", \"laminar\", \"laminar\", \"laminar\", \"laminar\", \"laminar\", \"lange\", \"lange\", \"lange\", \"lange\", \"lange\", \"lange\", \"lange\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"late\", \"late\", \"late\", \"late\", \"late\", \"latency\", \"latency\", \"latency\", \"latency\", \"latency\", \"latency\", \"latency\", \"latency\", \"latency\", \"lateness\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"lb\", \"lb\", \"lb\", \"lcls\", \"lda\", \"lda\", \"lda\", \"lda\", \"lda\", \"lda\", \"lda\", \"lda\", \"lda\", \"lda\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"lengthscale\", \"lengthscale\", \"lengthscale\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"lgn\", \"lgn\", \"lgn\", \"lgn\", \"lgn\", \"lgn\", \"lgn\", \"lgn\", \"lgn\", \"lgn\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"lip\", \"lip\", \"lip\", \"lip\", \"lip\", \"lip\", \"lip\", \"lip\", \"lip\", \"lip\", \"lmsr\", \"lmsr\", \"lmsr\", \"lmsr\", \"lmsr\", \"lmsr\", \"lmsr\", \"lmsr\", \"lmsr\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logop\", \"logop\", \"logop\", \"logop\", \"logop\", \"loo\", \"loo\", \"loo\", \"loo\", \"loo\", \"loo\", \"loo\", \"loo\", \"loo\", \"losers\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"lsp\", \"lsp\", \"lsp\", \"lsp\", \"lsp\", \"lsp\", \"lsp\", \"lsp\", \"lter\", \"lter\", \"lter\", \"lter\", \"lter\", \"lter\", \"lter\", \"lter\", \"madelon\", \"madelon\", \"madelon\", \"majani\", \"majani\", \"majani\", \"maker\", \"maker\", \"maker\", \"maker\", \"maker\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manuscripts\", \"manuscripts\", \"manuscripts\", \"manuscripts\", \"maps\", \"maps\", \"maps\", \"maps\", \"maps\", \"maps\", \"maps\", \"maps\", \"maps\", \"maps\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"market\", \"market\", \"market\", \"market\", \"market\", \"market\", \"market\", \"market\", \"market\", \"market\", \"markets\", \"markets\", \"markets\", \"markets\", \"markets\", \"markets\", \"markets\", \"markets\", \"markets\", \"markets\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"metaaction\", \"metaaction\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"michi\", \"michi\", \"michi\", \"michi\", \"michi\", \"michi\", \"mim\", \"mim\", \"mim\", \"mim\", \"mim\", \"mim\", \"mim\", \"mjp\", \"mjps\", \"mmpp\", \"mmpps\", \"modalities\", \"modalities\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"modulated\", \"modulated\", \"modulated\", \"modulated\", \"modulated\", \"modulated\", \"modulated\", \"modulated\", \"molecule\", \"morphogenesis\", \"morphogenesis\", \"morphogenesis\", \"morphogenesis\", \"morphogenesis\", \"morphogenesis\", \"morphogenesis\", \"morphogenesis\", \"morrison\", \"morrison\", \"morrison\", \"morrison\", \"morrison\", \"movement\", \"movement\", \"movement\", \"movement\", \"movement\", \"movement\", \"movement\", \"movement\", \"movement\", \"movement\", \"mp\", \"mp\", \"mp\", \"mp\", \"mp\", \"mp\", \"mp\", \"mp\", \"mp\", \"mp\", \"msr\", \"msr\", \"msr\", \"msr\", \"msr\", \"msr\", \"msr\", \"msr\", \"msr\", \"msr\", \"mst\", \"mst\", \"mst\", \"mst\", \"mst\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutations\", \"mutations\", \"mutations\", \"mutations\", \"mutations\", \"mutations\", \"mutations\", \"mutations\", \"myopic\", \"myopic\", \"myopic\", \"myopic\", \"myopic\", \"myopic\", \"myopic\", \"naming\", \"naming\", \"naming\", \"naming\", \"naming\", \"naming\", \"naming\", \"naming\", \"naming\", \"naming\", \"nataoqu\", \"nba\", \"nba\", \"nba\", \"nba\", \"nca\", \"nca\", \"nca\", \"nca\", \"nca\", \"nca\", \"nee\", \"netr\", \"netr\", \"netr\", \"netr\", \"netr\", \"netr\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"nominal\", \"nominal\", \"nominal\", \"nominal\", \"nominal\", \"nominal\", \"nominal\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonnal\", \"nonnal\", \"nonnal\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norwich\", \"norwich\", \"norwich\", \"norwich\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"novellino\", \"nucleotide\", \"nucleotide\", \"nucleotide\", \"nucleotide\", \"nucleotide\", \"nucleotide\", \"nucleotide\", \"nucleotide\", \"nucleotide\", \"nucleotides\", \"nucleotides\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"nwrst\", \"nwrst\", \"nwrst\", \"nwrst\", \"nwrst\", \"nwrst\", \"nwrst\", \"ob\", \"ob\", \"ob\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"objectivity\", \"objectivity\", \"objectivity\", \"objectivity\", \"objectivity\", \"objectivity\", \"objectivity\", \"objectivity\", \"offline\", \"offline\", \"offline\", \"offline\", \"offline\", \"offline\", \"offline\", \"offline\", \"ohem\", \"ohem\", \"olfactory\", \"olfactory\", \"olfactory\", \"olfactory\", \"olfactory\", \"olfactory\", \"olfactory\", \"olfactory\", \"olfactory\", \"omv\", \"omv\", \"omv\", \"omv\", \"omv\", \"omv\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"organizing\", \"organizing\", \"organizing\", \"organizing\", \"organizing\", \"organizing\", \"organizing\", \"organizing\", \"organizing\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orthonormal\", \"orthonormal\", \"orthonormal\", \"orthonormal\", \"orthonormal\", \"orthonormal\", \"orthonormal\", \"orthonormal\", \"orthonormal\", \"orthonormal\", \"outperfonn\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"overfeat\", \"paired\", \"paired\", \"paired\", \"paired\", \"paired\", \"paired\", \"paired\", \"paired\", \"paired\", \"palm\", \"palm\", \"palm\", \"palm\", \"palm\", \"palm\", \"palm\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"parts\", \"parts\", \"parts\", \"parts\", \"parts\", \"parts\", \"parts\", \"parts\", \"parts\", \"parts\", \"party\", \"party\", \"party\", \"party\", \"party\", \"party\", \"party\", \"party\", \"party\", \"patch\", \"patch\", \"patch\", \"patch\", \"patch\", \"patch\", \"patch\", \"patch\", \"patch\", \"patterns\", \"patterns\", \"patterns\", \"patterns\", \"patterns\", \"patterns\", \"patterns\", \"patterns\", \"patterns\", \"patterns\", \"pdfs\", \"pdfs\", \"pdfs\", \"pdfs\", \"pdfs\", \"pdfs\", \"pdfs\", \"pdfs\", \"pecoc\", \"pecoc\", \"pecoc\", \"pecoc\", \"pecoc\", \"pecoc\", \"pecoc\", \"pecoc\", \"pecoc\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"perkel\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"personalized\", \"personalized\", \"personalized\", \"personalized\", \"personalized\", \"personalized\", \"personalized\", \"personalized\", \"pf\", \"pf\", \"pf\", \"pf\", \"pf\", \"pf\", \"pf\", \"pf\", \"pf\", \"pf\", \"pgmm\", \"pgmm\", \"pgmm\", \"pgmm\", \"pgmm\", \"pgmm\", \"pgmm\", \"pgmm\", \"pgmm\", \"pik\", \"pik\", \"pik\", \"pik\", \"pik\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"polytrodes\", \"pooling\", \"pooling\", \"pooling\", \"pooling\", \"pooling\", \"pooling\", \"pooling\", \"pooling\", \"pooling\", \"pooling\", \"pose\", \"pose\", \"pose\", \"pose\", \"pose\", \"pose\", \"pose\", \"pose\", \"pose\", \"pose\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"prepyriform\", \"prepyriform\", \"prepyriform\", \"prepyriform\", \"pressure\", \"pressure\", \"pressure\", \"pressure\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problems\", \"problems\", \"problems\", \"problems\", \"problems\", \"problems\", \"problems\", \"problems\", \"problems\", \"problems\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"proposals\", \"proposals\", \"proposals\", \"proposals\", \"proposals\", \"proposals\", \"proposals\", \"proposals\", \"proposals\", \"proximal\", \"proximal\", \"proximal\", \"proximal\", \"proximal\", \"proximal\", \"proximal\", \"proximal\", \"proximal\", \"proximal\", \"psc\", \"psc\", \"psc\", \"psc\", \"psc\", \"ptn\", \"ptn\", \"ptn\", \"ptn\", \"ptn\", \"pxa\", \"pxa\", \"pxa\", \"pxa\", \"pxa\", \"pzru\", \"pzru\", \"pzru\", \"pzru\", \"qmax\", \"qmax\", \"qmax\", \"qmax\", \"qmax\", \"qmax\", \"qmax\", \"qmax\", \"qmax\", \"qnn\", \"qnn\", \"qnn\", \"qnn\", \"qnn\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"queuing\", \"queuing\", \"queuing\", \"queuing\", \"queuing\", \"queuing\", \"queuing\", \"rai\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rat\", \"rat\", \"rat\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rats\", \"rats\", \"rats\", \"reactive\", \"reactive\", \"reactive\", \"reactive\", \"reactive\", \"reactive\", \"recombination\", \"recombination\", \"recombination\", \"recombination\", \"recombination\", \"recombination\", \"recombination\", \"recombination\", \"recombination\", \"recombination\", \"reconstructed\", \"reconstructed\", \"reconstructed\", \"reconstructed\", \"reconstructed\", \"reconstructed\", \"reconstructed\", \"ree\", \"ree\", \"ree\", \"ree\", \"ree\", \"ree\", \"ree\", \"ree\", \"ree\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regulatory\", \"regulatory\", \"regulatory\", \"regulatory\", \"regulatory\", \"regulatory\", \"regulatory\", \"regulatory\", \"regulatory\", \"regulatory\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"replicates\", \"replicates\", \"replicates\", \"replicates\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"rescore\", \"residual\", \"residual\", \"residual\", \"residual\", \"residual\", \"residual\", \"residual\", \"residual\", \"residual\", \"residual\", \"resnet\", \"resnet\", \"resnet\", \"resnet\", \"resnet\", \"resnet\", \"resnet\", \"resnet\", \"resnet\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"resting\", \"resting\", \"resting\", \"resting\", \"resting\", \"resting\", \"resting\", \"resting\", \"resting\", \"resting\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"rij\", \"rij\", \"rij\", \"rij\", \"rij\", \"rij\", \"rij\", \"rij\", \"rij\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"rivest\", \"rivest\", \"rivest\", \"rivest\", \"rivest\", \"rivest\", \"rivest\", \"rl\", \"rl\", \"rl\", \"rl\", \"rl\", \"rl\", \"rl\", \"rl\", \"rmm\", \"rmm\", \"rmm\", \"rmm\", \"rmm\", \"rmm\", \"rmm\", \"rmm\", \"rmm\", \"rmse\", \"rmse\", \"rmse\", \"rmse\", \"rmse\", \"rob\", \"roi\", \"roi\", \"roi\", \"roi\", \"roi\", \"roi\", \"roi\", \"roi\", \"roi\", \"rois\", \"rois\", \"rois\", \"rois\", \"rois\", \"rois\", \"rois\", \"rois\", \"rois\", \"room\", \"room\", \"room\", \"room\", \"room\", \"room\", \"room\", \"room\", \"room\", \"room\", \"rpn\", \"rpn\", \"rpn\", \"rpn\", \"rpn\", \"rpn\", \"rpn\", \"rpn\", \"rpn\", \"rst\", \"rst\", \"rst\", \"rst\", \"rst\", \"rst\", \"rst\", \"rst\", \"ruttor\", \"saliency\", \"saliency\", \"saliency\", \"saliency\", \"saliency\", \"saliency\", \"saliency\", \"saliency\", \"saliency\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sampler\", \"sampler\", \"sampler\", \"sampler\", \"sampler\", \"sampler\", \"sampler\", \"sampler\", \"sampler\", \"sampler\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"sampling\", \"sampling\", \"sampling\", \"sampling\", \"sampling\", \"sampling\", \"sampling\", \"sampling\", \"sampling\", \"sampling\", \"sapalm\", \"sapalm\", \"sapalm\", \"sapalm\", \"sapalm\", \"sapalm\", \"sapalm\", \"sapalm\", \"sapalm\", \"sapalm\", \"sapiro\", \"sapiro\", \"sapiro\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"schmidt\", \"schmidt\", \"schmidt\", \"schmidt\", \"schmidt\", \"schmidt\", \"schmidt\", \"schmidt\", \"schmidt\", \"schmidt\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"scoreparts\", \"scoring\", \"scoring\", \"scoring\", \"scoring\", \"scoring\", \"scoring\", \"scoring\", \"scoring\", \"scoring\", \"scoring\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"segments\", \"segments\", \"segments\", \"segments\", \"segments\", \"segments\", \"segments\", \"segments\", \"segments\", \"segments\", \"sensations\", \"sensations\", \"sensations\", \"sensations\", \"sensations\", \"sensations\", \"sensations\", \"separable\", \"separable\", \"separable\", \"separable\", \"separable\", \"separable\", \"separable\", \"separable\", \"separable\", \"separable\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"sgcp\", \"shepherds\", \"shortcuts\", \"shortcuts\", \"shortcuts\", \"shortcuts\", \"shortcuts\", \"shortcuts\", \"shortcuts\", \"shortcuts\", \"shunting\", \"shunting\", \"shunting\", \"shunting\", \"shunting\", \"shunting\", \"shunting\", \"shunting\", \"siet\", \"siet\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparsified\", \"sparsified\", \"sparsified\", \"sparsified\", \"sparsified\", \"sparsified\", \"sparsified\", \"sparsified\", \"sparsified\", \"spelling\", \"spelling\", \"spelling\", \"spelling\", \"spoken\", \"spoken\", \"spoken\", \"sse\", \"sse\", \"sse\", \"sse\", \"sse\", \"sse\", \"sse\", \"sse\", \"sse\", \"ssm\", \"ssm\", \"ssm\", \"ssm\", \"ssm\", \"ssm\", \"ssm\", \"ssm\", \"ssm\", \"staged\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"stich\", \"stimberg\", \"stitching\", \"stitching\", \"stitching\", \"stitching\", \"stitching\", \"stitching\", \"stitching\", \"stitching\", \"stitching\", \"storkey\", \"storkey\", \"storkey\", \"stride\", \"stride\", \"stride\", \"stride\", \"stride\", \"stride\", \"stride\", \"stride\", \"stride\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"subnetwork\", \"subnetwork\", \"subnetwork\", \"subnetwork\", \"subnetwork\", \"subnetwork\", \"subnetwork\", \"subnetwork\", \"subnetwork\", \"subspace\", \"subspace\", \"subspace\", \"subspace\", \"subspace\", \"subspace\", \"subspace\", \"subspace\", \"subspace\", \"subspace\", \"sudden\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"suppress\", \"suppress\", \"suppress\", \"suppress\", \"suppress\", \"suppress\", \"suppress\", \"survival\", \"survival\", \"survival\", \"survival\", \"survival\", \"survival\", \"survival\", \"survival\", \"survival\", \"svm\", \"svm\", \"svm\", \"svm\", \"svm\", \"svm\", \"svm\", \"svm\", \"svm\", \"svm\", \"svms\", \"svms\", \"svms\", \"svms\", \"svms\", \"svms\", \"svms\", \"svms\", \"svms\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"table\", \"table\", \"table\", \"table\", \"table\", \"table\", \"table\", \"table\", \"table\", \"table\", \"taite\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"tch\", \"tch\", \"tch\", \"tcw\", \"tcw\", \"tcw\", \"te\", \"te\", \"te\", \"te\", \"te\", \"te\", \"te\", \"team\", \"team\", \"team\", \"team\", \"team\", \"tenn\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensors\", \"tensors\", \"tensors\", \"tensors\", \"tensors\", \"tensors\", \"tensors\", \"tensors\", \"tensors\", \"terences\", \"terences\", \"terences\", \"therapy\", \"therapy\", \"therapy\", \"therapy\", \"therapy\", \"therapy\", \"therapy\", \"therapy\", \"therapy\", \"therapy\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"ti\", \"ti\", \"ti\", \"ti\", \"ti\", \"ti\", \"ti\", \"ti\", \"ti\", \"ti\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tiv\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"transcribe\", \"transcribes\", \"transcribes\", \"transduction\", \"transduction\", \"transduction\", \"transduction\", \"transduction\", \"transduction\", \"transduction\", \"transduction\", \"transductive\", \"transductive\", \"transductive\", \"transductive\", \"transductive\", \"transductive\", \"transductive\", \"transductive\", \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"trees\", \"trees\", \"trees\", \"trees\", \"trees\", \"trees\", \"trees\", \"trees\", \"trees\", \"trees\", \"trous\", \"trous\", \"trous\", \"trous\", \"trous\", \"trous\", \"trous\", \"tticedu\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"uij\", \"ul\", \"ul\", \"ul\", \"ul\", \"ul\", \"ul\", \"un\", \"un\", \"un\", \"un\", \"un\", \"un\", \"un\", \"un\", \"un\", \"unblocking\", \"unblocking\", \"unblocking\", \"unblocking\", \"unblocking\", \"unblocking\", \"unblocking\", \"unblocking\", \"unblocking\", \"uniformization\", \"unigram\", \"unigram\", \"unigram\", \"unigram\", \"unigram\", \"unigram\", \"unil\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"unrevealed\", \"unrevealed\", \"unrevealed\", \"unrevealed\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"va\", \"va\", \"va\", \"va\", \"va\", \"va\", \"va\", \"va\", \"va\", \"val\", \"val\", \"val\", \"val\", \"val\", \"val\", \"val\", \"val\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vero\", \"viewpoint\", \"viewpoint\", \"viewpoint\", \"viewpoint\", \"viewpoint\", \"viewpoint\", \"viewpoint\", \"viewpoint\", \"viewpoint\", \"visibility\", \"visibility\", \"visibility\", \"visibility\", \"visibility\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"vndfm\", \"vndfm\", \"vndfm\", \"vndfm\", \"voice\", \"voice\", \"waij\", \"waij\", \"wave\", \"wave\", \"wave\", \"wave\", \"wave\", \"wave\", \"wave\", \"wave\", \"wave\", \"wd\", \"wd\", \"wd\", \"wd\", \"wd\", \"webspam\", \"webspam\", \"webspam\", \"webspam\", \"webspam\", \"webspam\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weighted\", \"weighted\", \"weighted\", \"weighted\", \"weighted\", \"weighted\", \"weighted\", \"weighted\", \"weighted\", \"weighted\", \"wia\", \"wia\", \"wia\", \"wia\", \"wia\", \"wia\", \"wia\", \"wia\", \"wia\", \"wingfield\", \"winnas\", \"winner\", \"winner\", \"winner\", \"winner\", \"winner\", \"winner\", \"winner\", \"winner\", \"winner\", \"winner\", \"winning\", \"winning\", \"winning\", \"winning\", \"winning\", \"winning\", \"winning\", \"winning\", \"winning\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"worker\", \"worker\", \"worker\", \"worker\", \"worker\", \"worker\", \"worker\", \"worker\", \"worker\", \"worker\", \"wrapper\", \"wrapper\", \"wrapper\", \"wrapper\", \"wrapper\", \"wrapper\", \"wrapper\", \"wrapper\", \"wrapper\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xj\", \"xj\", \"xj\", \"xj\", \"xj\", \"xj\", \"xj\", \"xj\", \"xj\", \"xj\", \"xn\", \"xn\", \"xn\", \"xn\", \"xn\", \"xn\", \"xn\", \"xn\", \"xn\", \"xn\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xs\", \"xs\", \"xs\", \"xs\", \"xs\", \"xs\", \"xs\", \"xs\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xuv\", \"xuv\", \"xuv\", \"xuv\", \"xuv\", \"xuv\", \"xuv\", \"xuv\", \"xuv\", \"ybit\", \"ybit\", \"ybit\", \"ybit\", \"ybit\", \"ybit\", \"yga\", \"yga\", \"yga\", \"yga\", \"yga\", \"ygs\", \"ygs\", \"ygs\", \"ygs\", \"ygs\", \"ygs\", \"yjl\", \"yjl\", \"yjl\", \"yjl\", \"yjl\", \"yjl\", \"yjl\", \"yjl\", \"yjl\", \"yn\", \"yn\", \"yn\", \"yn\", \"yn\", \"yn\", \"yn\", \"yn\", \"yn\", \"zijc\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [7, 5, 3, 6, 9, 2, 1, 8, 10, 4]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el3331359576286932805634280779\", ldavis_el3331359576286932805634280779_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el3331359576286932805634280779\", ldavis_el3331359576286932805634280779_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el3331359576286932805634280779\", ldavis_el3331359576286932805634280779_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pickle\n",
        "import pyLDAvis\n",
        "\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))\n",
        "\n",
        "# # this is a bit time consuming - make the if statement True\n",
        "# # if you want to execute visualization prep yourself\n",
        "if 1 == 1:\n",
        "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "    with open(LDAvis_data_filepath, 'wb') as f:\n",
        "        pickle.dump(LDAvis_prepared, f)\n",
        "\n",
        "# load the pre-prepared pyLDAvis data from disk\n",
        "with open(LDAvis_data_filepath, 'rb') as f:\n",
        "    LDAvis_prepared = pickle.load(f)\n",
        "\n",
        "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_prepared_'+ str(num_topics) +'.html')\n",
        "\n",
        "LDAvis_prepared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78mLd_C23Z_c"
      },
      "source": [
        "** **\n",
        "#### Closing Notes\n",
        "Machine learning has become increasingly popular over the past decade, and recent advances in computational availability have led to exponential growth to people looking for ways how new methods can be incorporated to advance the field of Natural Language Processing.\n",
        "\n",
        "Often, we treat topic models as black-box algorithms, but hopefully, this article addressed to shed light on the underlying math, and intuitions behind it, and high-level code to get you started with any textual data.\n",
        "\n",
        "In the next article, we’ll go one step deeper into understanding how you can evaluate the performance of topic models, tune its hyper-parameters to get more intuitive and reliable results.\n",
        "\n",
        "** **\n",
        "#### References:\n",
        "1. Topic model — Wikipedia. https://en.wikipedia.org/wiki/Topic_model\n",
        "2. Distributed Strategies for Topic Modeling. https://www.ideals.illinois.edu/bitstream/handle/2142/46405/ParallelTopicModels.pdf?sequence=2&isAllowed=y\n",
        "3. Topic Mapping — Software — Resources — Amaral Lab. https://amaral.northwestern.edu/resources/software/topic-mapping\n",
        "4. A Survey of Topic Modeling in Text Mining. https://thesai.org/Downloads/Volume6No1/Paper_21-A_Survey_of_Topic_Modeling_in_Text_Mining.pdf\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}